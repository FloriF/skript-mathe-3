\documentclass[a4paper, 12pt,titlepage, pdf, headsepline]{article}
%Packages%

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage[pd,col,3d]{mgtex_24}
\usepackage{tikz}
\usepackage{marvosym}
\usepackage{ mathrsfs }
\usepackage{gensymb}
\usepackage{framed}
\usepackage{arydshln}
\usepackage[loose]{units}
\usepackage{polynom}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{upgreek}
\usepackage{hyperref}
\usetikzlibrary{tikzmark,calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows}

%TitlePage%
\usepackage[automark]{scrpage2}
\ihead{\headmark}
\chead{}
\pagestyle{scrheadings}

\begin{titlepage}
	\title{Mathematik III}
	\date{\today}
	\author{Marius Hobbhahn, Florian Friedrich}
\end{titlepage}

%NewCommands%
\newcommand\smallO{
	\mathchoice
	{{\scriptstyle\mathcal{O}}}
	{{\scriptstyle\mathcal{O}}}
	{{\scriptscriptstyle\mathcal{O}}}
	{\scalebox{.7}{$\scriptscriptstyle\mathcal{O}$}}
}
\newcommand{\grad}[1]{\textrm{grad(}#1\textrm{)}}
\newcommand{\f}[2]{\nicefrac{#1}{#2}}
\newcommand{\R}{\mathds{R}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\N}{\mathds{N}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\C}{\mathds{C}}
\newcommand{\uline}[1]{\underline{#1}}
\newcommand{\id}{\textrm{id}}
\newcommand{\ggT}[1]{\textrm{ggT($#1$)}}
\newcommand{\rg}{\textrm{rg}}
\newcount\colveccount
\newcommand*\colvec[1]{
	\global\colveccount#1
	\begin{pmatrix}
		\colvecnext
	}
	\def\colvecnext#1{
		#1
		\global\advance\colveccount-1
		\ifnum\colveccount>0
		\\
		\expandafter\colvecnext
		\else
	\end{pmatrix}
	\fi
}
\newcommand{\vecspace}[2]{\langle#1\rangle_{#2}}
\newcommand{\vecspaceR}[1]{\vecspace{#1}{\R}}
\newcommand{\qed}{\hfill$\square$}
\newcommand*{\verticalline}[2]{%
	\begin{tikzpicture}[remember picture, overlay]
	\draw [densely dashed] ($(pic cs:#1) + (1ex,1.8ex)$)
	-- ($(pic cs:#2) + (1ex,-0.3ex)$);
	\end{tikzpicture}
}
\newcommand\tmark[1]{%
	\tikz[overlay,remember picture,baseline] \node [anchor=base] (#1) {};}

\newcommand\TikzLine[3][]{%
	\begin{tikzpicture}[overlay,remember picture]
	\draw[#1] (#2.north west) -- (#3.south east);
	\end{tikzpicture}}

%RenewCommands%

\renewcommand{\Re}[1]{\text{Re}(#1)}
\renewcommand{\Im}[1]{\text{Im}(#1)}
\renewcommand{\i}{\textbf{\textit{i}}}
\renewcommand{\div}{\textrm{  div }}
\renewcommand{\>}{\rightarrow}
\renewcommand{\*}{\cdot}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\phi}{\varphi}
\renewenvironment{rcases}{% 
	\left.\renewcommand*\lbrace.% 
	\begin{cases}}% 
	{\end{cases}\right\rbrace}
\renewcommand{\vec}[1]{\colvec{#1}}

%Document%

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Vektorräume}
\small{\uline{Bemerkung}: 1.1-1.10 identisch mit 8.1-8.10 aus Mathematik 2, SS16}
\subsection{Definition (Reelle Vektorräume)}
\label{1.1}
Ein \underline{$\R$-Vektorraum $V$} ist eine nichtleere Menge, deren Elemente \underline{Vektoren} genannt werden (Bezeichnung mittels kleiner lateinischer Buchstaben, $v,w,x,y,...$), auf der eine Addition $+$ definiert ist, $+\colon V\times V\>V$; und eine Multiplikation mit reellen Zahlen ('Skalare') (Bezeichnung mittels kleiner griechischer Buchstaben $\alpha, \beta, \gamma, \lambda,\mu,...$), $\*\colon\R\times V\>V$, so dass gilt:
\begin{itemize}
	\item[(1.1)] \label{1.1(1.1)}$u+v+w=u+(v+w)\qquad\forall u,v,w\in V$
	\item[(1.2)] \label{1.1(1.2)}Es existiert ein Vektor $\O\in V$ ('\underline{Nullvektor}') mit $v+\O=\O+v=v\qquad\forall v\in V$
	\item[(1.3)] \label{1.1(1.3)} Zu jedem $v\in V$ existiert ein Vektor $-v\in V$ mit $v+(-v)=\O$
	\item[(1.4)] $u+v=v+u\qquad\forall u,v\in V$
\end{itemize}
(Diese Eigenschaften (1.1) bis (1.4) kann man zusammenfassen als '$(V,+)$ ist eine kommutative Gruppe').
\begin{itemize}
	\item[(2.1)] \label{1.1(2.1)}$\overset{\textrm{Addition in }\R}{(\lambda+\mu)}\*v=\lambda\*v\overset{\textrm{Addition in }V}{+}\mu\*v\qquad\forall\lambda,\mu\in\R,v\in V$
	\item[(2.2)] $\lambda(v+w)=\lambda v+\lambda w\qquad\forall\lambda\in\R,v,w\in V$
	\item[(2.3)] $\overset{\textrm{Multiplikation in }\R}{(\lambda\*\mu)}\*v=\lambda\*\overset{\textrm{Multiplikation mit Skalar}}{(\mu\*v)}\qquad\forall\lambda,\mu\in\R,v\in V$
	\item[(2.4)] $1\*v=v\qquad\forall v\in V$
\end{itemize}
\subsection{Beispiel}
\label{1.2}
\begin{itemize}
	\item[a)] trivialer Vektorraum Nullraum: $V=\{\O\}$\\
	      Es gilt $\O+\O\coloneqq\O,\quad\lambda\*\O\coloneqq\O\qquad\forall\lambda\in\R$
	\item[b)] $V=\R^n$, Raum aller 'Spaltenvektoren' der Länge $n$ über $\R$, Elemente haben die Form $\vec3{x_1}{...}{x_n}$ mit $x_1,...,x_n\in\R$.\\
	      $\O=\vec3{0}{...}{0},\quad\vec3{x_1}{...}{x_n}+\vec3{y_1}{...}{y_n}=\vec3{x_1+y_1}{...}{x_n+y_n},\quad\lambda\*\vec3{x_1}{...}{x_n}=\vec3{\lambda\*x_1}{...}{\lambda\*x_n}$
	\item[c)] $\R$ ist ein $\R$-Vektorraum.\\
	      Vektoren: reelle Zahlen.\\
	      Skalare: reelle Zahlen.\\
	      $\O=0$
	\item[d)] Funktionenraum:\\
	      $M\neq\emptyset$ Menge. $V=\mathcal{F}(M,\R)\coloneqq\{f\colon M\>\R\}$\\
	      Menge der auf $M$ definierten reellen Funktionen.\\
	      Für $f,g\in V,\quad\lambda\in\R$ sei
	      \begin{itemize}
	      	\item $f+g\colon M\>\R,\quad(f+g)(x)=f(x)+g(x)\quad\forall x\in M$
	      	\item $\lambda\*f\colon M\>\R,\quad(\lambda\*f)(x)=\lambda\*f(x)\quad\forall x\in M$
	      \end{itemize}
	      Dann ist $V$ mit $\R,+,\*$ ein Vektorraum. Nullvektor ist $f=0\colon M\>\R,\\\quad f(x)=0\qquad\forall x\in M$.\\
	      (kurz: $f\equiv0$, identisch Null)
\end{itemize}
\subsection{Lemma}
Sei $V$ ein $\R$-Vektorraum, $v\in V,\quad\lambda\in\R$
\begin{itemize}
	\item[a)] $0\*v=\O$
	\item[b)] $\lambda\*\O=\O$
	\item[c)] Zu jedem $v\in V$ ist der Vektor $-v$ aus \hyperref[1.1]{(1.3)} in \hyperref[1.1]{1.1} eindeutig bestimmt.
	\item[d)] $(-1)\*v=-v$
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
	\item[a)] \begin{align*}
	      \O\overset{\textrm{\hyperref[1.1]{(1.3)}}}{=}\overbrace{\underbracket{0\*v}_{}}^{x}+\overbrace{(-0\*v)}^{-x}&=\underbracket{(0+0)v}_{}+(-0\*v)\\
	      &\overset{\textrm{\hyperref[1.1]{(2.1)}}}{=}(0\*v+0\*v)+(-0\*v)\\
	      &\overset{\textrm{\hyperref[1.1]{(1.1)}}}{=}0\*v+(0\*v+(-0\*v))\\
	      &\overset{\textrm{\hyperref[1.1]{(1.3)}}}{=}0\*v+\O\\
	      &\overset{\textrm{\hyperref[1.1]{(1.2)}}}{=}0\*v
	\end{align*}
	\item[b)] Wie a), starte mit $\O=\lambda\*\O+(-\lambda\*\O)$, erhalte $\O=\lambda\*\O$
	\item[d)] \begin{align*}
	      \underbracket{v+(-1\*v)}_{}&=1\*v+(-1\*v)\\
	      &\overset{\textrm{\hyperref[1.1]{(2.1)}}}{=}(1+(-1))v\\
	      &=0\*v\\
	      &\overset{\textrm{a)}}{=}\O\\
	      &\overset{\textrm{\hyperref[1.1]{(1.3)}}}{=}v+(-v)
	\end{align*}
	Addiere auf beiden Seiten $-v$:
	\begin{align*}
		\underbracket{v+(-1)v}_{}+(-v) & =v+(-v)+(-v)        \\
		                               & \Rightarrow-1\*v=-v 
	\end{align*}
	\item[c)] Angenommen, zu $v\in V$ gibt es $-v$ und $-v^\prime$ mit $v+(-v)=\O$ und $v+(-v^\prime)=\O$. Dann ist $v+(-v)=v+(-v^\prime)\overset{+(-v)\textrm{auf beiden Seiten}}{\Rightarrow}-v=-v^\prime$
\end{itemize}
\hfill$\square$
\subsection{Definition (Untervektorraum)}
\label{1.4}
Sei $V$ ein $\R$-Vektorraum.\\
Eine Teilmenge $U\subseteq V,\quad U\neq\emptyset$ heißt \underline{Unter(vektor)raum von $V$}, falls $U$ bezüglich der Addition auf $V$ und der Multiplikation mit Skalaren selbst ein Vektorraum ist.
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $V=\R^2,\quad U=\Big\{\vec2{0}{0}\Big\}$ ist Unterraum von $V$
	\item[b)] $V=\R^2,\quad U=\Big\{\vec2{1}{2}\Big\}$ ist kein Unterraum von $V$, z.B. \hyperref[1.1]{(1.2)} ist verletzt, Addition funktioniert auch nicht: $\vec2{1}{2}+\vec2{1}{2}=\vec2{2}{4}\notin U$
	\item[c)] $V=\R^2,\quad U=\Big\{\vec2{\lambda}{0}\Big|~\lambda\in\R\Big\}$ ist ein Unterraum von $V$ (prüfe alle Eigenschaften von \hyperref[1.1]{Definition 1.1}) $\>$ umständlich, einfacher geht es mit \hyperref[1.6]{Definition 1.6}
\end{itemize}
\subsection{Satz (Unterraumkriterium)}
\label{1.6}
Sei $V$ ein $\R$-Vektorraum, sei $\emptyset\neq U\subseteq V$.\\
Dann ist $U$ Unterraum von $V$ genau dann, wenn gilt ($\Leftrightarrow$):
\begin{itemize}
	\item[(1)] $v\in U,\quad\lambda\in\R\Rightarrow\lambda\*v\in U$
	\item[(2)] $v,w\in U\Rightarrow v+w\in U$
\end{itemize}
(oder äquivalent: $\forall v,w\in U, \forall\lambda,\mu\in\R$ ist $\lambda\*v+\mu\*w\in U$)\\
Man sagt: $U$ ist abgeschlossen bezüglich der Vektoraddition und der Multiplikation mit Skalaren.
\subsubsection*{Beweis}
\begin{itemize}
	\item[$\Rightarrow$] ist klar, da $U$ laut \hyperref[1.4]{Definition 1.4} selbst Vektorraum
	\item[$\Leftarrow$] rechne die Vektorraumaxiome nach (\hyperref[1.1]{Definition 1.1}, also z.B. $\O\in U$,...)
\end{itemize}
\hfill$\square$
\subsection{Beispiel}
\label{1.7}
\begin{itemize}
	\item[a)] $\quad$\\
	      \begin{minipage}[c]{0.5\textwidth}
	      	$V$ ist ein $\R$-Vektorraum, $\O\neq v\in V$.\\
	      	Dann ist $G=\{\lambda\*v|\lambda\in\R\}$ ein Unterraum.\\
	      	$V=\R^2,\R^3$: $G$ ist Gerade durch Nullpunkt (geometrisch), z.B.\\ $v=\vec2{2}{1},w=\vec2{1}{2}$\\
	      	Aber: $G^\prime=\{w+\lambda\*v|\lambda\in\R,\quad w\in V\}$ ist kein Unterraum für $w\neq \mu\*v,\quad \mu\in\R$. Warum? Z.B. $\O\notin G^\prime$
	      \end{minipage}
	      \begin{minipage}[c]{0.25\textwidth}$\quad$
	      	\begin{tikzpicture}[scale=0.5]
	      		\draw[->]
	      		(-0.2,0) -- (5,0) node[right] {$x$};
	      		\draw[->]
	      		(0,-0.5) -- (0,5) node[above] {$f(x)$};
	      		\draw[color=blue, thick, ->]
	      		(0,0)--(2,1)node[below]{$v$};
	      		\draw[color=blue, thick, ->]
	      		(0,0)--(1,2)node[above]{$w$};
	      		\draw[color=blue, ->]
	      		(0,0)--(4,2)node[below]{$\quad2\*v$, $G$};
	      		\draw[color=blue, ->]
	      		(0,0)--(2,4)node[above]{$2\*w$};
	      		\draw[color=blue, ->]
	      		(0,1.5)--(3,3)node[right]{$G^\prime$};
	      	\end{tikzpicture}
	      \end{minipage}
	\item[b)] $V=\R^3,\qquad U_1=\Big\{\vec3{x_1}{x_2}{x_3}\in\R^3|x_1+x_2-x_3=0\Big\}$ ist Unterraum. Wir zeigen (1), (2) aus \hyperref[1.6]{1.6}:
	      \begin{itemize}
	      	\item $U_1\neq\emptyset$, z.B. $\O=\vec3{0}{0}{0}\in U_1$, denn $\overset{x_1}{0}+\overset{x_2}{0}-\overset{x_3}{0}=0$
	      	\item[(1)] Sei $\lambda\in\R,\quad v=\vec3{v_1}{v_2}{v_3}\in U_1$, d.h. $v_1+v_2-v_3=0$\\
	      	      Prüfe: Ist $\lambda\*v\in U_1$? $\lambda\*v=\vec3{\lambda\*v_1}{\lambda\*v_2}{\lambda\*v_3}$
	      	      \begin{align*}
	      	      	\lambda\*v_1+\lambda\*v_2-\lambda\*v_3 & =\lambda(v_1+v_2-v_3) \\
	      	      	                                       & =\lambda\*0           \\
	      	      	                                       & =0                    
	      	      \end{align*}
	      	      Also ist $\lambda\*v\in U_1$
	      	\item[(2)] Seien $v=\vec3{v_1}{v_2}{v_3},\quad w=\vec3{w_1}{w_2}{w_3}\in U_1$, d.h. $v_1+v_2-v_3=0,\quad w_1+w_2-w_3=0$. Gilt $v+w\in U_1$?  $v+w=\vec3{v_1+w_1}{v_2+w_2}{v_3+w_3}$
	      	      \begin{align*}
	      	      	(v_1+w_1)+(v_2+w_2)-(v_3+w_3) & =\underbrace{(v_1+v_2-v_3)}_{=0}+\underbrace{(w_1+w_2-w_3)}_{=0} \\
	      	      	                              & =0                                                               
	      	      \end{align*}
	      	      Also $v+w\in U_1$
	      	\item Geometrische Interpretation:\\
	      	      \begin{align*}
	      	      	U_1 & =\Bigg\{\vec3{x_1}{x_2}{x_1+x_2}\Bigg|~ x_1,\quad x_2\in\R\Bigg\}                \\
	      	      	    & =\Bigg\{x_1\*\vec3{1}{0}{1}+x_2\*\vec3{0}{1}{1}\Bigg|~ x_1,\quad x_2\in\R\Bigg\} 
	      	      \end{align*}
	      	      D.h. $U_1$ ist die Ebene durch $\mathcal{O}=\vec3{0}{0}{0}$ mit den Richtungsvektoren $\vec3{1}{0}{1}$ und $\vec3{0}{1}{1}$
	      \end{itemize}
	\item[c)] $U_2=\Bigg\{\vec3{x_1}{x_2}{x_3}\in\R^3~\Bigg|~x_1+x_2-x_3=1\Bigg\}$ ist kein Unterraum. Z.B. $\vec3{0}{0}{0}=\O\notin U_2$: $0+0-0=0\neq1$.\\
	      Anderes Argument: Sei $\lambda\in\R,\quad x=\vec3{x_1}{x_2}{x_3}\in U_2$, d.h. $x_1+x_2-x_3=1$. Gilt $\lambda\*x\in U_2$? $\lambda\*x=\vec3{\lambda x_1}{\lambda x_2}{\lambda x_3}$
	      \begin{align*}
	      	\lambda x_1+\lambda x_2-\lambda x_3 & =\lambda\underbrace{(x_1+x_2-x_3)}_{=1}               \\
	      	                                    & =\underbrace{\lambda=1}_{\textrm{nur für }\lambda=1} 
	      \end{align*}
	      $\Rightarrow$ nicht erfüllt für $\lambda\neq1$.\\
	      Geometrische Interpretation:\\
	      \begin{align*}
	      	U_2 & =\Bigg\{\vec3{x_1}{x_2}{x_1+x_2-1}~\Bigg|~x_1,\quad x_2\in\R\Bigg\}                              \\
	      	    & =\Bigg\{\vec3{0}{0}{-1}+x_1\*\vec3{1}{0}{1}+x_2\*\vec3{0}{1}{1}~\Bigg|~x_1,\quad x_2\in\R\Bigg\} 
	      \end{align*}
	      Ebene durch $\vec3{0}{0}{-1}$ mit Richtungsvektoren $\vec3{1}{0}{1}$ und $\vec3{0}{1}{1}$
	\item[d)] $U_3=\Bigg\{\vec3{x_1}{x_2}{x_3}\in\R^3~\Bigg|~x_1^2+x_2^2+x_3^2\leq1\Bigg\}$ ist kein Unterraum, z.B.\\
	      $\vec3{1}{0}{0}\in U_3,\qquad1^2+0^2+^2\leq1\quad\checkmark$, aber\\
	      $2\*\vec3{1}{0}{0}=\vec3{2}{0}{0}\notin U_3$, denn $2^2+0^2+0^2\nleq1$\\
	      Geometrische Interpretation:\\
	      $U_3$ ist eine Kugel um $\vec3{0}{0}{0}$ mit Radius 1
	\item[e)] $I\subseteq\R$ Intervall\\Menge $C(I)$ ($C$: continuous, stetig) der stetigen Funktionen auf $I$ ist Unterraum von $\mathcal{F}(I,\R)$ (vgl. \hyperref[1.2]{Beispiel 1.2}d)).\\
	      Menge der diffbaren Funktionen auf $I$ ist Unterraum von $C(I)$.
\end{itemize}
\subsection{Satz (Verknüpfungen von UVR)}
$V$ ist ein $\R$.Vektorraum, $U_1,U_2$ sind Unterräume von $V$.
\begin{itemize}
	\item[a)] $U_1\cap U_2=\{u\in V|u\in U_1\wedge u\in U_2\}$ ist Unterraum von $V$.
	\item[b)] $U_1+U_2\coloneqq\{u_1+u_2|u_1\in U_1\wedge u_2\in U_2\}$ \underline{Summe} von $U_1,U_2$ ist Unterraum von $V$\\
	      (das ist nicht die Vereinigung $U_1\cup U_2$!)
\end{itemize}
\subsubsection*{Beweis}
Prüfe \hyperref[1.6]{Unterraumkriterium 1.6}
\begin{itemize}
	\item[a)] Übung: Prüfe $\O\in U_1\cap U_2$? $\checkmark$, (1), (2)
	\item[b)] \begin{itemize}
	\item $U_1+U_2\neq\emptyset$, denn $U_1+U_2\ni\O=\underbrace{\O}_{\in U_1}+\underbrace{\O}_{\in U_2}$
	\item Seien $v=u_1+u_2, \quad u_1\in U_1,\quad u_2\in U_2$ und\\
	      $w=u_1^\prime+u_2^\prime,\quad u_1^\prime\in U_1,\quad u_2^\prime\in U_2$,\\
	      also $v,w\in U_1+U_2$ und $\lambda,\mu\in\R$.\\
	      \begin{align*}
	      	\Rightarrow\qquad\lambda v+\mu v & =\lambda(u_1+u_2)+\mu(u_1^\prime+u_2^\prime)                                                         \\
	      	                                 & =\underbrace{\lambda u_1+\mu u_1^\prime}_{\in U_1}+\underbrace{\lambda u_2+\mu u_2^\prime}_{\in U_2} 
	      	                                 & \in U_1+U_2                                                                                          
	      \end{align*}
\end{itemize}
\end{itemize}
\subsection{Bemerkung}
\begin{itemize}
	\item[a)] lässt sich für unendlich viele Unterräume ausweiten
	\item[b)] lässt sich für endlich viele Unterräume ausweiten
	\item $U_1\cup U_2$ ist im Allgemeinen \underline{kein} Unterraum
\end{itemize}
\subsection{Beispiel}
\begin{itemize}
	\item $v=\vec2{1}{0}\in\R^2\qquad\qquad G_1=\{\lambda v|\lambda\in\R\}$
	\item $w=\vec2{2}{1}\in\R^2\qquad\qquad G_2=\{\mu w|\mu\in\R\}$
\end{itemize}
(vgl. \hyperref[1.7]{1.7}a), Geraden durch $\vec2{0}{0}$, Unterräume
\begin{itemize}
	\item $G_1+G_2$ ist Ebene
	\item $G_1\cap G_2$ ist $\O=\vec2{0}{0}$
\end{itemize}
	
%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%
%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%
%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%
%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%
%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%
%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%%MATHEMATIK 3%
	
\subsection{Beispiel}
\marginpar{18.10.16}
\begin{minipage}[c]{0.5\textwidth}
	\InitGraph{5}{8}{1}{1}{0.7cm}
	\Viewpoint(300,70,10,8)[1]
	\SetCMYKColor(0.5,0,0,0)
	\ShowFullPlaneThrough(2.1,1,1)NormalTo(0,2,-2)(2.1)
	\Text[t]{$E$}
	\SetCMYKColor(0,0,0,255)
	\DDArrowAt(0,0,0)(6,0,0)
	\Text[b]{x}
	\DDArrowAt(0,0,0)(4,1,1)
	\SetDashed
	\DDArrowAt(0,0,0)(0,1000,0)
	\Text[b]{y}
	\SetNormal
	\DDArrowAt(0,30,0)(0,1000,0)
	\DDArrowAt(0,0,0)(0,0,6)
	\Text[l]{z}
	\SetCMYKColor(0,1,1,0)
	\DDArrowAt(0,0,0)(2,0,0)
	\Text[b]{$v$}
	\SetCMYKColor(1,1,0,0)
	\DDArrowAt(0,0,0)(0,1,1)
	\Text[t]{$u$}
	\Text[t]{$u+2v$}
	\CloseGraph
\end{minipage}
\begin{minipage}[c]{0.65\textwidth}
	\begin{itemize}
		\item $ u = \vec3{0}{1}{1}$
		\item $ v = \vec3{2}{0}{0}$
		\item $ E = \Bigg\{ \lambda_1 \cdot \vec3{0}{1}{1} + \lambda_2 \cdot \vec3{2}{0}{0} \Bigg|~ \lambda_1, \lambda_2 \in \R\Bigg\}$
	\end{itemize}
\end{minipage}
\\
\\
\\
\begin{itemize}
	\item E $\subseteq \R^3 $ ist Untervektorraum (UVR) und wird \uline{aufgespannt/erzeugt} von $u$ und $v$. Man nennt $\Bigg\{\vec3{0}{1}{1},\vec3{2}{0}{0}\Bigg\}$ \uline{Erzeugendensystem} von $E$.
	\item D.h. $w \in E \Leftrightarrow \exists \lambda_1, \lambda_2 \in \R: w = \underbrace{\lambda_1 \cdot u + \lambda_2 \cdot v}_{\text{Linearkombination von $u$ und $v$}}$
	\item $w \notin E$, z.B. $w = \vec3{0}{0}{1}$ ergibt: \\
	      $\vec3{0}{0}{1} = \lambda_1 \cdot u + \lambda_2 \cdot v = \lambda_1 \vec3{0}{1}{1} + \lambda_2 \vec3{2}{0}{0} \\
	      \\
	      \Rightarrow \begin{rcases}
	      \textrm{Letzte Zeile: }1=\lambda_1\\
	      \textrm{Zweite Zeile: }0=\lambda_1
	\end{rcases}$\Lightning
	\\
	$\Rightarrow \vec3{0}{0}{1} \notin E$
\end{itemize}
\subsubsection*{Beispiel}
\marginpar{(Nachtrag vom 19.10.16)}
\begin{itemize}
	\item[a)] $E = \Bigg\langle \vec3{0}{1}{1}, \vec3{2}{0}{0} \Bigg\rangle_{\R}$
	\item[b)] $\R^n$ wird erzeugt von $e_j = \vec5{0}{\vdots}{1}{\vdots}{0}$, wobei j die Stelle ist, an der der Vektor 1 ist. \\
	      $R^n = \Bigg\langle\vec4{1}{0}{0}{\vdots}, \vec4{0}{1}{0}{\vdots}, ...,\vec4{0}{0}{\vdots}{1}\Bigg\rangle_\R$ "kanonische Einheitsvektoren" \\
	      $v = \vec3{v_1}{\vdots}{v_n} = v_1 \cdot e_1 + v_2 \cdot e_2 + ... + e_n \cdot v_n$
	\item[c)] Spannen $\vec2{1}{1}$ und $\vec2{1}{2}$ den $\R^2$ auf? \\
	      Wenn ja, dann muss für $\vec2{x}{y} \in  \R^2\qquad \alpha, \beta \in \R$ existieren mit 
	      \begin{align*}
	      	                &   & \alpha \cdot \vec2{1}{1} + \beta \cdot \vec2{1}{2} & = \vec2{x}{y} \\
	      	\Leftrightarrow &   & \alpha + \beta                                     & = x           \\
	      	                &   & \alpha + 2\beta                                    & = y           \\
	      	\Rightarrow     &   & \alpha                                             & = x - \beta   \\
	      	                &   &                                                    & = y - 2 \beta \\
	      	\Leftrightarrow &   & \beta                                              & = y - x       \\
	      	                &   & \alpha                                             & = 2x -y       
	      \end{align*}
	      $\Rightarrow$\quad Allg. $ \vec2{x}{y} = (2x-y) \cdot \vec2{1}{1} + (y-x)\cdot \vec2{1}{2} \Rightarrow \R^2 = \Big\langle \vec2{1}{1}, \vec2{1}{2} \Big\rangle_{\R}$
	\item[d)] Spannen $\vec2{1}{2}$ und $\vec2{3}{6}$ den $\R^2$ auf? \\
	      Nein, denn $\vec2{3}{6}$ ist $3 \cdot \vec2{1}{2} \Rightarrow \Big\langle\vec2{1}{2}, \vec2{3}{6} \Big\rangle_{\R} =  \Big\langle \vec2{1}{2} \Big\rangle_{\R} 
	      = \Big\{\lambda \cdot \vec2{1}{2}\Big|~ \lambda \in \R \Big\} \subsetneq \R^2$
	\item[e)]
	      $\Big\langle \vec2{1}{0}, \vec2{0}{1} \Big\rangle_{\R} = \Big\langle \vec2{1}{1}, \vec2{1}{2} \Big\rangle_{\R} = \R^2$, d.h. Erzeugendensysteme sind \uline{nicht} eindeutig!
	\item[f)] 
	      $\Big\langle \vec2{1}{1}, \vec2{1}{2}, \vec2{2}{3} \Big\rangle_{\R} = \Big\langle \vec2{1}{1}, \vec2{1}{2} \Big\rangle_{\R}$, da $\vec2{2}{3} = 
	      \vec2{1}{1} + \vec2{1}{2}$.\\
	      D.h. $M = \Big\{\vec2{1}{1}, \vec2{1}{2}, \vec2{2}{3}\Big \}$ ist kein \uline{minimales} Erzeugendensystem des $\R^2$, denn $v \in M$ kann immer dargestellt werden als Linearkombination von Vektoren aus $M \setminus \{v\}$. \\
	      Man sagt: $\vec2{1}{1}, \vec2{1}{2}, \vec2{2}{3}$ sind \uline{linear abhängig}.
\end{itemize}
\subsection{Definition (Linearkombination, Erzeugendensystem)}
$V: \R$-VR (V ist Vektorraum in den reellen Zahlen) \\
\begin{itemize}
	\item[(i)] $v_1, ... , v_m \in V$ und $\lambda_1,...,\lambda_m \in \R$\\ Der Vektor $\lambda_1 \* v_1 + ... + \lambda_m \* v_m$ heißt \uline{Linearkombination} von $v_1,...,v_m$.
	\item[(ii)] Sei $M \subseteq V$. Dann ist
	      \begin{center}
	      	$\langle M \rangle_{\R} = \big\{ \sum_{k = 1}^{n} \lambda_k \cdot v_k~ \big|~ \lambda_k \in \R, v_k \in M, n \in \mathds{N}\big\}$
	      \end{center}
	      der \underline{von $M$ aufgespannte/erzeugte UVR} von V \\
	      \\
	      Vereinbarung: $\langle \emptyset \rangle = \{\mathcal{O}\}$\\
	      Schreibweise: $M = \{v_1,...,v_m\}$\\
	      \noindent\hspace*{22mm}$\langle M \rangle_{\R} = \langle v_1,..., v_m\rangle_{\R} $
	\item[(iii)]
	      Ist $V = \langle M \rangle_{\R}$, so heißt $M$ ein \uline{Erzeugendensystem} von $V$. $V$ heißt \uline{endlich erzeugt}, falls es ein endliches Erzeugendensystem gibt.
\end{itemize}
\subsection{Bemerkung}
$M \subseteq V \Rightarrow \langle M \rangle_{\R}$ ist der kleinste UVR von $V$, der $M$ enthält.\\
\subsubsection*{Beweis}
\begin{itemize}
	\item $\langle M \rangle_{\R}$ ist UVR. erfüllt Kriterien von \hyperref[1.6]{1.6}, daher klar: \\
	      \hyperref[1.6]{1.6} 2) erfüllt. $u \in \langle M \rangle_{\R} \Rightarrow u = \lambda_1 \cdot v_1 + ... + \lambda_n \cdot v_n\quad(M = \{v_1, ..., v_n \})\\ \Rightarrow \lambda \cdot u = \underbrace{\lambda  \lambda_1}_{\in \R} \cdot v_1 + ... + \underbrace{\lambda \lambda_n}_{\in \R} \cdot v_n$\\
	      \hyperref[1.6]{1.6} 3) ähnlich.
	\item Angenommen $U$ ist der kleinste UVR, so dass $M \subseteq U$. \\
	      Z. z.: $\langle M \rangle_{\R} = U.$\\
	      Wegen \hyperref[1.6]{1.6} enthält $U$ alle Linearkombinationen von Vektoren aus M. \\
	      $\Rightarrow \langle M \rangle_{\R} \subseteq U \Rightarrow U$ kann nicht kleiner sein als $\langle M \rangle_{\R} \Rightarrow \langle M \rangle_{\R} = U$\hfill$\square$
\end{itemize}
\subsubsection*{Beispiel}
\marginpar{19.10.16}
$M = \Bigg\{\vec3{0}{1}{1} \Bigg\} \Rightarrow \vecspaceR{M} = \Bigg\{ \lambda \vec3{0}{1}{1} \Bigg|~  \lambda  \in  \R\Bigg \}$ Gerade 
\begin{itemize}
	\item $\vecspaceR{M} \supseteq M$
	\item $ E = \Bigg\{\lambda_1 \vec3{0}{1}{1}  + \lambda_2 \vec3{2}{0}{0} \Bigg|~ \lambda_1, \lambda_2 \in \R \Bigg\} \supseteq M $
\end{itemize}
$\vecspaceR{M}$ Gerade, E Ebene, d.h. E ist größer als $\vecspaceR{M}$\\
$\vecspaceR{M}$ ist der kleinste UVR von $\R^3$, der M enthält.
\subsection{Definition (Lineare Unabhängigkeit)}	
\begin{itemize}
	\item $V\colon\quad\R - VR,\quad v_1,...,v_n$ heißen \uline{linear unabhängig}, wenn gilt: 
	      \begin{center}
	      	$\begin{rcases}
	      		\lambda_1 \cdot v_1 + ... + \lambda_m \cdot v_m = 0 \\
	      		\lambda_1,...,\lambda_m\in\R
	      	\end{rcases}
	      	\Rightarrow \underbrace{\lambda = \lambda_2 = ... = \lambda_m = 0}_{\textrm{einzige Lösung!}}$
	      \end{center}
	\item $M \subseteq V$ heißt linear unabhängig, wenn gilt: \\
	      Für beliebiges $m \in \N $ und $v_1,...,v_m \in M$ paarweise verschieden sind $v_1,...,v_m$ linear unabhängig
	\item Ist in obigen beiden Fällen (mindestens) $\lambda_i \neq 0$, dann sind die Vektoren linear abhängig
\end{itemize}
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $\mathcal{O}$ ist linear abhängig, da $\lambda \cdot \mathcal{O} = \mathcal{O} \qquad \forall \lambda \neq 0$
	\item[b)] Sind $\vec2{1}{2} , \vec2{-3}{1}, \vec2{1}{-5}$ linear abhängig in $\R^2$ ? \\
	      $\lambda_1 \cdot \vec2{1}{2} + \lambda_2 \cdot \vec2{-3}{1} + \lambda_3 \cdot \vec2{1}{-5} = \mathcal{O}$\\
	      $\begin{cases}
	      	\text{I}  \qquad \lambda_1 -3\lambda_2 + \lambda_3     & = 0 \\
	      	\text{II} \qquad 2 \lambda_1 + \lambda_2 - 5 \lambda_3 & = 0 
	      \end{cases}$\quad
	      Erfüllt für $\lambda_1 = \lambda_2 = \lambda_3 = 0$. Aber hier gibt es noch die Lösung: $\lambda_1 = 2,\quad \lambda_2 = \lambda_3 = 1$!\\
	      $\Rightarrow$ Vektoren sind linear abhängig 
	\item[c)] 
	      $\vec3{1}{0}{0}, \vec3{0}{1}{0}, \vec3{0}{0}{1}$ linear unabhängig (l.u.) in $\R^3$ 
	\item[d)]
	      $v \neq \mathcal{O},\quad v \in V,\quad v$, ist linear unabhängig \\
	      Angenommen es existiert $\lambda \neq 0$ mit $\lambda \cdot v = \mathcal{O}$. \\
	      $\Rightarrow v = (\frac{1}{\lambda} \cdot \lambda)\* v = \frac{1}{\lambda} \cdot (\lambda \cdot v) = \mathcal{O}$ \Lightning
	\item[e)]
	      \begin{align*}
	      	v,w \text{ linear abhängig } & \Leftrightarrow v = \lambda w \text{ , für ein } \lambda \in \R \\
	      	                              & \Leftrightarrow v \in \vecspaceR{w}                              
	      \end{align*}
	\item[f)]In $V = \mathcal{F}(\R{, \R}) = \{ f: \R \rightarrow \R \vert \text{ f Abbildung} \} $ sind die Vektoren
	      \begin{itemize}
	      	\item $f(x) = x,\quad g(x) = x^2 $ linear unabhängig
	      	\item $f(x) = \sin^2(x),\quad g(x) = \cos^2(x), \quad h(x) = 2$ linear abhängig: \\
	      	      \begin{align*}
	      	      	2 & =2\*(\sin^2x+\cos^2x)                                                                              \\
	      	      	  & =2\sin^2x+2\cos^2x                                                                                 \\
	      	      	0 & =\underbrace{2}_{\lambda_1}\sin^2x+\underbrace{2}_{\lambda_2}\cos^2x\underbrace{-1}_{\lambda_3}\*2 
	      	      \end{align*}
	      \end{itemize}
\end{itemize}
\subsection{Satz (Lineare Unabhängigkeit)}
\label{1.16}
$M = \{v_1,...,v_n \} \subseteq V$\\
\begin{itemize}
	\item[(i)] $M$ linear unabhängig $\Leftrightarrow  \text{ Zu jedem } v \in \vecspaceR{M}$ gibt es eindeutig bestimmte \\
	      \noindent\hspace*{43mm}$\lambda_1, ... \lambda_n  \in \R:v = \sum_{i= 1}^{n} \lambda_i \cdot v_i$
	\item[(ii)] $M$ linear unabhängig, $v \notin \vecspaceR{M} \Rightarrow M \cup \{ v \} $ linear unabhängig 
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
	\item[(i)] 
	      \begin{itemize}
	      	\item[($\Leftarrow$)] $ \mathcal{O} \in \vecspaceR{M} \Rightarrow \exists$ eindeutig bestimmte $\lambda_1, ... , \lambda_m \in \R:$\\
	      	      \noindent\hspace*{23mm}$\mathcal{O} = \lambda_1 \cdot v_1 + ... + \lambda_n \cdot v_n$ \\
	      	      Gleichung erfüllt für $\lambda_1 = ... = \lambda_n = 0$ (eindeutige Lösung)
	      	\item[($\Rightarrow$)] Sei $M$ linear unabhängig, $v \in \vecspaceR{M}$\\
	      	      Angenommen $v = \sum_{i = 1}^{n} \lambda_i \cdot v_i = \sum_{i = 1}^{n} \mu_i \cdot v_i $\\
	      	      \noindent\hspace*{25mm}$\Leftrightarrow  \sum_{i=1}^{n} \underbrace{(\lambda_i - \mu_i)}_{=0\textrm{, da }M\textrm{ linear unabhängig}} \cdot v_i = \mathcal{O}$\\
	      	      \noindent\hspace*{25mm}$\Rightarrow \lambda_i = \mu_i \qquad \forall i = 1,...,n$
	      \end{itemize}
	\item[(ii)] Z.z.: $\sum_{i=1}^{n} \lambda_i \cdot v_i + \lambda \cdot v = \mathcal{O} \Rightarrow \lambda_i = 0 \quad\forall i, \lambda = 0$ \\
	      Annahme: $\lambda \neq 0 \Rightarrow v = \underbrace{-\frac{\lambda_1}{\lambda}}_{\in\R} \cdot v_1 - ... - \frac{\lambda_n}{\lambda} \cdot v_n $\\
	      \noindent\hspace*{29mm}$\Rightarrow v \in \vecspaceR{M}$\Lightning. Also $\lambda = 0$ \\
	      $\lambda_i = 0$, weil M linear unabhängig.\hfill$\square$
\end{itemize}
\subsection{Satz (Lineare Unabhängigkeit)}
\label{1.17}
$M \subseteq V$ linear unabhängig genau dann, wenn gilt:
\begin{align*}
	N \subseteq M,\quad \vecspaceR{N} & = \vecspaceR{M} \Rightarrow N = M 
\end{align*}
In Worten: Man kann von $M$ keinen Vektor weglassen, ohne dass der von $M$ aufgespannte Raum sich verkleinert.  \\
\subsubsection*{Beweis}
\begin{itemize}
	\item[$(\Rightarrow)$] Sei $M \subseteq V$ linear unabhängig. \\
	      Angenommen: Man kann doch aus $M$ Vektoren weglassen, d.h.
	      \begin{align*}
	      	N\subseteq M,\quad\vecspaceR{N}=\vecspaceR{M}\textrm{ und }N\neq M 
	      \end{align*}
	      \begin{align*}
	      	N\neq M & \Rightarrow\exists x\in M\setminus N      & \textrm{(da }N\subseteq M\textrm{)}                \\
	      	        & \Rightarrow\exists v_1,...,v_n\in N       & \textrm{paarweise verschieden und}                 \\
	      	        & \qquad\exists\lambda_1,...,\lambda_n\in\R & \textrm{so dass}                                   \\
	      	        & \qquad x=\lambda_1v_1+...+\lambda_nv_n    & \textrm{(da }\vecspaceR{N}=\vecspaceR{M}\textrm{)} \\
	      	&\Rightarrow\lambda_1v_1+...+\lambda_nv_n-x=\mathcal{O}\\
	      	&\qquad\underbrace{v_1,...,v_n}_{\in N},\quad \underbrace{x}_{\in M\setminus N}\textrm{ paarweise verschieden}
	      \end{align*}
	      Da $N\subseteq M$, ist $\underbrace{v_1,...,v_n,x}_{\textrm{linear abhängig}}\in M\Rightarrow M$ linear abhängig\Lightning\\
	      Also muss $N=M$ gelten.
	\item[$(\Leftarrow)$] Sei $M$ linear abhängig. \\
	      Z.z. Man kann Vektoren aus $M$ weglassen, d.h.:
	      \begin{align*}
	      	\exists N \subseteq M,\quad \vecspaceR{N} = \vecspaceR{M}\textrm{ und }N \neq M 
	      \end{align*}
	      $M$ linear abhängig $\Rightarrow \exists n \in \N\quad \exists v_1, ..., v_n \in M$\\
	      \noindent\hspace*{37mm}$\exists \lambda_1,...,\lambda_n \in \R $ (mit $\lambda_i \neq 0$ für ein i) \\
	      \noindent\hspace*{37mm}$\lambda_1 \cdot v_1 + ... + \lambda_n \cdot v_n = 0$\\
	      \\
	      O.B.d.A: $\lambda_1 \neq 0 \Rightarrow v_1 = -\frac{\lambda_2}{\lambda_1} \cdot v_2 - \frac{\lambda_3}{\lambda_1} \cdot v_3 - ... -\frac{\lambda_n}{\lambda_1} \cdot v_n$\\
	      Setze $N = M \setminus \{v_1 \} \Rightarrow N \neq M$\\
	      Da $v_1$ Linearkombination von $v_2,...,v_n$ folgt:\\
	      Jede Linearkombination von $v_1,...,v_n$ lässt sich ausdrücken als Linearkombination von $v_2,...,v_n \Rightarrow \vecspaceR{N} = \vecspaceR{M}$\hfill$\square$
\end{itemize}
\subsection*{Basis und Dimension}
\marginpar{25.10.16}
Ein minimales Erzeugendensystem heißt Basis.
\subsection{Definition (Basis)}
$V$ \underline{endlich} erzeugter $\R$-VR. Eine endliche Menge $B\subseteq V$ heißt \underline{Basis}, falls
\begin{itemize}
	\item $\langle B\rangle_\R=V$ und
	\item $B$ linear unabhängig.
\end{itemize}
Für $V=\{\mathcal{O}\}$ ist $B=\emptyset$ die Basis.
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $\{e_1,...,e_n\}$ ist Basis von $\R^n$ ('Standard-/kanonische Basis')
	\item[b)] Basis ist nicht eindeutig.\\
	      $B_1=\Big\{\vec{2}{1}{0},\vec{2}{0}{1}\Big\},\qquad B_2=\Big\{\vec{2}{1}{1},\vec{2}{1}{2}\Big\}$\\
	      $\Rightarrow \langle B_1\rangle_\R=\langle B_2\rangle_\R$, da: $\vec{2}{1}{0}=2\vec{2}{1}{1}-\vec{2}{1}{2}$ und $\vec{2}{0}{1}=\vec{2}{1}{2}-\vec{2}{1}{1}$\\
	      $\Rightarrow\vec{2}{1}{0},\vec{2}{0}{1}\in\langle B_2\rangle_\R\Rightarrow\R^2=\langle B_1\rangle_\R\subseteq\langle B_2\rangle_\R$
\end{itemize}
\subsection{Satz (Existenz von Basen)}
\label{1.20}
$V$ \underline{endlich} erzeugter $\R$-VR $\Rightarrow$ Jedes endliche Erzeugendensystem enthält Basis.
\subsubsection*{Beweis}
Sei $M\subseteq V$ endlich, $\langle M\rangle_\R=V$
\begin{itemize}
	\item $M$ linear unabhängig $\rightarrow$ fertig
	\item $M$ linear abhängig $\overset{\hyperref[1.17]{1.17}}{\Rightarrow}$ Man kann aus $M$ einen Vektor $v\in M$ weglassen,\\
	      \noindent\hspace*{38mm} so dass $\langle M\setminus\{v\}\rangle_\R=V=\langle M\rangle_\R$. Nach endlich vielen\\
	      \noindent\hspace*{38mm} Schritten liefert das Verfahren eine Basis.\hfill$\square$
\end{itemize}
\subsection*{Fragen}
\begin{itemize}
	\item Basis nicht eindeutig. Sind alle Basen gleich groß?
	\item geg. $w=\vec{3}{\frac{1}{3}}{0}{1}\in\R^3,\qquad S=\{e_1,e_2,e_3\}$. Wie kann man $w$ zu einer Basis ergänzen? Welche Vektoren aus $S$ sind geeignet?
	      \begin{align*}
	      	w=\frac{1}{3}e_1+e_3 & =\{\underbrace{w,e_1,e_3}_{\textrm{linear abhängig}}\}\textrm{ keine Basis, aber}                            \\
	      	                     & \quad\{\underbrace{w,e_1,e_2}_{\textrm{linear unabhängig}}\}\textrm{ Basis und }\{w,e_2,e_3\}\textrm{ Basis} 
	      \end{align*}
\end{itemize}
\subsection{Satz (Austauschlemma)}
$V$ endlich erzeugter $\R$-VR. Gegeben: $w\in V,\quad w\neq\mathcal{O},\quad w=\sum_{i=1}^{n}\lambda_iv_i$, wobei $B=\{v_1,...,v_n\}\subseteq V$ Basis von $V$.\\
$\Rightarrow\underbrace{\big(B\setminus\{v_j\}\big)\cup\{w\}}_{(\star)}$ Basis, falls $\underbracket{\lambda_j\neq0}$
\subsubsection*{Beweis}
Z.z: ($\star$) ist Basis.
\begin{itemize}
	\item[1)] ($\star$) ist linear unabhängig.\\
	      Z.z: \begin{align*}
	      \sum_{i\neq j}^{}\mu_iv_i+\mu w=0&\Rightarrow\mu_i=0\textrm{ und }\mu=0\\
	      \\
	      \sum_{i\neq j}^{}\mu_iv_i+\mu w&=\sum_{i\neq j}^{}\mu_iv_i+\mu\bigg(\sum_{i=1}^{n}\lambda_iv_i\bigg)\\
	      &=\sum_{i\neq j}^{}(\mu_i+\mu\lambda_i)v_i+\mu\lambda_jv_j\\
	      &=0\\
	      \\
	      B=\{v_1,...,v_n\}\textrm{ Basis }&\Rightarrow\mu\lambda_j=0\textrm{ und }\mu_i+\mu\lambda_i=0\quad\forall i\neq j\\
	      \underbracket{\lambda_j\neq 0}&\Rightarrow\mu=0\Rightarrow\mu_i+\underbrace{\mu\lambda_i}_{=0}=\mu_i=0\quad\forall i\neq j
	\end{align*}
	\item[2)] ($\star$) erzeugt $V$.\\
	      \begin{align*}
	      	  &   & w & =\lambda_jv_j+\sum_{i\neq j}^{\lambda_iv_i} &   & |\colon\lambda_j\textrm{, da }\lambda_j\neq 0 \\
	      	&\Leftrightarrow &v_j&=\frac{1}{\lambda_j}w-\sum_{i\neq j}^{}\frac{\lambda_i}{\lambda_j}v_i&\\
	      	&\Rightarrow &v_j&\in\langle(B\setminus\{v_j\})\cup\{w\}\rangle_\R&\\
	      	&\Rightarrow&\langle(B\setminus\{v_j\})\cup\{w\}\rangle_\R&=\langle B\cup\{w\}\rangle_\R=V&
	      \end{align*}
\end{itemize}
\subsection{Satz (Steinitz'scher Austauschsatz)}
\label{1.22}
Geg. $w_1,...,w_m\in V$ linear unabhängig, $\{v_1,...,v_n\}$ Basis von $V$.\\
Es folgt:
\begin{itemize}
	\item[a)] Aus den $n$ Vektoren $v_1,...,v_n$ kann man $n-m$ Vektoren auswählen, die mit $w_1,...,w_m$ eine Basis bilden.
	\item[b)] $m\leq n$
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
	\item[a)] \begin{itemize}
	\item[1)] $w_1\in V\Rightarrow w_1=\sum_{i=1}^{n}\lambda_iv_i$\\
	      Wären alle $\lambda_i=0$, dann wäre auch $w_1=\mathcal{O}$. Da $\mathcal{O}\in V$ linear abhängig ist, wäre also auch $w_1,...,w_m$ linear abhängig. $\Lightning$\\
	      Also: Mindestens ein $\lambda_i\neq 0$\\
	      O.B.d.A. $\lambda_1\neq 0$ (sonst umnummerieren) $\overset{\textrm{\hyperref[1.20]{1.20}}}{\Rightarrow}\{w_1,v_2,...,v_n\}$ ist Basis von $V$
	\item[2)]  $w_2\in V\Rightarrow \mu_1w_1+\sum_{i=2}^{n}\mu_iv_i$\\
	      Wären alle $\mu_2,...,\mu_n=0$, so wäre $w_2=\mu_1w_1$, also auch $w_1,w_2$ linear abhängig. $\Lightning$, da $\{w_1,...,w_m\}$ linear unabhängig.\\
	      $\Rightarrow$ Mindestens ein $\mu_i\neq 0,\quad i\in\{2,...,n\}$\\
	      O.B.d.A. $\mu_2\neq 0\overset{\textrm{\hyperref[1.20]{1.20}}}{\Rightarrow}\{w_1,w_2,v_3,...,v_n\}$ Basis von $V$
\end{itemize}\hfill$\square$
\item[b)] $\rightarrow$ Übung
\end{itemize}
\subsection{Korollar}
\label{1.23}
$V$ endlich erzeugter $\R$-VR
\begin{itemize}
	\item[i)] Je zwei Basen von $V$ enthalten gleich viele Elemente.
	\item[ii)] Basisergänzungssatz\\
	      Jede linear unabhängige Teilmenge von $V$ lässt sich zu einer Basis von $V$ ergänzen.
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
	\item[i)] $B,\tilde{B}$ Basen\\
	      $B$ linear unabhängig $\overset{\textrm{\hyperref[1.22]{1.22}b)}}{\Rightarrow}|B|\leq|\tilde{B}|$\\
	      $\tilde{B}$ linear unabhängig $\overset{\textrm{\hyperref[1.22]{1.22}b)}}{\Rightarrow}|\tilde{B}|\leq|B|$\\
	      $\Rightarrow|B|=|\tilde{B}|$
	\item[ii)] Wähle beliebige Basis von $V$ und tausche aus(\hyperref[1.22]{1.22}a)).
\end{itemize}
\subsection{Satz (Basis)}
\label{1.24}
$V$ endlich erzeugter $\R$-VR, $B\subseteq V$.\\
Dann sind äquivalent:
\begin{itemize}
	\item[i)] $B$ ist Basis
	\item[ii)] $B$ ist maximale linear unabhängige Menge in $V$
	\item[iii)] $B$ ist minimales Erzeugendensystem
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
	\item[i)$\Rightarrow$ii)] Wegen \hyperref[1.23]{1.23} (linear unabhängige Menge zu Basis ergänzen, alle Basen gleich groß)
	\item[ii)$\Rightarrow$i)] (Bzw. $\neg$i)$\Rightarrow\neg$ii).) $B$ keine Basis, $B$ linear unabhängig\\
	      $\Rightarrow\langle B\rangle_\R\subsetneq V\Rightarrow\exists v\in V\setminus\langle B\rangle_\R\colon B\cup\{v\}$ linear unabhängig
	\item[i)$\Rightarrow$iii)] Satz \hyperref[1.17]{1.17} 
\end{itemize}\hfill$\square$
\subsection{Definition (Dimension)}
\marginpar{26.10.16}
$V: \R $-VR\\
\begin{itemize}
	\item[i)] Ist $V$ endlich erzeugbar, $B$ Basis von $V,\quad|B| = n$ so hat $V$ die Dimension $n$, $\dim(V) = n$
	\item[ii)] Ist $V$ nicht endlich erzeugbar, so heißt $V$ unendlichdimensional. 
\end{itemize}
\subsection{Korollar}
\label{1.26}
dim $V = n, B \subseteq V, |B| = n$.\\
Dann ist $B$ Basis von $V$, wenn $B$ linear unabhängig oder $\vecspaceR{B} = V$
\subsubsection*{Beweis} Folgt aus \hyperref[1.24]{1.24}
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $\{e_1,...,e_n\}$ Basis von $\R^n \Rightarrow \dim( \R^n) = n$
	\item[b)] $\vecspaceR{\emptyset} = \{\mathcal{O}\} \Rightarrow dim(\{\mathcal{O}\}) = 0$
	\item[c)] Bilden $\vec3{1}{0}{0}, \vec3{0}{1}{0}, \vec3{0}{1}{1}$ Basis von $V$? \\
	      Ja, weil linear unabhängig (siehe \hyperref[1.26]{Korollar 1.26}).
	\item[d)] $V = \R^4, U = \Bigg\langle u_1 = \vec4{1}{2}{0}{1},u_2 = \vec4{0}{2}{1}{0}\Bigg\rangle_\R$ \\
	      $u_1, u_2$ linear unabhängig $\Rightarrow \dim(U) = 2$ \\
	      Ergänze $u_1,u_2$ zu Basis von $V = \R^4$ \\
	      \begin{itemize}
	      	\item 1. Möglichkeit (Austauschlemma + Steinitz) \\
	      	      $\{e_1,e_2,e_3,e_4 \}$ Basis von $\R^4$ \\
	      	      $u_1 = \vec4{1}{2}{0}{1} = e_1 + 2e_2 + e_4 \Rightarrow \{u_1, e_2,e_3, e_4\}$ Basis von $\R^4$ \\
	      	      $u_2 = \vec4{0}{2}{1}{0} = 2 e_2 + e_3 \Rightarrow \{u_1,u_2, e_3,e_4 \}$ Basis von $\R^4$ \\
	      	      (Basis könnte auch anders aussehen, nur beispielhaft dargestellt)
	      	\item 2. Möglichkeit (\hyperref[1.16]{1.16}) \\
	      	      \begin{itemize}
	      	      	\item $e_1 \notin U$ ($\star$)(nachrechnen) \\
	      	      	      $\overset{\hyperref[1.16]{1.16}}{\Rightarrow} \{u_1,u_2,e_1\}$ linear unabhängig
	      	      	\item $e_4 \notin \vecspaceR{\{u_1,u_2,e_1\}}$ (nachrechnen) \\
	      	      	      $\overset{\hyperref[1.16]{1.16}}{\Rightarrow} \{u_1,u_2,e_1,e_4\}$ linear unabhängig und damit Basis (\hyperref[1.26]{Korollar 1.26})
	      	      \end{itemize}
	      	      ($\star$) Angenommen: 
	      	      \begin{align*}
	      	      	e_1 = \vec4{1}{0}{0}{0} & = \lambda_1 \cdot u_1 + \lambda_2 \cdot u_2                                                           \\
	      	      	\vec4{1}{0}{0}{0}       & = \lambda_1 \vec4{1}{2}{0}{1} + \lambda_2 \vec4{0}{2}{1}{0}                                           \\
	      	      	                        & \Leftrightarrow                                                                                       
	      	      	\begin{cases}
	      	      	I\qquad                 & 1 = \lambda_1                                                                                         \\
	      	      	II\qquad                & 0 = 2 \lambda_1 + 2 \lambda_2                                                                         \\
	      	      	III\qquad               & 0 = \lambda_2                                                                                         \\
	      	      	IV\qquad                & 0 = \lambda_1 \textrm{\qquad \Lightning\quad zu I}                                                    
	      	      	\end{cases} 
	      	      	\\
	      	      	                        & \Rightarrow e_1 \notin \vecspaceR{\{u_1,u_2\}} \Rightarrow \{u_1,u_2,e_1\} \text{ linear unabhängig} 
	      	      \end{align*}
	      \end{itemize}
\end{itemize}
\subsection{Satz (Dimensionssatz)}
$V\quad\R$-VR, $\dim(V) = n$ \\
\begin{itemize}
	\item[i)] $U \subseteq V $ ist UVR $ \Rightarrow \dim(U) \leq n$
	\item[ii)] $U \subseteq W \subseteq V,\qquad U,W \textrm{ sind } UVR$ mit $\dim(U) = \dim(W) \Rightarrow U = W$
	\item[iii)] $\dim(U+W) = \dim(U) + \dim(W) - \dim(U \cap W)$
\end{itemize}	
\subsubsection*{Beweis}
\begin{itemize}
	\item[i)] Basis von $U$ kann man zu Basis von $V$ ergänzen $\Rightarrow \dim(U) \leq \dim(V)$
	\item[ii)] $\dim(U)= \dim(W) \overset{U\subseteq W}{\Rightarrow}$ Basis von $U$ auch Basis von $W \Rightarrow U = W$
	\item[iii)] Sei $\{v_1,...,v_k\}$ Basis von $U \cap W$ \\
	      Ergänze $\{v_1,...,v_k\}$ zu 
	      \begin{itemize}
	      	\item[a)] Basis $\{v_1,...,v_k, u_{k+1},...,u_m\}$ von $U$
	      	\item[b)] Basis $\{v_1,...,v_k,w_{k+1},...,w_l \}$ Basis von $W$
	      \end{itemize}
	      \uline{Behauptung:} $B = \{v_1,...,v_k,w_{k+1},...,w_l,u_{k+1},...,u_m\}$ Basis von $U+W$ \\
	      \begin{itemize}
	      	\item[1)] $B$ linear unabhängig\\
	      	      Sei\\
	      	      $\overbrace{\lambda_1 v_1 + ...+ \lambda_k v_k}^{=v} + \overbrace{\mu_{k+1} u_{k+1} + ... + \mu_m u_m}^{=u} + \overbrace{\gamma_{k+1} w_{k+1} +... + \gamma_l w_l}^{=w} = 0$\\
	      	      $ \qquad \lambda_i, \mu_j, \gamma_r \in \R$ \\
	      	      \\
	      	      Es ist $w \in U \cap W$, da
	      	      \begin{itemize}
	      	      	\item $w = \underbrace{\gamma_{k+1} w_{k+1}}_{\in W} + ... + \underbrace{\gamma_l w_l}_{\in W} \in W $
	      	      	\item $w = -\underbrace{u}_{\in U}- \underbrace{v}_{\in U} \in U$ \\
	      	      	      Also: $w \in U \cap W$.
	      	      	      \\
	      	      	      \\
	      	      	      $\Rightarrow \exists \alpha_1,...,\alpha_k \in \R: w = \alpha_1 v_1 + ... + \alpha_k v_k$ \\
	      	      	      $\Rightarrow w = \gamma_{k+1} w_{k+1} + ... + \gamma_l w_l = \alpha_1 v_1 + ... + \alpha_k v_k$ \\
	      	      	      $\Rightarrow  \gamma_{k+1} w_{k+1} + ... + \gamma_l w_l - \alpha_1 v_1 - ... - \alpha_k v_k = 0$ \\
	      	      	      $\{v_1,...,v_k,w_{k+1} , ... , w_l\}$ linear unabhängig\\
	      	      	      $\Rightarrow 
	      	      	      \gamma_{k+1} = ... = \gamma_l = \alpha_1 = ... = \alpha_k = 0$\\
	      	      	      $\Rightarrow w = \mathcal{O}$ und $ v + u + w = v + u = \lambda_1 v_1 + ... + \lambda_k v_k + \mu_{k+1}u_{k+1} + ... + \mu_m u_m = 0$\\
	      	      	      $\{v_1, ..., v_k, u_{k+1}, ... , u_m \}$ linear unabhängig (Basis von $U$) \\
	      	      	      $\Rightarrow \lambda_1 = ... = \lambda_k = \mu_{k+1} = ...= \mu_m = 0$
	      	      \end{itemize}
	      	\item[2)] $\vecspaceR{B} = U + W$, da: 
	      	      \begin{itemize}
	      	      	\item $\vecspaceR{B} \subseteq U + W$ (da $\underbrace{u+v}_{\in U} + \underbrace{w}_{\in W} \in U + W$) 
	      	      	\item $U \subseteq \vecspaceR{B}$ (da Basis von $U$ in $B$) \\
	      	      	      \item$W \subseteq \vecspaceR{B} $
	      	      \end{itemize}
	      	      $\Rightarrow U + W  \subseteq \vecspaceR{B}$
	      	      			
	      \end{itemize}
\end{itemize}\hfill$\square$
\subsection{Bemerkung (Koordinaten)}
\label{1.29}
Geg.: Basis $\{v_1,...,v_n\}$ von $V$,	 Vektor $u \in V$ \\
$\Rightarrow u = \lambda_1 v_1 + ... + \lambda_n v_n$\\
$\lambda_i$ eindeutig und heißen Koordinaten von $u$ bezüglich der Basis $B$. \\
z.B.: $\vec3{2}{1}{3} = \vec3{1}{0}{0} + \vec3{0}{1}{0} + 3 \vec3{\frac{1}{3}}{0}{1} \Rightarrow \vec3{2}{1}{3}$ hat Koordinaten 1,1,3 bezüglich \\$B = \Bigg\{\vec3{1}{0}{0}, \vec3{0}{1}{0}, \vec3{\frac{1}{3}}{0}{1}\Bigg\}$
	
%%2. Matrizen und lineare Gleichungssysteme%%
%%2. Matrizen und lineare Gleichungssysteme%%
%%2. Matrizen und lineare Gleichungssysteme%%
\newpage
\section{Matrizen und lineare Gleichungssysteme}
\marginpar{02.11.16}
\subsection{Beispiel}
\begin{itemize}
	\item Ein Bauer besitzt Kühe und Gänse
	\item Insgesamt 18 Tiere mit 40 Beinen
	\item Frage: Wieviele der Tiere sind Kühe?
\end{itemize}
\uline{Lineares Gleichungssystem (LGS):}
$\ast
\begin{cases}
	I:\qquad k + g   & = 18                                       \\
	II: \quad4k + 2g & = 40\qquad \Leftrightarrow\quad 2k +g = 20 \\
\end{cases}$ \\
\noindent\hspace*{49mm}
$\Rightarrow g = 20 -2k = 18-k \Leftrightarrow k = 2 \Rightarrow g = 16$\\
\\
\uline{Vektorenschreibweise von $\ast$:} \\
$\vec2{k+g}{4k+2g} = \vec2{18}{40}$ oder $k\vec2{1}{4} + g\vec2{1}{2} = \vec2{18}{40}$\\
\\
\uline{Matrixschreibweise:} \\
$\underbrace{\begin{pmatrix}
	1&1 \\
	4&2 \\
	\end{pmatrix}}_{\textrm{Matrix}} \cdot \vec2{k}{g} = \vec2{18}{40}$
\subsection{Definition (Matrix)}
\label{2.2}
Allgemeines lineares Gleichungssystem: \\
Gegeben:
\begin{itemize}
	\item Unbekannte $x_1,...,x_n \in \R, n \in \mathds{N}$
	\item $m \in \mathds{N}$ Gleichungen
	\item Koeffizienten $a_{ij} \in \R, i = 1,...,m; j = 1,...,n$
\end{itemize}
$a_{11}x_1 + a_{12}x_2 + ... + a_{1n}x_n = b_1$\\
$a_{21}x_1 + a_{22}x_2 + ... + a_{2n}x_n = b_2$\\
$\vdots \qquad \qquad \vdots \quad \qquad \vdots \qquad \quad \vdots\quad\qquad\vdots$\\
$a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}x_n = b_m$\\
\\
\\
\\
\\\\
\uline{Matrixschreibweise:} \\
$A x = b$ mit \\
\begin{itemize}
	\item$A=\underset{\textrm{Spalte}}{\begin{pmatrix}
		a_{11}&a_{12}&\cdots&a_{1n}\\
		a_{21}&a_{22}&\cdots&a_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		a_{m1}&\underset{\uparrow}{a_{m2}}&\cdots&a_{mn}
		\end{pmatrix}}\leftarrow\textrm{Zeile}$
	\item
	      $x = \vec3{x_1}{\vdots}{x_n} \in \R^n$
	      \item$b = \vec3{b_1}{\vdots}{b_m} \in \R^m$
\end{itemize}
Man schreibt $A = (a_{ij})_{\substack{i=1,...,m\\j = 1,...,n}}$ oder nur $A =(a_{ij})$, wenn $m,n$ schon bekannt.
\begin{itemize}
	\item $a_{ij} \in \R$ - \uline{Eingänge} der Matrix $A$
	\item $A$ - reelle $m\times n$- Matrix
	\item $\mathcal{M}_{m,n}(\R)$ - Menge aller reellen $m \times n$ - Matrizen
	\item $\mathcal{M}_{n,n}(\R) = M_n (\R)$ - quadratische Matrizen 
\end{itemize}
$(\ast\ast)\quad$Dabei ist\\
$Ax\coloneqq x_1 \vec3{a_{11}}{\vdots}{a_{m1}} + x_2 \vec3{a_{12}}{\vdots}{a_{m2}} + \cdots + x_n \vec3{a_{1n}}{\vdots}{a_{mn}} =
\begin{pmatrix}
	a_{11}x_1 + a_{12}x_2 + ... + a_{1n}x_n                   \\
	\vdots\quad+\quad\vdots\quad+\quad\vdots\quad+\quad\vdots \\
	a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}x_n                   \\
\end{pmatrix} \in \R^m$
\subsection{Bemerkung}
Aus $(\ast\ast)$ ergibt sich: $A: \R^n \rightarrow \R^m , x \longmapsto A \cdot x$ für $A \in \mathcal{M}_{m,n}(\R)$\\
$A$ bildet Vektoren auf Vektoren ab.\\
\\\\\\\\
Matrizen können nicht nur zur Lösung von LGS verwendet werden, sondern auch in der Geometrie: \\
\subsection{Beispiel:}
\label{2.4}
\begin{itemize}
	\item[a)] Spiegelung $S_y$ in $\R^2$ an $y$-Achse\\
	      \begin{minipage}[c]{0.5\textwidth}
	      	\InitGraph{6}{3.5}{2.5}{0}{1cm}
	      	\Coordinates(4,2)(4,2)
	      	\SetDarkgrey
	      	\TextAt(3.5,0)[b]{$x$}
	      	\TextAt(0,3.5)[l]{$y$}
	      	\MoveTo(0,0)
	      	\PaintTriangle(2,0)(0,2)(0,0)
	      	\Text[tr]{$D$}
	      	\SetLightgrey
	      	\PaintTriangle(-2,0)(0,2)(0,0)
	      	\Text[tl]{$D'$}
	      	\SetBlack
	      	\MoveTo(1.5,1.5)
	      	\Text[r]{$\vec2{x}{y}$}
	      	\MoveTo(1,2)
	      	\Text[t]{$S_y$}
	      	\Bezier(-1.5,1.5)(1,3)(1.5,1.5)
	      	\MoveTo(-1.5,1.5)
	      	\ArrowDirection(200,0.1)
	      	\Text[l]{$\vec2{-x}{y}$}
	      	\CloseGraph
	      \end{minipage}
	      \begin{minipage}[c]{0.5\textwidth}
	      				
	      	$S_y: \vec2{x}{y} \mapsto \vec2{-x}{y}\quad x,y \in \R$\\
	      	$S_y: \begin{pmatrix}
	      	s_{11} &s_{12} \\
	      	s_{21} &s_{22} \\
	      	\end{pmatrix} \\
	      	S_y \vec2{x}{y} = \begin{pmatrix}
	      	s_{11} +s_{12} \\
	      	s_{21} +s_{22} \\
	      	\end{pmatrix} = \vec2{-x}{y}$\\
	      	$\Rightarrow s_{11} = -1\quad s_{12} = 0\quad s_{21} = 0\quad s_{22} = 1\\
	      	S_y = \begin{pmatrix}
	      	-1 &0 \\
	      	0 &1 \\
	      	\end{pmatrix}$\\
	      	$S_y$ bildet $D$ auf $D'$ ab.
	      \end{minipage}
	\item[b)] Drehung $D_\phi$ um $\phi \in [0,2\pi)$ \\
	      Vorüberlegung am Einheitskreis:\\
	      \\
	      		
	      \InitGraph{15}{4.5}{0}{0}{1cm}
	      \OpenWindowAt(1,1)(5,4)(1,1)
	      \TextAt(3,1.5)[c]{$\vec2{1}{0} \overset{D_{\phi}}{\rightarrow}\vec2{\cos\phi}{\sin\phi}$}
	      \Axes
	      \SetDotted
	      \CircleAt(0,0)(2)
	      \SetNormal
	      		
	      \ArrowAt(0,0,2,0)
	      \ArrowAt(0,0,1,1.72)
	      \EllipticArcAt(0,0)(1,1)(0,60)
	      \TextAt(0.2,0)[tr]{$\phi$}
	      \SetDashed
	      \LineAt(0,-1,1,-1)
	      \TextAt(0.5,-0.6)[c]{$\cos\phi$}
	      \LineAt(-1,0,-1,1.72)
	      \TextAt(-1,1)[c]{$\sin\phi$}
	      \SetNormal
	      \CloseWindow
	      \OpenWindowAt(7,1)(5,4)(4,1)
	      \TextAt(-3,2.5)[c]{$\vec2{0}{1}\overset{D_{\phi}}{\rightarrow}\vec2{-\sin\phi}{\cos\phi}$}
	      \Axes
	      \SetDotted
	      \CircleAt(0,0)(2)
	      \SetNormal
	      		
	      \ArrowAt(0,0,0,2)
	      \ArrowAt(0,0,-1.72,1)
	      \EllipticArcAt(0,0)(1,1)(90,60)
	      \TextAt(0,0.2)[tl]{$\phi$}
	      \SetDashed
	      \LineAt(0,-1,-1.72,-1)
	      \TextAt(-0.8,-0.6)[c]{$-\sin\phi$}
	      \LineAt(1,0,1,1.72)
	      \TextAt(1,1)[c]{$\cos\phi$}
	      \SetNormal
	      		
	      \CloseWindow
	      \CloseGraph
	      \\
	      \begin{minipage}[c]{0.5\textwidth}
	      	\InitGraph{6}{3.5}{2.5}{0}{1cm}
	      	\Coordinates(4,2)(4,2)
	      	\TextAt(3.5,0)[b]{$x$}
	      	\TextAt(0,3.5)[l]{$y$}
	      	\MoveTo(0,0)
	      	\Text[tr]{$\phi$}
	      	\MoveTo(1.5,1.5)
	      	\Text[r]{$\vec2{x}{y}$}
	      	\MoveTo(1,2)
	      	\Text[t]{$D_\phi$}
	      	\Bezier(-1.5,1.8)(1,3)(1.5,1.5)
	      	\MoveTo(-1.5,1.8)
	      	\ArrowDirection(200,0.1)
	      	\Text[l]{$\vec2{x'}{y'}$}
	      	\ArrowAt(0,0,2,1)
	      	\ArrowAt(0,0,-1.5,1.5)
	      	\EllipticArcAt(0,0)(0.8,0.8)(30,105)
	      	\CloseGraph
	      \end{minipage}
	      \begin{minipage}[c]{0.5\textwidth}
	      	$D_{\phi}: \vec2{x}{y} \rightarrow \vec2{x'}{y'}$\\
	      	$D_{\phi}= \begin{pmatrix}
	      	d_{11} &d_{12} \\
	      	d_{21} &d_{22} \\
	      	\end{pmatrix}\\
	      	\quad\Rightarrow D_{\phi}  \vec2{1}{0} = \vec2{d_{11}}{d_{21}} = \vec2{\cos \phi}{\sin\phi}$\textrm{ und }\\
	      	$D_\phi \vec2{0}{1} = \vec2{d_{12}}{d_{22}}= \vec2{-\sin\phi}{\cos\phi}$\\
	      	$\Rightarrow D_\phi = (D_\phi \cdot e_1, D_\phi \cdot e_2) = \begin{pmatrix}
	      	\cos\phi &- \sin\phi \\
	      	\sin\phi &\cos\phi \\
	      	\end{pmatrix}$
	      \end{minipage}
\end{itemize}
\subsection{Bemerkung}
\label{2.5}
Aus \hyperref[2.4]{Beispiel 2.4} b) und \hyperref[2.2]{Definition 2.2} ergibt sich: \\
$A \cdot e_j = 1 \cdot \vec3{a_{1j}}{\vdots}{a_{mj}}\quad$ ($j$-te Spalte von $A\in\mathcal{M}_{m,n}(\R)$) \\
$\Rightarrow A = (\underbrace{A_{e_1}, A_{e_2},...,A_{e_n}}_{\textrm{Spalten}})$
\subsection{Satz (Rechenregeln)}
\label{2.6}
$A \in \mathcal{M}_{m,n}(\R)\qquad x,y \in \R^n$\\
\begin{itemize}
	\item[i)] $A(\lambda x) = \lambda (A \cdot x) \qquad \lambda \in \R$
	\item[ii)] $A(x+y) = Ax +  Ay$
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
	\item[i)] \begin{align*}
	      A(\lambda x) &= (\lambda x_1) \underbrace{A \cdot e_1}_{\textrm{1. Spalte}} + (\lambda x_2)A e_2 +...+ (\lambda x_n)\underbrace{A e_n}_{n\textrm{-te Spalte}}\\
	      &= \lambda[x_1 (Ae_1) + ... + x_n (Ae_n)] \\
	      &= \lambda (Ax)
	\end{align*}
	\item[ii)] Übung
\end{itemize}
\subsection{Beispiel}
\begin{itemize}
	\item[a)]~\\ \begin{minipage}[c]{0.5\textwidth}
		$A \cdot x = (D_\pi \circ S_y) \cdot \vec2
		{x_1}{x_2} \\
		\noindent\hspace*{6.5mm}= D_\pi \vec2{-x_1}{x_2} \\
		 \noindent\hspace*{6.5mm}=\begin{pmatrix}
	      -1& 0 \\
	      0 &1 \\
	\end{pmatrix} \vec2{-x_1}{x_2} \\
	\noindent\hspace*{6.5mm}= \vec2{x_1}{-x_2}\\
	\\
	\Rightarrow \vec2{x_1}{x_2} \overset{A}{\mapsto} \vec2{x_1}{-x_2}\\
	 A = \begin{pmatrix}
	1  &0 \\
	0 &-1 \\
	\end{pmatrix}$\\
	\\
	 $A=D_\pi\circ S_y$ bildet $D$ auf $D''$ ab.
\end{minipage}
	\begin{minipage}[c]{0.5\textwidth}
		\InitGraph{6}{6}{2.5}{2.5}{0.75cm}
		\Coordinates(4,2)(4,2)
		\SetDarkgrey
		\TextAt(3.5,0)[b]{$x$}
		\TextAt(0,3.5)[l]{$y$}
		\MoveTo(0,0)
		\PaintTriangle(2,0)(0,2)(0,0)
		\Text[tr]{$D$}
		\SetWheat
		\PaintTriangle(-2,0)(0,2)(0,0)
		\Text[tl]{$D'$}
		\SetBlack
		\MoveTo(1.5,1.5)
		\Text[r]{$\vec2{x_1}{x_2}$}
		\MoveTo(1,2)
		\Text[t]{$S_y$}
		\Bezier(-1.5,1.5)(1,3)(1.5,1.5)
		\MoveTo(-1.5,1.5)
		\ArrowDirection(200,0.1)
		\Text[l]{$\vec2{-x_1}{x_2}$}
		\SetOrange
		\PaintTriangle(0,0)(2,0)(0,-2)
		\TextAt(0.75,-0.75)[c]{$D''$} 
		\MoveTo(1.5,-1.5)
		\Text[r]{$\vec2{x_1}{-x_2}$}
		\SetBlack
		\Bezier(1.5,-1.75)(-1,-3)(-1.5,1)
		\MoveTo(1.5,-1.75)
		\ArrowDirection(20,0.1)
		\TextAt(-1.5,-1.5)[c]{$D_\pi$}
		\CloseGraph
	\end{minipage}
	\item[b)]
	      Berechnung Matrixprodukt (Verknüpfung) $A\*B$ \\
	      \begin{align*}
	      	\underbrace{\begin{pmatrix}
	      	a     & b     \\c&d
	      	\end{pmatrix}}_{A}\underbrace{\begin{pmatrix}
	      	e     & f     \\g&h
	      	\end{pmatrix}}_{B}\vec2{x_1}{x_2}&=\begin{pmatrix}
	      	a     & b     \\c&d
	      	\end{pmatrix}\Big[\underbrace{x_1\vec2{e}{g}+x_2\vec2{f}{h}}_{\in\R^2}\Big]\\
	      	&\overset{\hyperref[2.6]{2.6}}{=}\quad x_1\Big[\underbrace{e\vec2{a}{c}+g\vec2{b}{d}}_{\in\R^2}\Big]+x_2\Big[\underbrace{f\vec2{a}{c}+h\vec2{b}{d}}_{\in\R^2}\Big]\\
	      	&=\underbrace{\begin{pmatrix}
	      	ea+gb & fa+hb \\ec+gd&fc+hd
	      	\end{pmatrix}}_{\textrm{Matrixprodukt }A\*B}\vec2{x_1}{x_2}
	      \end{align*}
\end{itemize}	
\subsection{Definition (Matrixprodukt)}
$A=(a_{ij})\in\mathcal{M}_{m,n}(\R)\qquad B=(b_{jk})\in\mathcal{M}_{n,l}(\R)$
\begin{align*}
	A\*B   & =(c_{ik})\quad\in\mathcal{M}_{m,l}(\R)                     \\
	c_{ik} & =(i\textrm{-te Zeile von }A)\*(k\textrm{-te Spalte von }B) \\
	       & =a_{i1}b_{1k}+a_{i2}b_{2k}+\cdots+a_{in}b_{nk}             \\
	       & =\sum_{j=1}^{n}a_{ij}b_{jk}                                
\end{align*}	
(Skalarprodukt)
\subsection{Beispiel}
\marginpar{08.11.16}
$A = \begin{pmatrix}
\uline{1} &\uline{0} & \uline{-1}\\
2 & -3 & 1\\
\end{pmatrix},\quad B = \begin{pmatrix}
1 & \uline{2} & -1 \\
0 & \uline{0} & 0\\
0 & \uline{1} & 0 \\
\end{pmatrix}, \qquad
A \cdot B = \begin{pmatrix}
1 & \uline{1} & -1 \\
2 & 5 & -2 \\
\end{pmatrix}$\\
$B\cdot A$ nicht definiert!
\subsection{Satz + Definition (Vektorraum $\M_{m,n}(\R)$)}
$\mathcal{M}_{m,n}(\R)$ ist Vektorraum mit 
\begin{itemize}
	\item $A + B = (a_{ij} + b_{ij}) \qquad A,B \in \mathcal{M}_{m,n}(\R)$
	\item $\lambda \cdot A = (\lambda a_{ij})\qquad A \in \mathcal{M}_{m,n}(\R), \lambda \in \R$
\end{itemize}
Beweis: Siehe Hausaufgabe 03 Aufgabe 4a)
\subsection{Beispiel}
$A = \begin{pmatrix}
1 & 2 & 3 \\
-1 & 0 & 2 \\
\end{pmatrix}\qquad B = \begin{pmatrix}
0& 0& -3 \\
1&0&1 \\
\end{pmatrix} \\
A + B = \begin{pmatrix}
1 & 2 & 0 \\
0 & 0 & 3 \\
\end{pmatrix}, \qquad (-2) \cdot A = \begin{pmatrix}
-2 & -4 & -6 \\
2 & 0 & -4 \\
\end{pmatrix}$
\subsection{Definition (Matrizentransponierung)}
\begin{itemize}
	\item[i)] 
	      $A \in \mathcal{M}_{m,n}(\R),\quad A = (a_{ij})$.\\
	      Die zu \uline{$A$ transponierte Matrix} (Tauschen von Zeilen und Spalten):
	      \begin{center}
	      	$A^T = \begin{pmatrix}
	      	a_{11} & a_{21} & \cdots & a_{m1} \\
	      	a_{12} & a_{22} & \cdots & a_{m2} \\
	      	\vdots &\vdots&\ddots&\vdots\\
	      	a_{1n} & a_{2n} & \cdots & a_{mn} \\
	      	\end{pmatrix} \in \mathcal{M}_{m,n}(\R)$
	      \end{center}
	      z.B.: $A = \begin{pmatrix}
	      1 & 2 & 0 \\
	      -1 & 1 & 2 \\
	\end{pmatrix} \Rightarrow A^T = \begin{pmatrix}
	1 & -1 \\
	2 & 1 \\
	0 & 2 \\
	\end{pmatrix} $\\
	Eine Matix heißt \uline{symmetrisch}, wenn $A = A^T$, z.B.: 
	\begin{center}
		$A=\begin{pmatrix}
		1 & 2 & 0 \\
		2 & 3 & 4 \\
		0 & 4 & -1 \\
		\end{pmatrix}$
	\end{center}
	\item[ii)]
	      \begin{itemize}
	      	\item Nullmatrix: 
	      	      $\mathcal{O}_{m,n} = \begin{pmatrix}
	      	      0 & \cdots & 0 \\
	      	      \vdots & \ddots & \vdots \\
	      	      0 & \cdots & 0 \\
	      	\end{pmatrix} \in \mathcal{M}_{m,n}(\R)$
	      	\item Einheitsmatrix (nur Hauptdiagonale): 
	      	      $E_n = \begin{pmatrix}
	      	      1 & \dots & 0 \\
	      	      \vdots& \ddots &\vdots \\
	      	      0 & \dots& 1 \\
	      	\end{pmatrix} \in \mathcal{M}_{n}(\R)$
	      \end{itemize}
\end{itemize}
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $A = \begin{pmatrix}
	      1 & 1 \\
	      1 & 1 \\
	\end{pmatrix}\qquad B = \begin{pmatrix}
	2 & 0 \\
	3 & 0 \\
	\end{pmatrix}\\
	A \cdot B = \begin{pmatrix}
	5 & 0 \\
	5 & 0 \\
	\end{pmatrix} \neq B \cdot A = \begin{pmatrix}
	2 & 2 \\
	3 & 3 \\
	\end{pmatrix}$ Matrixmultiplikation nicht kommutativ!
	\item[b)]
	      $A \in \mathcal{M}_{m,n}(\R)$ \\
	      $A \cdot E_n = A$ und $E_m \cdot A = A$
\end{itemize}
	
	
\newpage
\section{Gruppen}
\subsection{Beispiel (Wiederholung zu Permutationen)}
Geg.: Menge $\{A,B,C\}$\\
Anordnungen: ABC, CAB, ACB, ... $\rightarrow$ $3 \cdot 2 \cdot 1= 3!$ Möglichkeiten\\
Jede Anordnung kann man auffassen als eineindeutige (bijektive) Abbildung\\
$\pi : \{A,B,C\} \rightarrow \{A,B,C\} \\
\pi: \begin{tabular}{c|c | c | c }
x&A & B & C \\ \hline
$\pi(x)$&A & C & B \\
\end{tabular}$
\subsection{Definition (Permutation)}	
\begin{itemize}
	\item Eine \uline{Permutation} ist eine eineindeutige Abbildung einer endlichen Menge auf sich selbst. Im Allgemeinen verwendet man die Menge $\{1,...,n\}$ und schreibt eine Permutation $\pi$ als Wertetabelle $\pi = \begin{pmatrix}
	      1~~~...~~~n\\\pi(1)~...~\pi(n)
	\end{pmatrix}$ oder als geordnete Liste der Werte $\pi = \pi(1)... \pi(n)$ 
	\item $\mathscr{S}_n$- Menge aller Permutationen von $\{1,...,n \},\qquad | \mathscr{S}_n| = n!$
\end{itemize}
\subsubsection*{Beispiel} $\mathscr{S}_2=\{\id,(AB)\}=\{\id,(12)\},\quad|\mathscr{S}_2|=2!=2$\\ mit $\id=\begin{pmatrix}
AB\\AB
\end{pmatrix},\qquad \pi=\begin{pmatrix}
AB\\BA
\end{pmatrix}$
\subsection{Beispiel} 	
\begin{minipage}[c]{0.5\textwidth}
	\begin{itemize}
		\item $M = \{1,2,...,5\}$ \\
		      $\pi = \pi(1)...\pi(5) = 23154$\\
		      oder $\pi = \vec2{12345}{23154}$
		\item id(i) $= i \qquad \forall i \in \{1,..,n\}$
	\end{itemize}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
	\begin{tikzpicture}
				
		\node (v1) at (0,0) {1};
		\node (v2) at (0.5,-1.5) {2};
		\node (v3) at (-1,-1.5) {3};
		\draw [->] (v1) edge (v2);
		\draw [->] (v2) edge (v3);
		\draw [->] (v3) edge (v1);
		\node (v4) at (2,0) {4};
		\node (v5) at (2,-1.5) {5};
		\draw [->] (v4) edge (v5);
		\draw [->](1.7454,-1.4777) .. controls (1,-1) and (1.5,-0.5) .. (1.7657,-0.1725);
	\end{tikzpicture}\\
	Graph der Permutation
\end{minipage}
\subsection{Bemerkung}
In Literatur oft \uline{Zyklenschreibweise}:\\
Zyklus $(a_1 a_2... a_k)$ bedeutet $\pi(a_i) = a_{i+1}$ und $\pi(a_k) = a_1$\\
z.B.: $\pi = (123)(45)$ 
\subsection*{Verknüpfung von Permutationen}
\subsection{Beispiel}
\begin{minipage}[c]{0.5\textwidth}
	\begin{tikzpicture}
				
		\node (v1) at (0,0) {1};
		\node (v2) at (0.5,-1.5) {2};
		\node (v3) at (-1,-1.5) {3};
		\draw [->] (v1) edge (v2);
		\draw [->] (v2) edge (v3);
		\draw [->] (v3) edge (v1);
		\node (v4) at (2,0) {4};
		\node (v5) at (2,-1.5) {5};
		\draw [->] (v4) edge (v5);
		\draw [->](1.7454,-1.4777) .. controls (1,-1) and (1.5,-0.5) .. (1.7657,-0.1725);
	\end{tikzpicture}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
	$\pi = \begin{pmatrix}
	12345 \\
	23154 \\
	\end{pmatrix} = (123)(45)$
\end{minipage}\\
\begin{minipage}[c]{0.5\textwidth}
	\begin{tikzpicture}
				
		\node (v1) at (-2,1.5) {1};
		\node (v2) at (0,1.5) {2};
		\node (v3) at (0,-0.5) {3};
		\node (v4) at (-2,-0.5) {4};
		\node at (2,0.5) {5};
		\draw [->] (v1) edge (v2);
		\draw [->] (v2) edge (v3);
		\draw [->] (v3) edge (v4);
		\draw [->] (v4) edge (v1);
		\draw [->](1.976,0.765) .. controls (2.5,1.5) and (2.5,-0.5) .. (1.952,0.2238);
	\end{tikzpicture}
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
	$\sigma = \begin{pmatrix}
	12345 \\
	23415 \\
	\end{pmatrix} = (1234)(5)$
\end{minipage}\\
\begin{minipage}[c]{0.5\textwidth}
	\begin{tikzpicture}
				
				
		\node (v1) at (-1.5,1.5) {1};
		\node (v2) at (0.5,1.5) {3};
		\node (v3) at (2,0) {5};
		\node (v4) at (0.5,-1) {4};
		\node (v5) at (-1.5,-1) {2};
		\draw [->] (v1) edge (v2);
		\draw [->] (v2) edge (v3);
		\draw [->] (v3) edge (v4);
		\draw [->] (v4) edge (v5);
		\draw [->] (v5) edge (v1);
	\end{tikzpicture} 
\end{minipage}
\begin{minipage}[c]{0.5\textwidth}
	$\pi\sigma= \begin{pmatrix}
	12345 \\
	31524 \\
	\end{pmatrix} = (13542)$
\end{minipage}\\
\subsection{Bemerkung} 
\label{3.6}
\begin{itemize}
	\item[a)] Die Verknüpfung von 2 Permutationen $\pi, \sigma$ ist wieder Permutation $\eta$ mit $\eta(i) = \pi \circ \sigma(i) = \pi(\sigma(i))$
	\item[b)] Fixpunkte mit $\pi(i)= i$ lässt man weg, z.B. $\underbrace{(123)(4)}_{\in \mathscr{S}_4} = (123)$
	\item[c)] Jede Permutation kann als Produkt disjunkter Zyklen geschrieben werden, z.B.: $ \underset{\textrm{Verkettung }\circ}{(34)\*(345)}  = (3)(45) = (45)$.\\
	      Zwei Zyklen heißen \uline{disjunkt}, wenn $\{a_1...a_k\}\cap\{b_1...b_j\}=\emptyset$.
	\item[d)] Permutationen sind nur in sehr seltenen Fällen kommutativ: \\
	      $(123)(23) = (12) \neq (23)(123) = (13)$
	\item[e)] Zyklendarstellung nicht eindeutig, z.B.: \\
	      $(123) = (231)$ oder $(34)(12) = (12)(34)$
\end{itemize}
\subsection{Beispiel}
\label{3.7}
\marginpar{09.11.16}
\begin{minipage}[c]{0.8\textwidth}
	\renewcommand{\arraystretch}{1.7}
	\begin{tabular}{ l | c c c c }
		$\substack{\textrm{Symmetrie-} \\\textrm{operationen des}\\\textrm{Rechtecks}}$& Identität & $\substack{\textrm{Spiegelung}\\\textrm{y-Achse}}$ & $\substack{\textrm{Spiegelung}\\\textrm{x-Achse}}$ & Drehung $180 \degree$ \\&
		\frame{\begin{tabular}{ c c }
		D & C \\ 
		A & B \\
	\end{tabular}}&
	\frame{\begin{tabular}{ c : c }
		C&D \\ 
		B&A \\
		\end{tabular}} &
	\frame{\begin{tabular}{ c c }
							
		A&B \\ \hdashline
		D&C \\ 
		\end{tabular}} &
	\frame{\begin{tabular}{ c c }
							
		B&A \\ 
		C&D \\ 
		\end{tabular}}  \\ \hline
	$\substack{\textrm{als Matrix}}$ &
	$E_2 = \begin{pmatrix}
	1 & 0 \\
	0 & 1 \\
	\end{pmatrix}$ & $S_y = \begin{pmatrix}
	-1 & 0 \\
	0 & 1\\
	\end{pmatrix}$ & $S_x = \begin{pmatrix}
	1 & 0 \\
	0 & -1 \\
	\end{pmatrix}$ & $D_\pi = \begin{pmatrix}
	-1 & 0 \\
	0 & -1 \\
	\end{pmatrix}$ \\ \hline
	$\substack{\textrm{als Permutation}\\ \textrm{der Ecken}}$ & id & 
	$\pi = (AB)(CD)$ &
	$\sigma = (AD)(BC)$ &
	$\eta = (AC)(BD)$ \\
				
	\end{tabular}
\end{minipage} \\
\\
\\
\subsubsection*{Verknüpfungstafel}
\begin{tabular}{c | c c c c}
	$\overset{Matrixmultiplikation}{\cdot} $ & $E_2$   & $S_y $  & $S_x$   & $D_\pi$  \\ \hline
	$E_2$                                    & $E_2$   & $S_y$   & $S_x$   & $D_ \pi$ \\
	$S_y$                                    & $S_y$   & $E_2$   & $D_\pi$ & $S_x$    \\
	$S_x$                                    & $S_x$   & $D_\pi$ & $E_2$   & $S_y$    \\
	$D_\pi$                                  & $D_\pi$ & $S_x$   & $S_y$   & $E_2$    \\
\end{tabular}
\subsection{Definition (Grundbegriffe)}
\begin{itemize}
	\item Seien $X,Y$ nichtleere Mengen, Eine Verknüpfung ' $\cdot$ ' ist eine Abbildung
	      \begin{center}
	      	$X \times X \rightarrow Y\qquad (a,b) \rightarrow a \cdot b  \qquad( \leftarrow$ 'Produkt' von a und b)
	      \end{center}
	\item Eine Menge $X \neq \emptyset$ heißt \uline{abgeschlossen} bzgl. einer Verknüpfung ' $\cdot$ ', falls $a \cdot b \in  X \qquad \forall a,b \in X$. \\
	      Beispiel: $X=\{-1,1\}$ mit '$\*$' Addition $\Rightarrow(-1)\*(1)=-1+1=0$
\end{itemize}
Die Menge $\{id, \pi ,\sigma, \eta \}$ aus \hyperref[3.7]{Beispiel 3.7} ist abgeschlossen bzgl. der Verkettung von Permutationen \\
\subsubsection*{Bemerkung} Die Verknüpfung von Elementen einer endlichen Menge stellt man anhand der Verknüpfungstafel dar, siehe \hyperref[3.7]{Beispiel 3.7}.
\subsection{Definition (Gruppe)}
\label{3.9}
\begin{itemize}
	\item[a)] Eine \uline{Gruppe} ist ein Paar $(G, \cdot) $ mit Menge $G \neq \emptyset$ und einer Verknüpfung $\cdot : \underbrace{G \times G \rightarrow G}_{\text{abgeschlossen!}}$, die folgende Eigenschaften erfüllt:
	      \begin{itemize}
	      	\item[1)] $(a \cdot b) \cdot c = a \cdot (b \cdot c) \qquad \forall a,b,c \in G\qquad~$ Assoziativität
	      	\item[2)] $\exists e \in G: a \cdot e = e \cdot a = a \qquad \forall a \in G\qquad$ Neutralelement $\upharpoonleft$
	      	\item[3)] $\forall a \in G\quad \exists a^{-1} \in G: a \cdot a^{-1} = a^{-1}\*a = e\qquad$ Inverse
	      \end{itemize}
	      Falls zusätzlich 
	      \begin{itemize}
	      	\item[4)] $a \cdot b = b \cdot a \qquad  \forall a,b \in G\qquad$ Kommutativität \end{itemize}
	      	      gilt, dann heißt $G$ \uline{abelsche Gruppe}.
	      	      		
	      	\item[b)] $| G |$ heißt \uline{Ordnung} der Gruppe $G$.
	      \end{itemize}
	      \subsection{Beispiel}
	      \label{3.10}
	      \begin{itemize}
	      	\item[a)] $(\{e\}, \cdot )$ ist Gruppe
	      	\item[b)] $\R, \mathds{Z}, \mathds{Q}$ mit ' + ' ist abelsche Gruppe. Inverse zu a ist -a.
	      	\item[c)] $\R, \mathds{Z}, \mathds{Q}$ mit ' $\cdot$ ' keine Gruppen. Problem: 0 besitz keine Inverse, weil \\
	      	      $0 \cdot a = 1$\Lightning
	      	\item[$\Rightarrow$]  $\R, \mathds{Q}$ mit ' $\cdot$ ' Gruppen, wenn man 0 weglässt
	      	\item[d)] Einzige \uline{endliche} Gruppen von reellen Zahlen: 
	      	      \begin{itemize}
	      	      	\item $(\{1\}, \cdot )$ bzw. $(\{0\}, + )$
	      	      	\item $(\{1,-1\}, \cdot)$
	      	      \end{itemize}
	      	      Für weitere endliche Gruppen muss man Restklassen (\hyperref[3.12]{Beispiel 3.12}) Matrizen oder Permutationen betrachten
	      	\item[e)] $\mathscr{S}_2 = \{\id, (12)\}$ und \\
	      	      $\mathscr{S}_3 = \{\id, (12), (23),(13),(123),(132)\}$ sind Gruppen (s. \hyperref[3.11]{3.11})
	      	\item[f)] $V_4 = \{\id, \pi, \sigma, \eta \}$ aus \hyperref[3.7]{Beispiel 3.7} ist die Symmetriegruppe des Rechtecks und heißt 'Kleinsche Vierergruppe' ($V_4$ Gruppe: s. \hyperref[3.16]{3.16} e).
	      \end{itemize}
	      \subsection{Satz (Symmetrische Gruppe)}
	      \label{3.11}
	      $\mathscr{S}_n$ ist eine \uline{nicht} abelsche Gruppe. (Name: Symmetrische Gruppe) \\
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item assoziativ: $\pi, \sigma, \eta \in \mathscr{S}_n \Rightarrow \underbrace{(\pi \cdot \sigma) \cdot \eta}_{\textrm{Verknüpfung von Abbildungen}} = \overset{\textrm{bijektive Abbildungen}}{\pi \overset{\uparrow}{\cdot} (\sigma \overset{\uparrow}{\cdot} \eta)}$
	      	\item Neutralelement: id, denn \\
	      	      $\id \cdot \pi = \pi \cdot \id = \pi\qquad\forall\pi\in\mathscr{S}_n$
	      	\item Inverse: Alle Pfeile eines Zyklus werden umgedreht, d.h. die Zyklen werden rückwärts gelesen:\\
	      	      \begin{tikzpicture}
	      	      			
	      	      	\node (v3) at (-0.5,2) {1};
	      	      	\node (v1) at (0.5,0) {2};
	      	      	\node (v2) at (-1.5,0) {3};
	      	      	\draw [->] (v1) edge (v2);
	      	      	\draw [->] (v2) edge (v3);
	      	      	\draw [->] (v3) edge (v1);
	      	      	\node (v6) at (3.5,2) {1};
	      	      	\node (v5) at (4.5,0) {2};
	      	      	\node (v4) at (2.5,0) {3};
	      	      	\draw [->] (v4) edge (v5);
	      	      	\draw [->] (v5) edge (v6);
	      	      	\draw [->] (v6) edge (v4);
	      	      	\node (v7) at (1,1) {};
	      	      	\node (v8) at (2,1) {};
	      	      	\draw [->] (v7) edge (v8);
	      	      \end{tikzpicture}\\
	      	      $\pi=(123)\qquad\qquad\qquad\pi^{-1}=(132)$\\
	      	      Fixpunkte und 2er-Zyklen ändern sich dabei nicht:\\
	      	      $\sigma = (1678)(23)\Rightarrow\sigma^{-1} = (1876)(23)$\\
	      	      Setzt man die Pfeile von den Graphen $\pi$ und $\pi^{-1}$ zusammen, ändert sich nichts, d.h. $\pi \cdot  \pi^{-1}(i) = i \Rightarrow \pi \cdot  \pi^{-1} = \id = \pi \cdot  \pi^{-1}$
	      	\item nicht abelsch: \hyperref[3.6]{Bemerkung 3.6}d)
	      \end{itemize}\qed\newpage
	      \marginpar{15.11.16}
	      \subsection{Beispiel}	
	      \label{3.12}
	      Restklassen modulo $n:  \mathds{Z}_n = \{0,1,...,n-1\}$, \\
	      \\
	      z. Bsp. $n=3$\\ 
	      \begin{minipage}[c]{0.5\textwidth}
	      	\begin{tikzpicture}[scale=0.5]
	      		\node at (-8,4) {$\mathds{Z}$};
	      		\draw  (-3,4) ellipse (3.5 and 5);
	      		\draw (-6,6) -- (0,6);
	      		\draw (-6,1.5) -- (0,1.5);
	      		\node at (-5,1) {0};
	      		\node at (-4,1) {$\pm$3};
	      		\node at (-3.5,0) {$\pm$6};
	      		\node at (-2.5,0.5) {$\pm$9};
	      		\node at (-1.5,0.5) {...};
	      		\node at (-5.5,5) {1};
	      		\node at (-4,5) {4};
	      		\node at (-2.5,5) {7};
	      		\node at (-5.5,3) {-2};
	      		\node at (-4,3) {-5};
	      		\node at (-2.5,3) {-8};
	      		\node at (-0.5,4) {...};
	      		\node at (-4.5,8) {2};
	      		\node at (-3,8) {5};
	      		\node at (-1.5,8) {8};
	      		\node at (-5,6.5) {-1};
	      		\node at (-3.5,6.5) {-4};
	      		\node at (-2,6.5) {-7};
	      		\node at (-1,7.5) {...};
	      		\draw (1.2588,6.1319) arc (-20.0001:20:4);
	      		\draw (0.5642,1.4288) arc (-40.0003:20:4);
	      		\draw (0.8492,0.8551) arc (20.0013:-20:2.5);
	      		\node at (2.75,7.5) {Rest 2};
	      		\node at (2.75,3.5) {Rest 1};
	      		\node at (2.75,0) {Rest 0};
	      	\end{tikzpicture}
	      \end{minipage}
	      \begin{minipage}[c]{0.5\textwidth}
	      	bei Division durch 3
	      \end{minipage}\\
	      $\mathds{Z}_3 = \{\underbrace{0,\qquad1,\qquad2}_{\textrm{Restklassen, Repräsentanten}}\}$ \\
	      \begin{itemize}
	      	\item[a)] $(\mathds{Z}_n, \oplus)$ mit $a \oplus b = a+b \mod n$. Z.B. in $\mathds{Z}_3$ ist $2 \oplus 1 = 0$ \\
	      	      \\
	      	      \uline{$(\mathds{Z}_n, \oplus)$ ist abelsche Gruppe}:
	      	      \begin{itemize}
	      	      	\item abgeschlossen: $a+b \mod n \in \{0,...,n-1\}$
	      	      	\item assoziativ: $a + (b + c) \mod n = (a + b) + c \mod n$
	      	      	\item Neutralelement: $a + 0 \equiv 0  + a \equiv a\quad\pmod{n}$
	      	      	\item Inverse zu $ a \in \mathds{Z}_n$: Für welches $b \in \mathds{Z}_n$ ist $a+b \mod n = 0$ ?\\
	      	      	      Wähle $b$ so, dass $a+b = n$, falls $a \neq 0$ (sonst b = 0) \\
	      	      	      z.B. in $\mathds{Z}_3: a = 1 \Rightarrow b = 2,\quad a = 2 \Rightarrow b = 1,\quad a = 0, b = 0$
	      	      	\item kommutativ: $a+b \mod n = b +a \mod n$
	      	      \end{itemize}
	      	\item[b)] $(\mathds{Z}_n, \odot)$ mit $a \odot b = ab \mod n$ \\
	      	      Ist i.A. keine Gruppe:
	      	      \begin{itemize}
	      	      	\item assoziativ $\checkmark$
	      	      	\item Neutralelement: $e = 1$ $\checkmark$
	      	      	\item Aber: 0 hat keine Inverse! Es gibt kein $a \in \mathds{Z}_n\colon \underbrace{0 \cdot a \mod n}_{0} = 1$ (\Lightning) \\
	      	      	      Hat $z \neq 0$ eine Inverse bzgl. $\odot$? \\
	      	      	      \\
	      	      	      $\bar{z}$ invers zu $z$, wenn $\bar{z} \cdot z \equiv 1\pmod{n}$ \\
	      	      	      z.B. in $\mathds{Z}_{15}$ gilt: 
	      	      	      \begin{itemize}
	      	      	      	\item $2 \cdot 8 = \quad16 \equiv 1 \pmod{15}$, d.h. 2 und 8 sind zueinander invers
	      	      	      	\item Alle Vielfachen von 5 haben Rest $0,5,10$, d.h. \\
	      	      	      	      $k\cdot 5 \mod 15 \in \{0,5,10 \} \quad\forall k \in \mathds{Z} \Rightarrow 5$ hat kein Inverses
	      	      	      \end{itemize}
	      	      	      Allgemein: 
	      	      	      \begin{align*}
	      	      	      	z \text{ invertierbar } & \Leftrightarrow \exists \bar{z} \in  \mathds{Z}_n: z \odot \bar{z} = 1                                  \\
	      	      	      	                        & \Leftrightarrow \exists \bar{z} \in \mathds{Z}_n\quad \exists q \in \mathds{Z}: \bar{z} \cdot z = qn +1 \\
	      	      	      	                        & \Leftrightarrow \exists \bar{z},q \in \mathds{Z}: \bar{z} \cdot z - qn = 1                              \\
	      	      	      	                        & \overset{*}{\Leftrightarrow} \text{ggT}(z,n)= 1                                                         
	      	      	      \end{align*}
	      	      \end{itemize} 
	      \end{itemize}
	      \subsubsection*{Beweis von *}
	      \begin{itemize}
	      	\item['$\Leftarrow$'] Lemma von Bézout/Erweiterter Euklidischer Algorithmus (EEA): \\
	      	      $a,b \in \mathds{Z} \Rightarrow \exists s,t \in \mathds{Z}: \text{ggT}(a,b)= s \cdot a + t \cdot b$\\
	      	      Hier: $a = z,\quad b = n,\quad s = \bar{z},\quad t = -q$
	      	\item['$\Rightarrow$'] Übung (Übungsblatt 5, A1c)
	      \end{itemize}
	      Also: Nur die zu $n$ teilerfremden Zahlen in $\mathds{Z}_n$ haben Inverse. Z.B.: In $\mathds{Z}_{15}$ sind $1,2,4,7,8,11,13,14$ bzgl. $\odot$ invertierbar. \\
	      \uline{Bezeichnung}: $\mathds{Z}^*_n = \{z \in \mathds{Z}_n \mid \text{ggT}(z,n)= 1\}$ ist Gruppe mit Ordnung $| \mathds{Z}^*_n | = \phi(n)$\\
	      (Eulersche $\phi-$Funktion, $\phi(n)$ ist Anzahl der zu $n$ teilerfremden Zahlen zwischen 1 und $n$).\\
	      \\
	      Berechnung der Inversen in $\mathds{Z}^*_n$: 
	      \begin{align*}		
	      	\text{EEA}:\qquad z \in \mathds{Z}^*_n & \Rightarrow \exists s,t \in \mathds{Z}: sz + tn = 1 \\
	      	                                       & \Rightarrow s \cdot z \equiv 1 \pmod{n}             \\
	      	                                       & \Rightarrow s \text{ invers zu } z                  
	      \end{align*}\\
	      \subsection{Satz (Eigenschaften von Gruppen)} 
	      \label{3.13}
	      $G$ Gruppe.
	      \begin{itemize}
	      	\item[i)] Das Neutralelement von $G$ ist eindeutig. 
	      	\item[ii)] Die Inverse zu jedem $a \in G$ ist eindeutig.
	      	\item[iii)] $a,b \in G \Rightarrow (ab)^{-1} = b^{-1} \cdot a^{-1}$
	      \end{itemize}
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[i)] Angenommen $e_1,e_2$ Neutralelemente \\
	      	      $\Rightarrow e_1 = e_1 \cdot e_2 = e_2$
	      	\item[ii)] Angenommen $a \in G$ hat 2 Inversen $x,y$\\
	      	      $x,y \in G \Rightarrow x = x\underbrace{(ay)}_{e} = \underbrace{(xa)}_e y = y$
	      	\item[iii)] 
	      	      \begin{itemize}
	      	      	\item[$\ast$] $(ab)^{-1} \cdot (ab) \underset{\textrm{Vor.}}{=} (b^{-1}a^{-1})(ab) = b^{-1}\underbrace{(a^{-1}a)}_e b = \underbrace{b^{-1}b}_e = e$
	      	      	\item[$\ast$] $(ab)(ab)^{-1}$ analog
	      	      \end{itemize}
	      	      \hfill$\square$
	      \end{itemize}
	      \subsection{Satz (Gleichungen lösen in Gruppen)}
	      $G$ Gruppe, $a,b \in G$
	      \begin{itemize}
	      	\item[i)] $\exists! x \in G: a \cdot x = b$. \quad Es ist $x = a^{-1} \cdot b$
	      	\item[ii)] $\exists! y \in G: y \cdot a = b$. \quad Es ist $y = b \cdot a^{-1}$
	      	\item[iii)] $ax = bx$ für ein $x \in G \Rightarrow a = b$\\
	      	      bzw. $ya = yb$ für ein $y \in G \Rightarrow a = b$ (Kürzungsregel)
	      \end{itemize}
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[i)] $x = a^{-1}b$ erfüllt $ax = a(a^{-1}b) = \underbrace{(aa^{-1})}_e b = b$
	      	\item[ii)] Analog zu i)
	      	\item[iii)] $a = a\underbrace{(xx^{-1})}_{e} = (ax)x^{-1} = (bx)x^{-1} = b\underbrace{(xx^{-1})}_{e} = b$ 
	      \end{itemize}
	      \hfill$\square$
	      \subsection*{Untergruppen und Nebenklassen}
	      \subsection{Definition (Untergruppe)}
	      $(G, \cdot)$ Gruppe, $\emptyset \neq U \subseteq G$.\\
	      $U$ heißt \uline{Untergruppe} von $G$ $(U \leq G)$, wenn $U$ bzgl. '$\cdot$' eine Gruppe ist.
	      	
	      \subsubsection*{Bemerkung}
	      \marginpar{22.11.16}
	      \begin{itemize}
	      	\item Abgeschlossenheit prüfen: $\forall u,v  \in U: uv \in U$
	      	\item $e$ von $G$ ist auch $e$ von $U$
	      	\item Inversen in $U$ gleich wie in $G$
	      \end{itemize}
	      (wegen \hyperref[3.13]{Satz 3.13})
	      \subsection{Beispiel}
	      \label{3.16}
	      \begin{itemize}
	      	\item[a)] $(\mathds{Z}, +) \leq (\mathds{Q}, + ) \leq (\R, + )$
	      	\item[b)] $(\{-1,1\}, \cdot) \leq (\mathds{Q}\setminus \{0\}, \cdot) \leq (\R \setminus\{0\}, \cdot)$
	      	\item[c)] $V_4 = \{\id,\underbrace{(AB)(CD)}_{\pi},\underbrace{(AC)(BD)}_{\sigma}, \underbrace{(AD)(BC)}_{\eta}\} \leq \mathscr{S}_4$ (Bsp. \hyperref[3.7]{3.7}, \hyperref[3.10]{3.10}) weil $V_4$ abgeschlossen, $id \in V_4,\quad \gamma^{-1} = \gamma \qquad \forall \gamma \in V_4$
	      \end{itemize}
	      \subsection{Beispiel}
	      Es ist $U = 3 \mathds{Z} = \{3k\mid k \in \mathds{Z}\}$ eine Untergruppe von $(\mathds{Z}, +)$.\\
	      \\
	      \begin{minipage}[c]{0.6\textwidth}
	      	\begin{itemize}
	      		\item Mehr Klassen gibt es nicht, da $3\mathds{Z} + 3 = 3\mathds{Z} + 0$, $3\Z+4=3\Z+1$, $3\Z-1=3\Z+2$
	      		\item Repräsentanten sind nicht eindeutig, $-1$ auch Repräsentant von $3 \mathds{Z}  +2 = 3\mathds{Z} - 1$
	      		\item Grundidee: Nebenklassen von $U$ unterteilen $G = \mathds{Z}$ in disjunkte Äquivalenzklassen.\\
	      		      Hier: $x \sim_U y \Leftrightarrow \exists u \in 3 \mathds{Z}: u + x = y$, z.B. $4 \sim_U 10$, da $\underbrace{6}_{\in 3\mathds{Z}} + 4 = 10$
	      	\end{itemize}
	      \end{minipage}
	      \begin{minipage}[c]{0.4\textwidth}
	      	\begin{tikzpicture}[scale=0.5]
	      				
	      		\draw  (-1.5,3) ellipse (2.5 and 4);
	      		\draw (-3.8215,4.522) -- (0.8231,4.522);
	      		\draw (-3.7944,1.5188) -- (0.778,1.5188);
	      		\node at (-1.5,5.5) {$3\Z+0$};
	      		\node at (-1.5,3) {$3\Z+1$};
	      		\node at (-1.5,0.5) {$3\Z+2$};
	      		\draw [->] ;
	      		\draw [->] ;
	      		\draw [->] ;
	      		\node (v5) at (3,3.5) {};
	      		\draw [decorate, decoration={brace, amplitude=5pt}](-4.5,4.5) node (v1) {} -- (-4.5,7);node[sloped, below] {false}
	      		\draw [decorate, decoration={brace, amplitude=5pt}](-4.5,1.5) node (v2) {} -- (v1);
	      		\draw [decorate, decoration={brace, amplitude=5pt}](-4.5,-1) -- (v2);
	      		\node (v4) at (-6.0104,3.2103) {};
	      		\draw (v4) edge node[sloped, below] {Restklassen/Nebenklassen} (v4);
	      		\draw (v5) edge node[sloped, below] {Repräsentanten} (v5);
	      		\draw [->](-0.5,0.5) .. controls (2.5,0) and (3,1.5) .. (3,2);
	      		\draw [->](-0.5,5.5) .. controls (1.5,6) and (2,5) .. (3,4.5);
	      		\draw [->](-0.5,3.5) .. controls (2,3.5) and (1.5,3.5) .. (2.5,3.5);
	      	\end{tikzpicture}
	      \end{minipage}
	      \subsection{Satz + Definition (Rechtsnebenklasse, Repräsentant)}
	      $G$ Gruppe, $U \leq G$. 
	      \begin{itemize}
	      	\item[i)] Für $x,y \in G: x \sim_U y \colon \Leftrightarrow \exists u \in U: ux = y$.\\
	      	      \uline{Behauptung}: $\sim_U$ Äquivalenzrelation.
	      	\item[ii)] $Ux := \{ux \mid u \in U \}$ (mit $x \in G$) heißt \uline{Rechtsnebenklasse} von $U$ in $G$. $x$ heißt \uline{Repräsentant} der Klasse $Ux$ [Linksnebenklassen analog: $xU$]
	      	\item[iii)] $\f{G}{U} := \{Ux\mid x \in G \}$ Menge der Rechtsnebenklassen von $U$ in $G$.\\
	      	      \uline{Behauptung}: $\f{G}{U}$ ist eine disjunkte Zerlegung von $G$ in Äquivalenzklassen $Ux$.
	      \end{itemize}
	      	
	      \begin{tikzpicture}[scale=0.5]
	      		
	      	\draw  (-1,1.5) ellipse (5.5 and 2);
	      	\node (v7) at (-4.4909,3.0058) {};
	      	\node (v8) at (-4.527,-0.0425) {};
	      	\node (v6) at (-2.4618,3.4116) {};
	      	\node (v5) at (-2.4888,-0.3635) {};
	      	\node (v3) at (-0.5228,3.4874) {};
	      	\node (v4) at (-0.4506,-0.4771) {};
	      	\node (v1) at (1.5064,3.2656) {};
	      	\node (v2) at (1.5064,-0.2697) {};
	      	\draw  (v1) edge (v2);
	      	\draw  (v3) edge (v4);
	      	\draw  (v5) edge (v6);
	      	\draw  (v7) edge (v8);
	      	\node at (-5.5,1.5) {$Ue$};
	      	\node at (-3.5,1.5) {$Ux_1$};
	      	\node at (-1.5,1.5) {$Ux_2$};
	      	\node at (0.5,1.5) {$...$};
	      	\node at (2.5,1.5) {$Ux_n$};
	      	\node at (-1.5,6) {Repräsentanten};
	      	\node at (-1.5,4.5) {};
	      	\draw [->](-5.2315,1.7647) .. controls (-5.5,5) and (-3,5) .. (-3,5);
	      	\draw [->](2.5781,1.7557) .. controls (2.5,5) and (0,5) .. (0,5);
	      	\draw [->](-3.3354,1.7828) .. controls (-4,4.5) and (-2.5,4.5) .. (-2.5,4.5);
	      	\draw [->](-1.3222,1.8008) .. controls (-1.5,3.5) and (-1.5,4.5) .. (-1.5,4.5);
	      	\node at (-1,-2.5) {Rechtsnebenklassen};
	      	\draw [->](-3,-1.5) .. controls (-6,-1.5) and (-5.5,0) .. (-5.5,0);
	      	\draw [->](1.5,-1.5) .. controls (3,-1.5) and (3,0) .. (3,0);
	      	\draw [->](1,-1.5) .. controls (1.5154,-0.7401) and (0.9473,-0.5416) .. (0.9473,-0.5416);
	      	\draw [->](-2.6782,-1.2992) .. controls (-3.5801,-0.7401) and (-3.2644,-0.5146) .. (-3.2644,-0.5146);
	      	\draw [->](-1.5509,-1.2541) .. controls (-1.5689,-0.8844) and (-1.5599,-0.6769) .. (-1.5599,-0.6769);
	      \end{tikzpicture}
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[i)]
	      	      \begin{itemize}
	      	      	\item $x \sim_U x$, da $\underbrace{e}_{\in U} \cdot x = x$ (Reflexivität)
	      	      	\item (Symmetrie)\begin{align*}
	      	      	      x \sim_U y &\Rightarrow \exists u \in U: ux = y\\
	      	      	      &\Rightarrow x = \underbrace{u^{-1}}_{\in U} y = x\\ &\Rightarrow y \sim_U x
	      	      	\end{align*} 
	      	      	\item  (Transitivität)\begin{align*}
	      	      	      x \sim_U y,~ y \sim_U z &\Rightarrow \exists u,u' \in U: ux = y,~ u'y = z\\& \Rightarrow u'y = u'(ux) = \underbrace{(u'u)}_{\in U}x = z \\&\Rightarrow x \sim_U z
	      	      	\end{align*}
	      	      \end{itemize}
	      	\item[iii)] 
	      	      \begin{itemize} 
	      	      	\item$Ux = \{ux | u \in U \} = \{y \in G | \underbrace{\exists u : ux = y}_{y \sim_U x} \}= \{y \in G | y \sim_U x \} \Rightarrow Ux$ Äquivalenzklassen von $x  \in G$
	      	      	\item Für je 2 Äquivalenzklassen $Ux, Uy$ gilt: $Ux = Uy$ oder $ Ux \cap Uy = \emptyset$ (wegen Transitivität)
	      	      \end{itemize} 
	      \end{itemize}
	      \subsection{Beispiel}
	      $\mathds{Z}_3 := \f{\mathds{Z}}{3\mathds{Z}} = \{3\mathds{Z} + 0,~ 3\mathds{Z} + 1, ~3\mathds{Z}  +2\}=\{3\Z+3,~3\Z-2,~3\Z+11\}$ \\
	      Man schreibt oft $\mathds{Z}_3 = \{\underline{0}, \underline{1},\underline{2}\}$ (wobei $j = 3\mathds{Z} +j$) oder einfach $\mathds{Z}_3 =\{0,1,2\}$\\
	      Allgemein: $\mathds{Z}_n := \f{\mathds{Z}}{n \cdot \mathds{Z}},~~ n \in \mathds{N}$ \\
	      Beobachtung in $\mathds{Z}_3:$ Ist $x \in \underline{1}, y \in \underline{2},$ dann ist immer $x + y \in\underline{0}$
	      \subsection{Kriterium}
	      \label{3.20}
	      $G$ Gruppe, $U \leq G$.\\ Für je 2 beliebige Klassen, $Ux, Uy ~~(x,y \in G)$ gelte: \\
	      $x' \in Ux, ~y' \in Uy \Rightarrow x' \cdot y' \in U(xy)$
	      \subsection{Definition (Wohldefiniertheit)}
	      \label{3.21}
	      Wenn \hyperref[3.20]{Kriterium 3.20} erfüllt ist, kann man auf $\f{G}{U}$ eine Verknüpfung definieren: \\
	      $\ast: \f{G}{U} \times \f{G}{U} \rightarrow \f{G}{U}$ mit \\
	      $(Ux) \ast (Uy) = U(\underbrace{xy}_{\text{Produkt in $G$}})$\\
	      Man sagt: Wenn \hyperref[3.20]{3.20} erfüllt, ist '$\ast$' \uline{wohldefiniert}.
	      \subsection{Beispiel}
	      \marginpar{23.11.16}
	      \begin{itemize}
	      	\item[a)] $\ast$ wohldefiniert auf $(\mathds{Z}_n, +)$ (ohne Beweis) \\
	      	      \uline{Bemerkung:} $x \sim_U y \Leftrightarrow \exists u \in 3 \mathds{Z}: u +x = y $\\
	      	      \noindent\hspace*{34.5mm}$\Leftrightarrow x \equiv y \pmod{3}$ \\
	      	      Daraus ergibt sich die Def. aus \hyperref[3.12]{Bsp. 3.12} mit $\mathds{Z}_3 = \{0,1,2\}$ und $x \oplus y = x +y \pmod{3}$
	      	\item[b)] $U = \{\id, (12)\} \leq \mathscr{S}_3$. Auf $\f{\mathscr{S}_3}{U}$ ist $\ast$ nicht wohldefiniert (Übung).
	      \end{itemize}
	      \subsection{Satz (Faktorengruppe/Quotientengruppe)}
	      $U \leq G,~~G$ Gruppe.\\ Wenn '$\ast$' aus \hyperref[3.21]{Def 3.21} wohldefiniert, dann ist $(\f{G}{U}, \ast)$ eine Gruppe. \\ (Name: \uline{Quotientengruppe/Faktorengruppe}) \\
	      \uline{Beweis:} Übung. \\
	      \uline{Bemerkung:} $G$ abelsch $\Rightarrow$ '$\ast$' immer wohldefiniert, d.h.
	      $\f{G}{U}$ Gruppe.
	      \subsection{Lemma}
	      $G$ Gruppe, $U \leq G,~~ U$ \uline{endlich} $\Rightarrow |Ux| = |U|\quad \forall x \in G$
	      \subsubsection*{Beweis}
	      $\phi: U \rightarrow Ux,~~ u \mapsto u \cdot x$ bijektiv:
	      \begin{itemize}
	      	\item surjektiv, da $\phi(U) = Ux$
	      	\item injektiv, da $\phi(u_1) = \phi(u_2) \Rightarrow u_1x = u_2x$\\
	      	      \noindent\hspace*{44.5mm}$\overset{\cdot x^{-1}}{\Rightarrow} u_1 = u_2$
	      \end{itemize}
	      $ \Rightarrow |U| = |Ux|$
	      \subsection{Theorem (Lagrange)}
	      $G$ endliche Gruppe, $U \leq G \Rightarrow |U|$ teilt $|G|$ und $| \f{G}{U} | = \frac{|G|}{|U|}$.
	      \subsubsection*{Beweis} Seien $U_{x_1},..., U_{x_q}$ die $q$ verschiedenen Rechtsnebenklassen von $U$ in $G$. \\
	      $\Rightarrow G = \dot{\bigcup}^q_{i=1} Ux_i \Rightarrow |G| =  \sum_{i = 1}^{q}  \underbrace{|Ux_i|}_{=|U|} = q \cdot |U|.$
	      \hfill$\square$
	      \subsection*{Ordnung und zyklische Gruppen}
	      \subsection{Definition (Potenzen)}
	      $(G, \cdot )$ Gruppe, $a \in G$.\\ Definiere $a^0 := e,\quad a^1 := a,\quad \underbrace{a^m := (a^{m-1}) \cdot a}_{\textrm{für }m \in \mathds{N}},\quad a^m := \underbrace{ (a^{-1})^{-m}}_{\textrm{für }m \in \mathds{Z}^-}$\\
	      als \uline{Potenzen} von $a \in G$.
	      \subsection{Satz (Rechenregeln)}
	      $G$ Gruppe, $a \in G$. Es gilt:
	      \begin{itemize}
	      	\item[i)] $(a^{-1})^m = (a^m)^{-1} = a^{-m} \qquad \forall m \in \mathds{Z}$
	      	\item[ii)] $a^ma^n = a^{m+n} \qquad \forall m,n \in \mathds{Z}$
	      	\item[iii)] $(a^m)^n = a^{m \cdot n} \qquad \forall m,n \in \mathds{Z}$
	      \end{itemize}
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[i)] 
	      	      \begin{itemize}
	      	      	\item[a)] $m$ positiv:
	      	      	      \begin{itemize}
	      	      	      	\item Inverses für $a^m$, wenn $m \geq 0:$ \\
	      	      	      	      Es ist $a^m  \cdot \underbrace{(a^{-1})^m}_{\text{Inverse}} = \underbrace{a \cdot a \cdot ... \cdot a}_{\text{m-mal}} \cdot \underbrace{ a^{-1} \cdot ... \cdot a^{-1}}_{\text{m -mal}} = e$ \\
	      	      	      	      $\Rightarrow (a^m)^{-1}= (a^{-1})^m$
	      	      	      	\item nach Definition: $a^{\overbracket{-m}^{\in \mathds{Z^-}}} = (a^{-1})^{+m}$ \\
	      	      	      	      $\Rightarrow$ i) gilt für $m \geq 0$
	      	      	      \end{itemize}
	      	      	\item[b)] $m$ negativ:
	      	      	      \begin{itemize}
	      	      	      	\item $a^{\overbracket{-m}^{ \in \mathds{N}}} = ((\underbrace{a^{-1}}_{\in G})^{-1})^{\overbracket{-m}^{ \in \mathds{N}}} \overset{\textrm{Def.}}{=} (a^{-1})^m$
	      	      	      	\item $a^{\overbracket{m}^{ \in \mathds{Z}^-}} = (a^-1)^{\overbracket{-m}^{ \in \mathds{N}}} \overset{\textrm{a)}}{=} (a^{-m})^{-1}$\\$ \Rightarrow (a^m)^{-1} = ((a^{-m})^{-1})^{-1} = a^{-m}$
	      	      	      \end{itemize}	
	      	      \end{itemize}
	      	\item[ii) + iii)] analog mit $m$ oder $n$ negativ oder positiv 
	      \end{itemize}
	      \subsection{Satz + Definition (Ordnung, zyklische Gruppe)}
	      $G$ endliche Gruppe, $g \in G$. 
	      \begin{itemize}
	      	\item[i)] Es gibt eine kleinste Zahl $n \in \mathds{N}$ mit $g^n = e$.
	      	      $n$ heißt \uline{Ordnung} $\smallO(g)$ von g.
	      	\item[ii)] $\{g^0 = e, g^1, g^2,...,g^{n-1}\} \leq G$ und heißt die von $g$ erzeugte \uline{zyklische Gruppe} $\langle g \rangle.$
	      	\item[iii)] $g^{|G|} = e$
	      \end{itemize}
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[i)] $G$ \uline{endlich} $\Rightarrow \exists i,j \in \mathds{N}: g^i = g^j$ und $i > j$ \\
	      	      \noindent\hspace*{17.5mm}$\Rightarrow g^{\overbracket{i-j}^{ \in \mathds{N}}} = g^i g^{-j} = \underbrace{g^i}_{=g^j} (g^j)^{-1} = e$ \\
	      	      Wähle $n =\min\{k \in \mathds{N}| g^k = e \}$.
	      	\item[ii)] 
	      	      \begin{itemize}
	      	      	\item $\langle g \rangle$ abgeschlossen, da $g^m \cdot g^k = g^{m+k} \in \langle g \rangle$ 
	      	      	\item $g^0 = e \in \langle g \rangle$
	      	      	\item $(g^m)^{-1} = g^{-m} = \underbrace{g^n}_{e} \cdot g^{-m} \in \langle g \rangle$
	      	      \end{itemize}
	      	\item[iii)] Lagrange:\quad $n \mid |G| \Rightarrow n \cdot k = | G |$ für ein $k  \in \mathds{N}$\\
	      	      \noindent\hspace*{32mm}$\Rightarrow g^{|G|} = g^{nk} =\underbrace{(g^n)^k}_{e} = e^k = e$
	      \end{itemize}\hfill$\square$
	      \subsection{Bemerkung}
	      Eine endliche Gruppe heißt zyklisch, falls sie von einem Element erzeugt wird.
	      \subsubsection*{Beispiel}
	      \begin{itemize}
	      	\item $(\mathds{Z}_n, \oplus)$ zyklisch, da $1 \in \mathds{Z}_n$ und $1^2 = 1 + 1 = 2,~~ 1^3 = 1 + 1 +1 = 3,~...,$\\$ 1^n = (1^{n-1}) \cdot 1 = (n -1) + 1 = n\textrm{ und } n \equiv 0 \pmod{n}$ \\
	      	      $\mathds{Z}_n$ hat Ordnung $n$, da $1^n = 0$
	      	\item Drehungen, die ein regelmäßiges $n-$Eck in sich selbst überführen, sind zyklisch: \\
	      	      $(ABC)^0 = id,~~ (ABC) = (ABC),~~ (ABC)^2 = (ACB),~~ (ABC)^3 = id$\\
	      	      $\langle (ABC) \rangle = \{\id, (ABC), (ACB)\} \leq \mathscr{S}_3$
	      	\item $\mathscr{S}_3$ oder $V_4$ nicht zyklisch.
	      \end{itemize}
	      \subsection{Korollar}
	      \begin{itemize}
	      	\item[i)] Satz von Euler:\\ $n \in \mathds{N},~~ a \in \mathds{Z},~~ \text{ggT}(a,n) = 1 \Rightarrow a^{\phi(n)} \equiv 1 \pmod{n}$
	      	\item[ii)] Kleiner Satz von Fermat:\\ $p$ Primzahl, $a \in \mathds{Z},~~ p \nmid a \Rightarrow a^{p-1} \equiv 1 \pmod{p}$
	      \end{itemize}
	      \subsubsection*{Beweis}
	      Wir können annehmen, dass $1 \leq a < n$, denn \\$a^{\phi(n)} \mod n = {\underbrace{(a \mod n)}_{\{1,...,n-1\}}}^{\phi(n)}\mod n$ \\
	      $\Rightarrow a \in \mathds{Z}^*_n \\
	      \mathds{Z}^*_n$ endliche Gruppe $\Rightarrow a^{\overbracket{|\mathds{Z}^*_n|}^{ = \phi(n)}} \equiv 1 \pmod{n}$\\
	      ii) Folgt aus i) für $n = p,~~ \phi(p) = p-1$
	      \hfill$\square$
	      \newpage
	      \section{Ringe und Körper}
	      \subsection*{Grundlegende Eigenschaften}
	      \subsection{Definition (Ring)}
	      Sei $\mathcal{R} \neq \emptyset$ eine Menge mit 2 Verknüpfungen + und $\cdot$.
	      \begin{itemize}
	      	\item[i)] Man nennt $(\mathcal{R}, + , \cdot)$ einen \uline{Ring}, wenn gilt: 
	      	      \begin{itemize}
	      	      	\item[1)] $(\mathcal{R}, + )$ ist abelsche Gruppe mit Neutralelement 0 und Inverse $-a$ von $a$.
	      	      	\item[2)] $(\mathcal{R}, \cdot)$ ist abgeschlossen und assoziativ (Halbgruppe).
	      	      	\item[3)] Distributivgesetze: $a \cdot (b+c) = ab + ac$\\
	      	      	      \noindent\hspace*{32.5mm}$ (a+b) \cdot c = ac + bc \qquad \forall a,b,c \in \mathcal{R}$
	      	      \end{itemize}
	      	      \marginpar{29.11.16}
	      	\item[ii)] $(\mathcal{R},+,\cdot)$ heißt \uline{kommutativ}, falls '$\cdot$' zusätzlich kommutativ ist
	      	\item[iii)] $(\mathcal{R},+,\cdot)$ heißt \uline{Ring mit Eins}, falls es bezüglich '$\cdot$' ein Neutralelement $\upharpoonleft$ gibt mit $a \cdot \upharpoonleft = \upharpoonleft \cdot a = a \qquad \forall a \in \mathcal{R}$.
	      	\item[iv)] Ist $(\mathcal{R},+,\cdot)$ Ring mit Eins, so heißen die bezüglich '$\cdot$' invertierbaren Elemente \uline{Einheiten}. \\
	      	      Bezeichnung: 
	      	      \begin{itemize}
	      	      	\item $a^{-1}$ Inverse von $a$ bzgl. '$\cdot$' \item $\mathcal{R}^* :=$ Menge aller Einheiten in $\mathcal{R}$
	      	      \end{itemize}
	      \end{itemize}
	      \subsection{Beispiel}
	      \label{4.2}
	      \begin{itemize}
	      	\item[a)] Trivialer Ring $(\{0\},+,\cdot)$
	      	\item[b)] $(\mathds{Z}, +,\cdot)$ kommutativer Ring mit Eins. \\
	      	      Einheiten: $1, -1\Rightarrow \underbrace{\mathds{Z}^* = \{-1,1\}}_{\text{kein Ring!}}$\\
	      	      Ebenso $(\mathds{Q},+,\cdot)$ und $(\mathds{R},+,\cdot)$ \\
	      	      mit $\mathds{Q}^* = \mathds{Q} \setminus \{0\}$ und $\mathds{R}^* =  \mathds{R} \setminus \{0\}$
	      	\item[c)] $(2\mathds{Z},+,\cdot)$ Ring, kommutativ, ohne Eins
	      	\item[d)] $n \in \mathds{N}_{\geq 2}: (\mathds{Z}_n, \oplus, \odot)$ kommutativer Ring mit Eins
	      	\item[e)] $(\R^n, + ,\cdot)$ kommutativer Ring mit Eins: ($\cdot$ und + Komponentenweise)\\
	      	      \uline{Bemerkung:} $\mathcal{R}_1,...,\mathcal{R}_n$ Ringe $\Rightarrow \mathcal{R}_1 \times ... \times \mathcal{R}_n$ Ring
	      	\item[f)] $(M_n(\R),+, \cdot)$ (für $n\geq 2$) Ring mit Eins $(= E_n)$. Nicht kommutativ! 
	      \end{itemize}
	      \subsection{Satz (Rechenregeln für Ringe)}
	      \label{4.3}
	      $(\mathcal{R}, +,\cdot)$ Ring, $a,b,c \in \mathcal{R}$
	      \begin{itemize}
	      	\item[i)] $a \cdot 0 = 0 \cdot a = 0$
	      	\item[ii)] $(-a) \cdot b = a \cdot (-b) = -(ab)$
	      	\item[iii)] $(-a)(-b) = ab$
	      \end{itemize}	
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[i)] Es ist $a \cdot 0 = a \cdot (0 + 0)= a \cdot 0 + a \cdot 0$\\
	      	      Addiere $-a \cdot 0:\qquad a \cdot 0 - a \cdot 0 = a \cdot 0 + a \cdot 0 - a \cdot 0 $\\
	      	      \noindent\hspace*{30mm}$\Leftrightarrow \qquad\qquad~0 = a \cdot 0$\\
	      	      Analog: $0 = 0 \cdot a$
	      	\item[ii)] Es ist $(-a)b + ab = \underbrace{(-a+a)}_{=0}b = 0 \cdot b \overset{\textrm{i)}}{=} 0\\
	      	      \Rightarrow (-a)b$ invers zu $ab$ und $(-a)b = -(ab)$ \\
	      	      Analog: $a(-b) = -(ab)$
	      	\item[iii)] $(-a)(-b) \overset{\textrm{ii)}}{=} -(a(-b)) \overset{\textrm{ii)}}{=} -(-(ab)) = ab$
	      \end{itemize}
	      \hfill$\square$
	      \subsection{Bemerkung}
	      \begin{itemize}
	      	\item[a)] $\mathcal{R}$ Ring mit Eins $\Rightarrow 1, -1 \in \mathcal{R}^*$ \\
	      	      Achtung! Z.B. in $(\mathds{Z}_2, \oplus, \odot)$ ist $1 = -1$
	      	\item[b)] In einem kommutativen Ring gilt der binomische Lehrsatz:\\
	      	      $(a+b)^n = \sum_{i = 0}^{n} {n\choose{i}} a^i \cdot b^{n-i}$
	      	\item[c)] In \hyperref[4.3]{4.3}: Rechenregeln für Multiplikation mit additiven Inversen, z.B.: $a\cdot(-b)$ Über Addition mit multiplikativen Inversen keine Aussage möglich (z.B. keine Regel für $a^{-1}+b$ ).
	      \end{itemize}
	      	
	      \subsection{Definition (Körper)}
	      Ein kommutativer Ring mit Eins $(\mathcal{K}, +, \cdot)$ heißt \uline{Körper}, falls $\mathcal{K}^* = \mathcal{K} \setminus \{0\}$. D.h. jedes $x \in \mathcal{K} \setminus \{0\}$ ist bezüglich '$\cdot$' invertierbar.
	      	
	      \subsection{Beispiel}
	      \begin{itemize}
	      	\item[a)] $(\mathds{Q},+,\cdot),\quad (\mathds{R},+,\cdot)$ Körper $[(\mathds{C},+,\cdot)$ auch] \\
	      	      $(\mathds{Z},+,\cdot)$ kein Körper, da $\mathds{Z}^* = \{1,-1\}$.
	      	\item[b)] $\mathds{Z}_n^* = \{z \in \mathds{Z}_n| \text{ggT}(z,n) = 1\}$ Gruppe bezüglich '$\odot$'\\
	      	      $\Rightarrow (\mathds{Z}_n, \oplus, \odot)$ Körper $\Leftrightarrow n$ Primzahl
	      \end{itemize}
	      	
	      \subsection{Satz (Rechenregeln für Körper: Nullteilerfreiheit)}
	      \label{4.7}	
	      $(\mathcal{K},+,\cdot)$ Körper, $a,b \in \mathcal{K}$. Dann gilt
	      \begin{itemize}
	      	\item[a)] alle Rechenregeln für Ringe gelten auch für Körper
	      	\item[b)] $ab = 0 \Leftrightarrow a = 0\vee b = 0$ \qquad [Gegenbeispiel: $(\mathds{Z}_6, \oplus, \odot)$, weil $2 \odot 3 = 0$]
	      \end{itemize}
	      \subsubsection*{Beweis}
	      \begin{itemize}
	      	\item[$'\Leftarrow'$] klar (\hyperref[4.3]{Satz 4.3}i))
	      	\item[$'\Rightarrow'$] $ab = 0$. Angenommen $a \neq 0 \Rightarrow b=1 \cdot b = (a^{-1}a)b = a^{-1}\underbrace{(ab)}_{= 0} \overset{\textrm{\hyperref[4.3]{4.3}i)}}{=} 0$\qed
	      \end{itemize}
	      	
	      \subsection*{Strukturgleichheit von Ringen}
	      \subsection{Definition (Ringhomomorphismus, Ringisomorphismus)}
	      Geg. $(\mathcal{R}, + , \cdot),\quad (\mathcal{R}', \boxplus, \boxdot)$  Ringe
	      \begin{itemize}
	      	\item[i)]$\psi: \mathcal{R} \rightarrow \mathcal{R}'$ heißt \uline{Ringhomomorphismus}, falls $\psi(x+y) = \psi(x) \boxplus \psi(y)$ und $\psi(xy)= \psi(x) \boxdot \psi(y)\quad \forall x,y \in \mathcal{R}$
	      	\item[ii)] Wenn $\psi$ bijektiv ist, heißt $\psi$ \uline{Ringisomorphismus}. In diesem Fall heißen $\mathcal{R}, \mathcal{R}'$ isomorph (d.h. sie sind strukturgleich). Man schreibt $\mathcal{R} \cong \mathcal{R}'$
	      \end{itemize}
	      	
	      \subsection{Beispiel}
	      \begin{itemize}
	      	\item[a)] $\psi: (\mathds{Z}, + ,\cdot) \rightarrow (\mathds{Z}_n,\oplus, \odot)$\\
	      	      \noindent\hspace*{17mm}$x \mapsto x \mod n$\\
	      	      $x +y \rightarrow x+y \pmod{n},\quad x \cdot y \rightarrow x \cdot y \pmod{n}$\\
	      	      $\psi$ Ringhomomorphismus\\
	      	      Nicht injektiv: $\psi(1)=\psi(n+1)=1$
	      	      \marginpar{30.11.16}
	      	\item[b)]  $(\{w,f\}, \text{XOR}, \land) \cong (\mathds{Z}_2, \oplus, \odot)$\\
	      	      Boolsche Algebra, siehe PÜ
	      \end{itemize}
	      \subsection*{Chinesischer Restsatz}
	      \subsection{Bemerkung}
	      \label{4.10}
	      Gegeben: $m_1,...,m_n \in \mathds{N},~~ a \in \mathds{Z},~~ M = m_1 \cdot ... \cdot m_n$ \\
	      \noindent\hspace*{11mm}$\Rightarrow\underbrace{(a \mod M)}_{r} \mod m_i = a \mod m_i \quad\forall i$
	      \subsubsection*{Beweis}
	      Z.z.: $r \equiv a \pmod {m_i}$\\
	      Division mit Rest: 
	      \begin{align*}
	      	\exists q \in \mathds{Z}: a & = qM + r                                                                                   \\
	      	                            & = \underbrace{\Big(q \frac{M}{m_i}\Big)}_{\in \mathds{Z}, \text{ da } m_i \vert M} m_i + r \\
	      	\Rightarrow a               & \equiv r \pmod{m_i}                                                                        \\
	      \end{align*}
	      \qed
	      \subsection{Chinesischer Restsatz}
	      \label{4.11}
	      Gegeben:
	      \begin{itemize}
	      	\item $m_1,...,m_n \in \mathds{N}$ paarweise teilerfremd
	      	\item $M = m_1 \cdot ... \cdot m_n$
	      	\item $a_1,...,a_n \in \mathds{Z}$
	      \end{itemize}
	      Dann existiert $0 \leq x < M$ mit \\
	      	
	      $\begin{rcases}
	      	x &\equiv a_1 \pmod{m_1} \\
	      	x &\equiv a_2 \pmod{m_2} \\
	      	&\vdots \\
	      	x &\equiv a_n \pmod{m_n} \\
	      \end{rcases}$ \uline{Simultane Kongruenz} 
	      	
	      \subsubsection*{Beweis}
	      Es ist ggT$\Big(m_i, \underbrace{\frac{M}{m_i}}_{M_i}\Big) = 1 \qquad \forall i \in \{1,...,n\}$. \\
	      $\overset{\text{EEA}}{\Rightarrow } \exists~ s_i, t_i \in \mathds{Z}: t_i m_i + s_i M_i = 1$ \\
	      	
	      Setze: $e_i := s_i M_i \Rightarrow e_i \equiv \begin{cases*}
	      1 \pmod{m_i} \\
	      0 \pmod{m_j},~ j \neq i\\
	\end{cases*}$ \\
	\\
	$\Rightarrow x \overset{\textrm{\hyperref[4.10]{4.10}}}{=} \sum_{i = 1}^{n}a_i e_i \pmod{M} $ ist Lösung der simultanen Kongruenz.
	\subsection{Beispiel}
	\begin{itemize}
		\item[a)] $m_1 = 3, ~~m_2 = 4, ~~m_3 = 5 \Rightarrow M = 60$\\
		      Finde $x \in [0,60)$ mit $x \equiv \begin{cases*}
		      	2 \pmod 3~~(=a_1)\\
		      	3 \pmod 4~~(=a_2)\\
		      	2 \pmod 5~~(=a_3)\\
		      	\end{cases*}$ \\
		      	Es ist 
		      	\begin{itemize}
		      		\item $M_1 = \frac{M}{m_1} = \frac{60}{3} = 20$ 
		      		\item  $M_2 = \frac{60}{4} = 15$
		      		\item $M_3 = \frac{60}{5} = 12$ 
		      	\end{itemize}
		      	EEA: 
		      	\begin{itemize}
		      		\item $7 \cdot \overbrace{3}^{m_1} + \underbracket{(-1) \cdot \overbrace{20}^{M_1}}_{e_1} = 1$
		      		\item $4 \cdot \overbrace{4}^{m_2} + \underbracket{(-1) \cdot \overbrace{15}^{M_2}}_{e_2} = 1$
		      		\item $5 \cdot \overbrace{5}^{m_3} \underbracket{+ (-2) \cdot \overbrace{12}^{M_3}}_{e_3} = 1$
		      	\end{itemize}
		      	$\Rightarrow x = [2 \cdot (-20) + 3 \cdot (-15) + 2 \cdot (-24)] \mod 60  =-133\mod 60= 47$
		      	\item[b)] Was ist $2^{1000} \mod \underbrace{1155}_{=\underset{m_1}{3} \cdot \underset{m_2}{5} \cdot \underset{m_3}{7} \cdot \underset{m_4}{11}}$ ?
		      	\begin{itemize}
		      		\item[1)] Berechne $2^{1000} \mod 3,5,7$ und $11$
		      		      \begin{itemize}
		      		      	\item $2^{1000} \mod 3 = (-1)^{1000} \mod 3 = 1=a_1$
		      		      	\item $2^{1000} \mod 5 = 4^{500} \mod 5 = (-1)^{500} = 1=a_2$
		      		      	\item $2^{1000} \mod 7 = \underbracket{2^3}_{=8}~^{\*333+1} \mod 7 = 1 \cdot 2\mod 7 = 2=a_3$
		      		      	\item $2^{1000} \mod 11 = \underbracket{2^5}_{=32}~{\*200} \mod 11 = (-1)^{200} = 1=a_4$
		      		      \end{itemize}
		      		\item[2)] Suche $0 \leq x < 1155$ mit $x \equiv \begin{cases*}
		      		      1 \pmod{3}\\
		      		      1 \pmod {5}\\
		      		      2 \pmod {7}\\
		      		      1 \pmod{ 11}\\
		      		\end{cases*}$\\
		      		Chinesischer Restsatz: $x = 331$
		      	\end{itemize}
		      	\end{itemize}
		      	\subsection{Satz (Eindeutigkeit Chines. Restsatz)}
		      	\label{4.13}
		      	Die Lösung $x$ aus \hyperref[4.11]{4.11} ist eindeutig.
		      	\subsubsection*{Beweis}
		      	Z.z.: $\psi : \mathds{Z}_M \rightarrow \mathds{Z}_{m_1} \times ... \times \mathds{Z}_{m_n},~~ x \mapsto (x \mod m_1,...,~x \mod m_n)$ ist bijektiv (Ringisomorphismus)
		      	\begin{itemize}
		      		\item $\psi$ Ringhomomorphismus:
		      		      \begin{align*}
		      		      	\psi (x \oplus y) & = \psi(x+y \mod M)                                                              \\
		      		      	                  & = ((x+y \mod M) \mod m_1,..., (x+y \mod M) \mod m_n)                            \\
		      		      	                  & \overset{\textrm{\hyperref[4.10]{4.10}}}{=~} (x+y \mod m_n,~..., ~x+y \mod m_n) \\
		      		      	                  & = \psi(x) \oplus \psi(y)                                                        
		      		      \end{align*}
		      		      Analog mit $\psi(x \odot y) = \psi(x) \odot \psi(y)$
		      		\item $\psi$ surjektiv:\\
		      		      Zu jedem $n$-Tupel aus $\underbrace{\mathds{Z}_{m_1} \times ... \times \mathds{Z}_{m_n}}_{\ni(a_1,...,a_n)}$ gibt es Lösung $x \in \mathds{Z}_M$ (\hyperref[4.11]{4.11}).
		      		\item $\psi$ injektiv: \\
		      		      Da $| \mathds{Z}_M| = | \mathds{Z}_{m_1} \times ... \times \mathds{Z}_{m_n}| \Leftrightarrow M = m_1 \cdot ... \cdot m_n$ \\
		      		      D.h. kein Element word doppelt 'getroffen' \\
		      	\end{itemize}
		      	$\Rightarrow \psi$ bijektiv, also Isomorphismus 
		      	\qed
		      	\subsection{Beispiel}
		      	Gilt $\phi (a \cdot b) = \phi(a) \cdot \phi(b)$ ? Nein.\\
		      	z.B. $\underbrace{\mathds{Z}_2^* = \{1\}}_{\phi(2) = 1},~~~ \underbrace{\mathds{Z}_4^* = \{1,3\}}_{\phi(4)= 2}$\\
		      	Aber: $\mathds{Z}_8^* =\{1,3,5,7\}$ und $4 = \phi(8) \neq \phi(2) \cdot \phi(4)$
		      	\subsection{Korollar}
		      	\begin{itemize}
		      		\item$M = m_1 \cdot .... \cdot m_n$ mit $m_i$  paarweise teilerfremd und $m_i \in M$ \\
		      		$\Rightarrow \phi(M) =\phi(m_1) \cdot ... \cdot \phi(m_n)$
		      		\item Insbesondere:\\ $M = p_1^{a_1} \cdot... \cdot p_k^{a_k},~~~ p_i \in \mathds{P}\textrm{ (Primzahl)},~ p_i \neq p_j$ für $i \neq j,~~ a_i \in \mathds{N}$\\
		      		      $\Rightarrow \phi(M) = (p_1 - 1)p_1^{a_1 - 1} \cdot ... \cdot (p_k - 1)p_k^{a_k - 1}$
		      	\end{itemize}
		      	\subsubsection*{Beweis}
		      	Wegen \hyperref[4.13]{4.13} ist $\mathds{Z}_M \cong \mathds{Z}_{m_1} \times ... \times \mathds{Z}_{m_n}$ mittels $\psi$.\\
		      	$\Rightarrow x$ Einheit $\Leftrightarrow \psi(x)=(x\mod m_1,...,~x\mod m_n)$ Einheit\\
		      	\noindent\hspace*{22mm}$\Leftrightarrow x \mod m_i \text{ Einheit } \forall i$
		      	$\Rightarrow \phi(M) = \phi(m_1) \cdot ... \cdot \phi(m_n)$\\
		      	Es ist $\phi(p^a) = \underbrace{p^a}_{|\mathds{Z}_{p^a}|} - \underbrace{p^{a-1}}_{\text{Vielfache von p in }\mathds{Z}_{p^a}} = (p-1)p^{a-1}$ \\
		      	\begin{tabular}{l | l | l | l }
		      		$a$ & $|\mathds{Z}_{p^a}|$ & Vielfache von $p$                                                            & $\phi(p^a) = |\mathds{Z}_{p^a}^*|$ \\ \hline 
		      		1   & $p$                  & $0 \cdot p = 0$                                                              & $p-1 = p^1 - p^0$                  \\
		      		2   & $p^2$                & $k\cdot p,~ \underbrace{0\leq k \leq p -1}_{p\text{ Möglichkeiten}}$        & $p^2 - p^1$                        \\
		      		3   & $p^3$                & $kp + k'p^2,~ \underbrace{0 \leq k,k' \leq p-1}_{p^2\text{ Möglichkeiten}}$ & $p^3 - p^2$                        \\
		      	\end{tabular}~~~ \qed
		      	\subsection*{Polynomringe}
		      	\marginpar{06.12.16}
		      	In Mathe I wurde für den Ring $(\mathds{Z},+,\*)$ folgendes eingeführt:
		      	\begin{itemize}
		      		\item Division mit Rest
		      		\item Erweiterter Euklidischer Algorithmus
		      		\item kgV, ggT, Primzahlzerlegung
		      	\end{itemize}
		      	\subsection{Definition (Polynom)}
		      	$\mathcal{K}$ - Körper mit Nullelement $\mathcal{O}$ und Einselement $\upharpoonleft$.
		      	\begin{itemize}
		      		\item[i)] Ein \uline{Polynom} über $\mathcal{K}$ ist ein Ausdruck $f = \underbrace{a_0x^0}_{a_0} + \underbrace{a_1x^1}_{a_1x} + ... + a_nx^n $ mit $ n \in \N, ~~a_i \in \mathcal{K}$ Koeffizienten von $f$ (auch $f(x)$ anstatt $f$).\\
		      		      Ist $a_i = 0 \quad\forall \{1,...,n\}$, so schreibt man $f= 0$ (Nullpolynom)
		      		\item[ii)] $\mathcal{K}[x]$ = Menge aller Polynome über $\mathcal{K}$ in einer Variablen $x$
		      		\item[iii)] $f,g \in \mathcal{K}[x]$ sind gleich, wenn gilt
		      		      \begin{itemize}
		      		      	\item[a)]  $f = a_0 + ... + a_n x^n$ \\
		      		      	      $g = b_0 +...+ b_mx^m$ mit $a_n \neq 0, ~b_m \neq 0$ \\
		      		      	      $\Rightarrow m = n$ und $a_i = b_i ~~\forall i = 1,...,n$\\
		      		      	      oder
		      		      	\item[b)] $f= 0$ und $g= 0$
		      		      \end{itemize}
		      	\end{itemize}
		      	\subsection{Beispiel}
		      	\begin{itemize}
		      		\item[a)] $f(x) = f = 3x^2 - \frac{2}{3}x +1  \substack{\in\mathds{Q}[x]\\ \in \mathds{R}[x]}$
		      		\item[b)] $g = x^7 + x^2 \in \mathds{Z}_2[x]$, d.h. Koeffizienten $\in \{0,1\}$
		      	\end{itemize}
		      	\subsection{Satz + Definition (Polynomring)}
		      	$\mathcal{K}$ Körper.\\
		      	$\mathcal{K}[x]$ ist kommutativer Ring mit Eins. Dabei ist für $f = \sum_{i=0}^{n}a_ix^i,$\\$g = \sum_{j = 0}^{m} b_j x^j$
		      	\begin{itemize}
		      		\item $f+g = \sum_{i = 0}^{\max\{m,n\}} (a_i + b_i)x^i$
		      		\item $f \cdot g  = (a_0 + a_1x + ... + a_nx^n)(b_0 + b_1x + ... + b_mx m)$\\
		      		      \noindent\hspace*{8.5mm}$= \underbrace{a_0 \cdot b_0}_{c_0} + \underbrace{(a_0 \cdot b_1 + a_1 \cdot b_0)}_{c_1}x + ... + \underbrace{a_nb_m}_{c_{n+m}}x^{n+m}$\\
		      		      mit $c_i = \sum_{k = 0}^{i}a_k \cdot b_{i-k}$ (\uline{Faltungsprodukt})\\
		      		      $[\textrm{Anmerkung: }a_i = 0 = b_j$ für $i>n\textrm{ bzw. }j>m]$	
		      		\item Einselement: $f=1$
		      		\item Nullelement $f = 0$
		      	\end{itemize}
		      	$\mathcal{K}[x]$ heißt der \uline{Polynomring} in einer Variablen über $\mathcal{K}$.\\
		      	\subsubsection*{Beweis}
		      	Ringeigenschaften nachrechnen \qed
		      	\subsection{Bemerkung}
		      	\begin{itemize}
		      		\item $a_0,~a_1x,~a_2x^2,...,~a_nx^n$ heißen \uline{Monome}
		      		\item $a_nx^n$ heißt \uline{Leitterm} von $f= a_0 + ... + a_nx^n$ mit $ a_n \neq 0$
		      	\end{itemize}
		      	\subsection{Beispiel}
		      	In $\mathds{Z}_3[x]$: \\
		      	$f = 2x^3 +1,~~~ g = x-1 = x +2$, da $-1 \equiv 2\pmod 3$
		      	\begin{itemize}
		      		\item $f+g = 2x^3 + x + \underbrace{1+2}_{\equiv 0 \pmod 3} = 2x^3 + x$
		      		\item $f \cdot g = (2x^3 + 1)(x+2) = 2x^4 + x + \underbrace{4x^3}_{\equiv 1\pmod{3}} + 2 = 2x^4 + x + x^3 + 2$
		      	\end{itemize}
		      	\subsection*{Grad eines Polynoms}
		      	\subsection{Definition (Grad)}
		      	$f \in \K[x], ~~f = a_0 + ... + a_nx^n  \qquad a_n \neq 0$\\
		      	$n$ heißt der \uline{Grad} von $f$, $\grad{f}= n$\\
		      	$\grad{0}= -\infty,~~~\grad{g}= 0$, falls $g$ konstant
		      	\subsection{Satz (Grad verknüpfter Funktionen)}
		      	\label{4.22}
		      	$\K$ Körper, $f,g \in \K[x]$.\\
		      	$\Rightarrow$ $\grad{f\*g} = \grad{f}+\grad{g}$ \\
		      	\uline{Konvention:} \qquad$-\infty - \infty = -\infty = -\infty + n = - \infty$
		      	\subsubsection*{Beweis}
		      	\begin{itemize}
		      		\item Stimmt für $f = 0 $ oder $g = 0$
		      		\item Angenommen die Leitterme von $f$ bzw. $g$ sind $a_nx^n$ bzw. $b_mx^m$ mit\\ $a_n \neq 0,~~ b_m \neq 0$.\\
		      		      $\Rightarrow$ $\grad{f} = n,~~\grad{g} = m$ und $\underbrace{a_n \cdot b_m x^{n+m}}_{\neq0\textrm{, da }\K\textrm{ Körper (\hyperref[4.7]{4.7})}}$ ist Leitterm von $f \cdot g $\\$
		      		      \Rightarrow \grad{fg} = n+m$ \qed
		      	\end{itemize}
		      	\subsection{Korollar (Inversen in $\K[x]$)}
		      	$\K[x]^* = \{f \in \K[x]\mid\grad{f}= 0 \}$ (nur konstante Polynome $\neq 0$ invertierbar)
		      	\subsubsection*{Beweis}
		      	$f \cdot f^{-1} = 1 \Rightarrow\grad{ff^{-1}}=\grad{f}+\grad{f^{-1}}\overset{\textrm{\hyperref[4.22]{4.22}}}{=}\grad{1}=0$\\
		      	\noindent\hspace*{19mm}$\Leftrightarrow\grad{f}=\grad{f^{-1}}=0$ \qed
		      	\subsection*{Polynomdivision mit Rest}
		      	\subsection{Bemerkung}
		      	Für $b \in \K$ ist $f(b) = \sum_{i = 0}^{n} a_i \cdot b^i$, falls $f = \sum_{i = 0}^{n} a_i \cdot x^i \in \K[x]$.\\
		      	Man kann zeigen, dass $\psi_b : \K[x] \rightarrow \K$\\
		      	\noindent\hspace*{52mm}$ f \mapsto f(b)$ ein surjektiver Homomorphismus ist.
		      	\subsection{Definition (Teilbarkeit)}
		      	$\K$ Körper, $f,g \in \K[x].$\\$ f \vert g,$ falls $q \in \K[x]$ existiert mit $g = qf$ (nach \hyperref[4.22]{4.22}: $\grad{f}\leq\grad{g}$).
		      	\subsection{Satz (Division mit Rest in $\K[x])$}
		      	$\K$ Körper, $f \in \K[x], ~~0 \neq g \in \K[x]$.\\
		      	Dann existieren eindeutig bestimmte Polynome $q,r \in \K[x]$ mit $f = qg + r$ und $\grad{r}<\grad{g}$.\\
		      	\uline{Bezeichnung}: $r = f \mod g,~~~ q = f \text{ div } g$
		      	\subsubsection*{Beweis}
		      	vgl. Mathe I für $\mathds{Z}$, Literatur \qed
		      	\subsection{Beispiel}
		      	$f = x^4 + 2x^3 - x +2$ und $ g = 3x^2-1 \in \mathds{Q}[x]$\\
		      	$\polylongdiv[style=C, div=:]{x^4+2x^3-x+2}{3x^2-1}$\\
		      	Mit $\frac{1}{3}x^2+\frac{2}{3}x+\frac{1}{9}=q$ und $-\frac{1}{3}x+\frac{19}{9}=r$ (Rest).\\
		      	Aufhören bei $\grad{r}<\grad{g}$!
		      	\subsection{Korollar}
		      	$\K$ Körper, $a \in \K,~~ f \in \K[x]$\\
		      	\\
		      	$\underbrace{(x -a)}_{\textrm{teilt f restlos}}\vert f \Leftrightarrow f(a) = 0$
		      	\marginpar{07.12.16}
		      	\subsubsection*{Beweis}
		      	\begin{itemize}
		      		\item[($\Rightarrow$)] $\exists q \in \K[x]: f = q(x-a) \Rightarrow f(a) = q(a)\underbrace{(a-a)}_{0} = 0 $
		      		\item[$(\Leftarrow)$] Division mit Rest: $f = q(x-a) +r$, $\grad{r}<\grad{x-a}$\quad (da $q\vert f$)\\
		      		      \noindent\hspace*{27mm}$\Rightarrow$ $\grad{r} \leq 0$, d.h. $r = c \neq 0$ konstant oder $r = 0$\\
		      		      \noindent\hspace*{32mm}$0 = f(a) = q(a) \underbrace{(a-a)}_{= 0} + r(a) \Rightarrow r = 0 $\qed
		      	\end{itemize}
		      	\subsection*{Euklidischer Algorithmus in $\K[x]$}
		      	\subsection{Definition (Normiertheit)}
		      	$\K$ Körper. 
		      	\begin{itemize}
		      		\item[i)] $f = a_0 + ... + a_n x^n \in \K[x],~~ a_n \neq 0$ heißt \uline{normiert}, wenn $a_n = 1$
		      		\item[ii)] $g,h \in \K[x],~~~g,h$ nicht beide 0.\\ $f = \text{ggT}(g,h),$ falls $f\in \K[x]$ normiertes Polynom von maximalem Grad ist, das $g$ und $h$ teilt.
		      		\item[iii)] $g,h \in \K[x] \setminus \{0\}.$\\$ f = \text{kgV}(g,h),$ falls $f \in  \K[x]$ ein normiertes Polynom von minimalem Grad ist, das von $g$ und $h$ geteilt wird.
		      	\end{itemize}
		      	\subsection{Bemerkung}
		      	\label{4.30}
		      	\begin{itemize}
		      		\item[a)] $g = x,~~ h = x+1 \in \mathds{Q}[x]$
		      		      \begin{itemize}
		      		      	\item $g \vert x(x+1),~~~ h \vert x(x+1)$
		      		      	\item $g \vert 2x(x+1), ~~~h \vert 2x(x+1)$
		      		      	\item kgV$(g,h) = x(x+1)  = x^2 + x$, da $2x^2 +2x$ nicht normiert!\\
		      		      	      $\rightarrow$ Normierung macht Ergebnisse eindeutig. 
		      		      \end{itemize}
		      		\item[b)] Normierung erfolgt, indem man durch Koeffizienten des Leitterms 'teilt': \\
		      		      $f = a_n x^n + ... + a_0 \Rightarrow a_n^{-1} \cdot f = \underbrace{x^n + ... + a_n^{-1}a_0}_{\text{normiert}}$
		      		\item[c)] kgV$(g,h)$ existiert und ist eindeutig.
		      		      \begin{itemize}
		      		      	\item Existenz: $g \vert gh,~~~ h \vert gh$ ($gh$ gemeinsames Vielfaches)
		      		      	\item Eindeutig : $f_1 = \text{kgV}(g,h), ~~~f_2 = \text{kgV}(g,h)\\
		      		      	      \Rightarrow g,h \vert f_1$ und $g,h \vert f_2\\
		      		      	      \Rightarrow g,h \vert (f_1 - f_2)$\\
		      		      	      $f_1,f_2$ normiert und von gleichem (minimalen) Grad.\\
		      		      	      $\Rightarrow \text{grad }(f_1- f_2) < \text{grad } (f_1) \\
		      		      	      \Rightarrow f_1 - f_2 = 0$, denn sonst wäre kgV$(g,h) = f_1 - f_2$  \qquad \Lightning zur Minimalität des Grades\\
		      		      	      $\Rightarrow$ kgV eindeutig.
		      		      \end{itemize}
		      		\item[d)] ggT$(g,h)$ existiert und ist eindeutig. Beweis folgt wie in Mathe I für $\mathds{Z}$ aus:
		      	\end{itemize}
		      	\subsection{Lemma von Bézout}
		      	$g,h \in \K[x]$ nicht beide gleich 0.\\
		      	$\Rightarrow \exists s,t \in \K[x]: sg + th = \text{ggT}(g,h)$ \\
		      	\subsubsection*{Beweis} Siehe \hyperref[4.33]{4.33} (EEA).
		      	\subsubsection*{Beweis Eindeutigkeit von ggT}
		      	$f =\text{ggT}(g,h), ~~~f' = \text{ggT}(g,h)$\\
		      	($f,f'$ Funktionen desselben Grades und normiert)\\$
		      	\Rightarrow \exists s',t' \in \K[x]: f' = s' \cdot g + t' \cdot h\\
		      	\\
		      	f \vert g \land f \vert h \Rightarrow f \vert f' \\
		      	\Rightarrow \exists q \in \K[x] : f' = qf\\
		      	\Rightarrow \text{grad}(f') = \text{grad } (q) + \text{grad } (f)\\
		      	\\
		      	\text{grad } (f)= \text{grad } (f') \Rightarrow \text{grad }(q) = 0 \\
		      	\text{grad } (q) = 0 \Rightarrow q = c \neq 0,~~ c \in \K \\
		      	\Rightarrow f' = cf\\
		      	f,f'$ normiert $\Rightarrow c = 1$\qed
		      	\subsection{Satz (Euklidischer Algorithmus EA in $\K[x]$)}
		      	\begin{algorithmic}[1]
		      		\algrenewcommand\algorithmicrequire{\textbf{Eingabe:}}
		      		\algrenewcommand\algorithmicensure{\textbf{Ausgabe:}}
		      		\Statex
		      		\Require $g,h\in\K[x]$, nicht beide gleich 0
		      		\If{$h=0$} \State$y\coloneqq g$ \EndIf
		      		\If{$h\vert g$} \State$y\coloneqq h$ \EndIf
		      		\If{$h\neq0\wedge h\nmid g$}
		      		\State $x\coloneqq g,~~y\coloneqq h$
		      		\While{$(x\mod y)\neq0$}
		      		\State $r\coloneqq x\mod y$
		      		\State $x\coloneqq y,~~y\coloneqq r$
		      		\EndWhile
		      		\EndIf
		      		\State $d\coloneqq a^{-1}_ny$ (Normierung von $y$, siehe \hyperref[4.30]{4.30})
		      		\Ensure $d=\ggT{g,h}$
		      	\end{algorithmic}
		      	\subsubsection*{Beweis}
		      	Wie für $\Z$ in Mathe I.\\
		      	Hinweis: $d\vert g$ und $d\vert h\Leftrightarrow d\vert (g\mod h)$ und $d\vert h$.\\
		      	Begründung: $g=qh+(g\mod h)$.
		      	\subsection{Satz (Erweiterter Euklidischer Algorithmus EEA  in $\K[x]$)}
		      	\label{4.33}
		      	\begin{algorithmic}[1]
		      		\algrenewcommand\algorithmicrequire{\textbf{Eingabe:}}
		      		\algrenewcommand\algorithmicensure{\textbf{Ausgabe:}}
		      		\Statex
		      		\Require $g,h\in \K[x]$, nicht beide gleich 0
		      		\If{$h=0$}
		      		\State $y\coloneqq g,~~s\coloneqq 1,~~t\coloneqq0$
		      		\EndIf
		      		\If{$h\vert g$}
		      		\State $y\coloneqq h,~~s\coloneqq 0,~~t\coloneqq1$
		      		\EndIf
		      		\If{$h\neq0\wedge h\nmid g$}
		      		\While{$(x\mod y)\neq0$}
		      		\State $q\coloneqq x\div y,~~r\coloneqq x\mod y$
		      		\State $s\coloneqq s_1-qs_2,~~t\coloneqq t_1-qt_2$
		      		\State $s_1\coloneqq s_2,~~s_2\coloneqq s$
		      		\State $t_1\coloneqq t_2,~~t_2\coloneqq t$
		      		\State $x\coloneqq y,~~y\coloneqq r$
		      		\EndWhile
		      		\EndIf
		      		\State $d\coloneqq a^{-1}_ny$ (Normierung von $y$, siehe \hyperref[4.30]{4.30})
		      		\State $s\coloneqq a^{-1}_ns,~~t\coloneqq a^{-1}_nt$ (Normierung von $s,t$, siehe \hyperref[4.30]{4.30})
		      		\Ensure $d=\ggT{g,h},~~~s,t$ für $\ggT{g,h}=sh+tg$
		      	\end{algorithmic}
		      	\subsection{Beispiel}
		      	$g = x^4 + x^3 + 2x^2 +1,~~~ h = x^3 + 2x^2 + 2\qquad  g,h \in \mathds{Z}_3[x]$\\			
		      	\begin{tabular}{c | c | c | c | c | c | c | c | c | c }
		      		$x$      & $y$                                           & $s_1$ & $s_2$  & $s$    & $t_1$  & $t_2$  & $t$    & $q$   & $r$      \\ \hline
		      		g        & h                                             & 1     & 0      &        & 0      & 1      &        &       &          \\ 
		      		h        & $x^2 +x$                                      & 0     & 1      & 1      & 1      & $2x+1$ & $2x+1$ & $x+2$ & $x^2 +x$ \\
		      		$x^2 +x$ & $\underbrace{2x+2}_{\textrm{ggT unnormiert}}$ & 1     & $2x+2$ & $2x+2$ & $2x+1$ & $x^2$  & $x^2$  & $x+1$ & $2x +2$  \\
		      	\end{tabular} \\	
		      	\subsubsection*{Nebenrechnung} \uline{Achtung:} Polynomdivision in $\mathds{Z}_3[x]$, nicht normale Polynomdivision!\\
		      	\begin{itemize}
		      		\item ~\\
		      		      $\begin{array}{r@{}c@{}r@{}c@{}r@{}c@{}r@{}c@{}rl}
		      		      	(x^4 & + & x^3 & + & 2x^2 &   &   & + & 1) & :  ~ (x^3+2x^2+2) = \overbrace{x+2}^{q} \\
		      		      	{}-x^4 &- & 2x^3 &  & &-&2x \\ \cline{1-7} 
		      		      	&  & 2x^3 & + & 2x^2 & + & x&+&1 \\
		      		      	&- &2x^3 & - & x^2 &  & &-&1 \\ \cline{2-9}
		      		      	     &   &     &   & x^2  & + & x &   &    & \qquad (=r)                             
		      		      \end{array}$
		      		\item ~\\
		      		      $\begin{array}{r@{}c@{}r@{}c@{}r@{}c@{}r@{}c@{}rl}
		      		      	(x^3 & + & 2x^2 &   &    & + & 2) & ~ :  ~ (x^2+x) =x+1 \\
		      		      	{}-x^3&-&x^2\\ \cline{1-3}
		      		      	&&x^2&&&+2\\
		      		      	&-&x^2&-&x\\\cline{2-5}
		      		      	     &   &      &   & 2x & + & 2  & \qquad (=r)         
		      		      \end{array}$
		      		\item $ t = 1-(x+1)(2x+1) = 1-(2x^2 + 1) = x^2$ \\
		      		\item ~\\
		      		      $\begin{array}{r@{}c@{}r@{}c@{}r@{}c@{}r@{}c@{}rl}
		      		      	(x^2 & + & x) & ~ :  ~ (2x+2) =2^{-1}x \\
		      		      	{}-x^2&-&x\\ \cline{1-3}
		      		      	&&0
		      		      \end{array}$
		      		\item Normierung von y: \\
		      		      \begin{align*}
		      		      	  & d = a_n^{-1} y = 2^{-1}(2x +2)                     \\&~~= x +1 \\
		      		      	  & s = 2^{-1}(2x +2) = x+1                            \\
		      		      	  & t = 2^{-1} \cdot x^2 = 2x^2,\text{ da } 2^{-1} = 2 
		      		      \end{align*}
		      		\item Probe: \\
		      		      \begin{align*}
		      		      	d = sg + th & =(x+1)(x^4 + x^3 + 2x^2 + 1) + (2x^2)(x^3 + 2x^2 + 2)         \\
		      		      	            & = x^5 + x^4 + 2x^3 + x + x^4 + x^3 + 2x^2 + 1 + 2x^5 x^4+ x^2 \\
		      		      	            & = 3x^5 + 3x^4 +3x^3 + 3x^2 + x+1                              \\
		      		      	            & = 0x^5 + 0x^4 +0x^3 + 0x^2 + x+1                              \\ 
		      		      	            & = x+1 = \text{ggT}(g,h)                                       
		      		      \end{align*}
		      	\end{itemize}
		      	\subsection*{Primelemente in $\K[x]$}
		      	\uline{Primelemente} sind Polynome, die sich nicht als Produkt von zwei Polynomen vom Grad $\geq 1$ darstellen lassen. So ist z.B. $2x^2 + 2x = 2x(x+1)$ kein Primelement, jedoch sind die Faktoren $2x$ und $x+1$ Primelemente.
		      	\subsection{Definition (Primelemente = irreduzible Polynome)}
		      	\marginpar{13.12.16}
		      	$p \in \K[x]$ mit $\grad{p}\geq 1$ heißt \uline{irreduzibel}, falls gilt: 
		      	\begin{center}
		      		$\forall f,g \in \K[x]: p = f \cdot g$ ist $\grad{f} = 0$ oder $\grad{g} = 0$
		      	\end{center}
		      	\subsection{Beispiel}
		      	\begin{itemize}
		      		\item[a)]
		      		      $x+1,~~ 2x \in \R[x]$ irreduzibel.\\
		      		      Allg.: $ax +b \quad (a\neq 0)$ irreduzibel in $\K[x]$
		      		\item[b)]
		      		      $x^2 - 2 \in \mathds{Q}[x]$ ist irreduzibel: \\
		      		      Angenommen nicht, dann $x^2 - 2 = \underbrace{(ax + b)}_{\text{Nullstelle:} -\frac{b}{a}} \underbrace{(cx+d)}_{\text{Nullstelle:} -\frac{d}{c}}\qquad(a,c\neq0) \\
		      		      \Rightarrow x^2 -2$ hat auch Nullstelle $-\frac{b}{a} \in \mathds{Q}$~\Lightning\\
		      		      Widerspruch: Nullstelle von $x^2 -2$ sind aus $\R$
		      		\item[c)] $x^2 -2 \in \R[x]$ nicht irreduzibel: \\
		      		      $x^2 - 2 = \underbrace{(x-\sqrt{2})}_{\in \R[x]} \cdot \underbrace{(x+\sqrt{2})}_{\in \R[x]}$
		      		\item[d)] $x^2+1$ hat in $\R$ keine Nullstelle und ist somit irreduzibel in $\R[x]$. \\
		      		      \uline{Anmerkung:} In $\mathds{C}[x]$ ist $x^2 +1$ kein Primelement (siehe \hyperref[5]{Kapitel 5})
		      		\item[e)]
		      		      $x^2 +1 = (x+2)(x+3)$ in $\mathds{Z}_5[x]$\\
		      		      $\rightarrow$ nicht irreduzibel in $\mathds{Z}_5[x]$ 
		      	\end{itemize}
		      	\subsection{Satz (Irreduzibles Polynom)}
		      	$f \in \K[x], ~~\grad{f} \geq 1$. Dann sind äquivalent: 
		      	\begin{itemize}
		      		\item[(1)] $f$ irreduzibel
		      		\item[(2)] $g,h \in \K[x], f \vert g\cdot h \Rightarrow f \vert g \lor f\vert h$
		      	\end{itemize}
		      	\subsubsection*{Beweis}
		      	\begin{itemize}
		      		\item[(1)$\Rightarrow$ (2)] 
		      		      \begin{align*}
		      		      	\text{Angenommen }f \nmid g & \overset{(1)}{\Rightarrow} \text{ggT}(f,g) = 1                                                     \\&~~\mathclap{\overset{\textrm{Bézout}}{\Rightarrow}} ~~~~ \exists s,t \in \K[x]: sf  + tg = 1\\
		      		      	                            & \Rightarrow sfh + tgh = h                                                                          \\
		      		      	                            & \text{ Wissen:} f \vert fsh \text{ und } f \vert tgh~~~ \text{($f\vert gh$ Voraussetzung von (2)}) \\
		      		      	                            & \Rightarrow f \vert h                                                                              
		      		      \end{align*}
		      		\item[$(2)\Rightarrow (1)$] 
		      		      \begin{align*}
		      		      	\text{Angenommen }f & =gh\text{. Zeigen: }\grad{h}=0.                                                    \\
		      		      	f = gh              & \overset{(2)}{\Rightarrow} f \vert g \lor f \vert h ~~~~\text{O.B.d.A: } f \vert g \\
		      		      	                    & \Rightarrow \grad{f} \underset{f\vert g}{\leq}  \grad{g}                           
		      		      	\underset{h\neq0}{\leq} \grad{h} + \grad{g} = \grad{\underbrace{h \cdot g}_{= f}} \\
		      		      	                    & \text{(damit müssen also alle '$\leq$' sein: '$=$')}                              \\
		      		      	                    & \Rightarrow \grad{h} = 0                                                           
		      		      \end{align*}
		      	\end{itemize}
		      	\qed
		      	\subsection{Korollar}
		      	$f \in \K[x],~~ \grad{f} = n \geq 1$. Dann: 
		      	\begin{itemize}
		      		\item[1)] $f$ hat höchstens $n$ Nullstellen $a_1,...,a_k \in \K$
		      		\item[2)] $f = (x-a_1) \cdot ... \cdot (x -a_k) \cdot \bar{f}$ mit  $\grad{\bar{f}} = \grad{f -k}$.\\
		      		      $[f \text{ normiert, } k = n \Rightarrow f=(x-a_1) \cdot ... \cdot (x-a_n)]$
		      	\end{itemize}
		      	\subsubsection*{Beweis}
		      	\begin{itemize}
		      		\item[\uline{$n=1$:}] $f = ax +b$ hat Nullstelle $-a^{-1}b$
		      		\item[\uline{$n>1$:}] Hat $f$ keine Nullstelle, so fertig. Sonst: \\
		      		      Sei $a$ Nullstelle $\Rightarrow f = (x-a)g, ~~\grad{g} = n-1$. \\
		      		      Sei $b \neq a$ weitere Nullstelle $\Rightarrow (x-b) \vert (x-a)g$\\
		      		      $x -b$ irreduzibel, $(x-b) \not \vert (x-a) \Rightarrow (x-b) \vert g\\ \Rightarrow b$ Nullstelle von $g$ 
		      	\end{itemize}
		      	Per Induktion hat $g \quad n-1$ Nullstellen. Behauptung folgt.\qed
		      	\subsection{Satz (Existenz eindeutiger irreduzibler Polynome)}
		      	$f \in \K[x]$ mit Leitterm $a_nx^n, ~~n \geq 1$ \\
		      	$\Rightarrow$ Es existieren eindeutig bestimmte irreduzible Polynome $p_1,...,p_l$ und $m_1,...,m_l\in \N$ mit $f = a_n p_1^{m_1} \cdot ... \cdot p_l^{m_l}$ 
		      	\subsubsection*{Beweis} Wie in $\mathds{Z}$.\qed
		      	\subsection{Bemerkung}
		      	\label{4.40}
		      	$(\mathds{Z}_n, \oplus, \odot)$ Körper $\Leftrightarrow n$ Primzahl \\
		      	Analog in $\K[x]$: \\
		      	Sei $f \in \K[x], ~~\grad{f} = n$ \\
		      	$(\K[x]_n, + , \odot_f)$ mit 
		      	\begin{itemize}
		      		\item $\K[x]_n := \{g \in \K[x] \mid \grad{g} < n \}$
		      		\item $g \odot_f h = (g \cdot h) \mod f$
		      	\end{itemize}
		      	ist kommutativer Ring mit Eins.\\
		      	\\
		      	$\K[x]_n^* = \{g \in \K[x]_n \mid \text{ggT}(g,f) = 1 \}$\\
		      	\\
		      	Man kann zeigen: 
		      	\begin{itemize}
		      		\item[a)] $\mathds{Z}_p[x]_n$ Körper der Ordnung $p^n \Leftrightarrow f$ irreduzibel, $p$ Primzahl.
		      		\item[b)] Jeder endliche Körper hat Primzahlpotenzordnung und ist durch seine Ordnung bis auf Isomorphie eindeutig festgelegt.
		      	\end{itemize}
		      	\newpage
		      	\section{Komplexe Zahlen}
		      	\label{5}
		      	\subsubsection*{Problem (16 Jhdt.):} 
		      	\begin{itemize}
		      		\item Gleichungen wie z.B. $x^2 = -1$ haben keine reelle Lösung. Dagegen hat\\ $x^2 = -1$ imaginäre Lösungen ('imaginaires' - Descartes) $x_{\nicefrac{1}{2}} = \pm \sqrt{-1}$
		      		\item $x^4 = 1$ hat zwei reelle Lösungen $x = \pm 1$ und zwei imaginäre Lösungen $x = \pm \sqrt{-1}$
		      		\item $x^2 + 2x +2$ hat die imaginären Lösungen $-1 \pm \sqrt{-1}$
		      	\end{itemize}
		      	\subsection{Definition (Grundbegriffe)}
		      	\begin{itemize}
		      		\item $\i  := \sqrt{-1}$ heißt \uline{imaginäre Einheit} (Euler 1777)
		      		\item $\mathds{C} := \{a +b\i \mid a,b \in \R \}$ Menge der komplexen Zahlen
		      		\item Für $z = a +b\i$ heißt $\Re{z} :=a$ \uline{Realteil} von $z$ und $\Im{z} := b$ \uline{Imaginärteil} von $z$
		      	\end{itemize}
		      	\subsection*{Gaußsche Zahlenebene und Polarkoordinaten}
		      	\subsection{Gaußsche Zahlenebene (1831)}
		      	\label{5.2}
		      	\begin{minipage}[c]{0.5\textwidth}
		      		\begin{tikzpicture}[scale=0.75]
		      			\node (v3) at (-3.5,0) {};
		      			\node (v4) at (5,0) {$\R$};
		      			\node (v2) at (0,5) {$\i\R$};
		      			\node (v1) at (0,-3.5) {};
		      			\draw [-latex] (v1) edge (v2);
		      			\draw [-latex] (v3) edge (v4);
		      			\draw [-latex] (0,0) node (v5) {} ellipse (2.5 and 2.5);
		      			\node[below] at (2.5,0) {1};
		      			\node[left] at (0,2.5) {$\i$};
		      			\node[below] at (-2.5,0) {-1};
		      			\node[left] at (0,-2.5) {$-\i$};
		      			\node (v6) at (3.5,4) {};
		      			\draw [-latex, thick] (v5) edge (v6);
		      			\node[right] (v8) at (3.5,4) {$\vec2{a}{b}\leftrightarrow z$};
		      			\node[below] (v9) at (3.5,0) {a};
		      			\node[left] (v7) at (0,4) {b};
		      			\draw [loosely dotted] (v7) edge (v8);
		      			\draw [loosely dotted] (v9) edge (v6);
		      			\draw [](1.1832,-0.038) arc (-0.847:40.6726:1.5);
		      			\node at (0.67,0.297) {$\phi$};
		      			\node (v10) at (1.6422,1.9183) {};
		      			\node (v13) at (0,-0.5) {};
		      			\node (v11) at (1.6422,-0.5) {};
		      			\node (v12) at (1.6422,0) {};
		      			\draw[very thin, densely dashed]  (v12) edge (v10);
		      			\draw [densely dashed, very thin] (v11) edge (v13);
		      					
		      			\node at (0.7034,-0.8071) {cos $\phi$};
		      			\node at (0,0) {};
		      			\node (v15) at (-0.2395,0.2705) {};
		      			\draw [decorate, decoration={brace, amplitude=5pt}] (v15) -- (3.0674,4.2231) node (v14){};
		      			\node at (1.0617,2.5078) {r};
		      			\node[right] at (1.8463,0.7981) {sin $\phi$};
		      		\end{tikzpicture}
		      	\end{minipage}
		      	\begin{minipage}[c]{0.05\textwidth}
		      		~\\
		      	\end{minipage}
		      	\begin{minipage}[c]{0.5\textwidth}
		      		Beobachtung: $a + b\i \leftrightarrow\vec2{a}{b} \in \R^2$ ('korrespondiert eineindeutig zu')\\
		      		\\
		      		$r=\sqrt{a^2+b^2}$\\
		      		$a = r \cdot \cos(\phi)$\\$b = r \cdot \sin(\phi)$
		      	\end{minipage}
		      	Daraus ergibt sich die Darstellung in Polarkoordinaten: \\
		      	$r \geq 0, \quad \phi \in [0,2 \pi)$ bzw. $(r,\phi)\in[0,\infty)\times[0,2\pi)$\\
		      		$\Rightarrow a+b\i = r(\cos(\phi) + \i \sin(\phi))$
		      		\subsection{Definition (Betrag)}
		      		\marginpar{14.12.16}
		      		Für $z = a+b\i \in \C$ ist $|z| := \sqrt{a^2 + b^2}$ der \uline{Betrag} von $z$.
		      		\subsection{Bemerkung}
		      		Jede Zahl $z = a+b\i \in \C \setminus \{0\} $ lässt sich durch den Winkel $\phi \in [0,2\pi)$ und durch den Betrag $|z|$ eineindeutig darstellen: 
		      			$z = |z|\underbrace{(\cos(\phi) + \i \sin(\phi)}_{e^{\i\phi}})$
		      			\subsection{Formel von Euler}
		      			\label{5.5}
		      			$e^{\i\phi} = \cos(\phi) + \i \sin(\phi),~~~ \phi \in \R$
		      			\subsubsection*{Beweisidee (später mit Taylorreihen)}
		      			$\underbrace{\sum_{k = 0}^{\infty} \frac{(\i\phi)^k}{k!}}_{\text{später: }e^{\i\phi}} = \underbrace{\sum_{k = 0}^{\infty}(-1)^k \cdot \frac{\phi^{2k}}{(2k)!}}_{\cos(\phi)\textrm{, gerade }k} + \i \cdot \underbrace{ \sum_{k = 0}^{\infty}(-1)^k \cdot \frac{\phi^{2k+1}}{(2k+1)!}}_{\sin(\phi)\textrm{, ungerade }k}$\\
		      			\\
		      			\uline{Anmerkung:} $\i^0 = 1,~~ \i^1 = \i,~~ \i^2 = -1,~~ \i^3 = -\i,~~ \i^4 = \i^0 = 1$\\
		      			$\Rightarrow \langle \i \rangle$ zyklische Gruppe der Ordnung 4
		      			\subsection{Bemerkung}
		      			Damit ergibt sich für $z \in \C$ die Darstellung $z = |z| e^{\i\phi},~~~ \phi$ wie in \hyperref[5.2]{Abbildung 5.2}
		      			\subsection{Bemerkung}
		      			$e^{\i\phi}$ liegt für $\phi \in \R$ auf dem Einheitskreis, d.h. $\phi \rightarrow e^{\i\phi}$ ist Kreisfunktion. Für Frequenzanalyse (Fourierreihen): \\
		      			$t$... Zeit, $\omega \in \Z$... Frequenz. \\
		      			Dann beschreibt $e^{\i(t \cdot 2\pi) \omega}$ eine Schwingung, z.B.:
		      			\begin{itemize}
		      				\item $\omega = 1:$ in einer Zeiteinheit (ZE) wird Einheitskreis 1 mal durchlaufen
		      				\item $\omega = k:$ in einer ZE wird Einheitskreis $k$ mal durchlaufen
		      			\end{itemize}
		      			\subsection*{Verknüpfungen auf $\C$}
		      			\begin{itemize}
		      				\item[1)] $(\C,+) \cong (\R^2, +)$, d.h. $(a+b\i)+(a' +b'\i) = (a +a') + (b +b')\i$ (Vektoraddition)
		      				\item[2)] Wie wählt man Multiplikation, so daß $\C$ Körper wird?\\
		      				      Man möchte, dass Potenzregel gilt, z.B:\\ $e^{\i \phi} \cdot e^{\i \phi'} = e^{\i(\phi + \phi')} \Leftrightarrow \\
		      				      (\cos \phi + \i \sin \phi)(\cos\phi' + \i \sin\phi') = \cos(\phi + \phi') + \i \sin(\phi + \phi')$\\
		      				      Damit scheidet die komponentenweise Multiplikation aus. Mit den üblichen Rechenregeln aus $\R$: \\
		      				      $(\cos\phi + \i\sin\phi)(\cos\phi' + \i\sin\phi') =\\
		      				      \underbrace{\cos\phi \cos\phi' - \sin\phi \sin\phi'}_{\cos(\phi + \phi')} + \i \underbrace{(\sin\phi\cos\phi' + \cos\phi \sin\phi')}_{\sin(\phi + \phi')}$\\
		      				      \\
		      				      \begin{minipage}[c]{0.5\textwidth}
		      				      	Für $z = a+b\i = |z| e^{\i\phi}$ und \\$z' = a' + b'\i = |z'|e^{\i\phi'}$ ist das Produkt $zz' = z |z| e^{\i\phi'} = |z'| |z| e^{\i(\phi + \phi')}$ eine \\Drehstreckung des Vektors $\vec2{a}{b}$
		      				      \end{minipage}
		      				      \begin{minipage}[c]{0.05\textwidth}
		      				      	~\\
		      				      \end{minipage}
		      				      \begin{minipage}[c]{0.45\textwidth}
		      				      	\begin{tikzpicture}[scale=0.5]
		      				      					
		      				      		\node (v3) at (-4.5,-0.5) {};
		      				      		\node (v4) at (5.5,-0.5) {$\R$};
		      				      		\node (v1) at (1,-1.5) {};
		      				      		\node (v2) at (1,6) {$\i\R$};
		      				      		\draw [-latex] (v1) edge (v2);
		      				      		\draw [-latex] (v3) edge (v4);
		      				      		\node (v5) at (1,-0.5) {};
		      				      		\draw [-latex, very thin](3,-0.5) arc (14.492:120:2.5);
		      				      					
		      				      		\node (v7) at (-3.5,3.5) {$|z'|ze^{\i\phi}$};
		      				      		\node (v6) at (3.5,1) {};
		      				      		\node[above] (v61) at (3.5,1) {$\vec2{a}{b}$};
		      				      		\draw [-latex, thick] (v5) edge (v6);
		      				      		\draw [-latex, thick] (v5) edge (v7);
		      				      		\node at (2.2246,-0.1) {$\phi$};
		      				      		\node at (0.5,1) {$\phi'$};
		      				      		\node[right] (v81) at (6,2.5) {$|z'|\vec2{a}{b}$};
		      				      		\draw [-latex, very thin, style=dotted](6.0636,2.5188) node (v8) {} arc (40.17:132.0957:6);
		      				      		\draw [-latex, thin] (v5) edge (v8);
		      				      		\draw [-latex, very thin](3,-0.5) arc (14.6283:39.0942:2.5);
		      				      	\end{tikzpicture}
		      				      \end{minipage}
		      				\item[3)] Die Inverse einer Drehstreckung $re^{\i\phi}$ ist dann eine Stauchung $\frac{1}{\phi}$ verknüpft mit einer Drehung um $-\phi:$\\
		      				      $z = re^{\i\phi} \Leftrightarrow z^{-1} = \frac{1}{r} e^{\i-\phi}$, da $zz^{-1} = r \frac{1}{r} e^{\i(\phi - \phi)} = 1 \cdot e^0 = 1$\\
		      				      \\
		      				      In der Schreibweise $z= a+b\i,~~ z' = a' + b'\i$ ergibt sich:\\ $zz' = (a +b\i)(a' + b'\i) = aa' -bb' + (ab' + ba')\i$, denn \\$a = r \cos \phi,~~ b= r \sin\phi,~~ a' = r' \cos\phi',~~ b' = r' \sin\phi'$.\\ \\
		      				      Für $z = a +b\i \in \C$ ist die Inverse\\ $z^{-1} = \frac{1}{a+b\i} = \frac{a -b\i}{(a+ b\i)(a -b\i)}= \frac{a-b\i}{a^2 - \i^2 b^2} = \frac{a-b\i}{a^2 +b^2}$
		      			\end{itemize}
		      			\subsection{Definition (Konjugierte)}
		      			Falls $z = a+b\i \in \C,$ heißt $\bar{z} := a-b\i$ die zu $z$ \uline{Konjugierte}.
		      				
		      			\subsection{Bemerkung}
		      			\begin{minipage}[c]{0.5\textwidth}
		      				\begin{itemize}
		      					\item Es folgt $z^{-1} = \frac{\bar{z}}{|z|^2}$
		      					\item $z \cdot \bar{z} = |z|^2 \in \R$
		      				\end{itemize}
		      			\end{minipage}
		      			\begin{minipage}[c]{0.5\textwidth}
		      				\begin{tikzpicture}[scale=0.5]
		      							
		      					\node (v1) at (-3.5,0) {};
		      					\node (v5) at (0,0) {};
		      					\node (v2) at (6,0) {$\R$};
		      					\node (v4) at (0,5) {$\i\R$};
		      					\node (v3) at (0,-4.5) {};
		      					\draw (-3,0) arc (180:0:3);
		      					\draw (3,0) arc (0:-140:3);
		      					\draw (-1.5,2.5981) arc (119.9998:240:3);
		      					\draw [-latex] (v1) edge (v2);
		      					\draw (v3) edge (v5.center);
		      					\draw (v5.center) edge (v4);
		      					\node (v6) at (4.5,4.5) {$z$};
		      					\node (v7) at (4.5,-4.5) {$\overline{z}$};
		      					\draw [-latex] (v5) edge (v6);
		      					\draw [-latex] (v5) edge (v7);
		      					\node (v8) at (1.5,-1.5) {};
		      					\draw [very thick, -latex] (v5) edge (v8);
		      					\node[right] at (1.5,-1.5) {$~z^{-1}=\frac{\overline{z}}{|z|^2}$};
		      					\node[below] at (2.1125,-2.1097) {$\frac{\overline{z}}{|z|}$};
		      				\end{tikzpicture}
		      			\end{minipage}
		      			\subsection{Satz ($\C$ Körper)}
		      			$(\C, + , \cdot)$ mit 
		      			\begin{itemize}
		      				\item $(a+b\i)+(a' +b'\i) = (a + a') + (b + b')\i$ und 
		      				\item $(a+b\i)(a'+ b'\i) = aa' - bb' + (ab' + a'b)\i$
		      			\end{itemize}
		      			ist ein Körper.\\
		      			\\
		      			Nullelement: $\mathcal{O} = 0+0\i$\\
		      			Einselement: $\upharpoonleft = 1+0\i$
		      			\subsubsection*{Beweis} Nachrechnen.\qed
		      			\subsubsection*{Beispiel}
		      			\begin{itemize}
		      				\item $(1+\i) = \sqrt{2} e^{\i\*\frac{\pi}{4}}$
		      				\item $(2+\i)(3-4\i) = 6 +4 + (3-8)\i = 10 -5\i$
		      				\item $\frac{\i+1}{2\i -1} = \frac{(\i+1)(2\i+1)}{\underbrace{(2\i-1)}_{z}\underbrace{(2\i+1)}_{\bar{z}}} =\frac{1-2 +\i(2+1)}{2^2 + 1^2} = -\frac{1}{5} + \frac{3}{5}\i$
		      			\end{itemize}
		      			\subsection{Rechenregeln (Konjunktion, Betrag)}
		      			\label{5.11}
		      			$w,z \in \C$
		      			\begin{itemize}
		      				\item[a)] $\overline{w \pm z} = \overline{w} \pm \overline{z}$\\
		      				      $\overline{w \cdot z} = \overline{w} \cdot \overline{z}$\\
		      				      $\bar{\bar{z}} = z$\\
		      				      $\Rightarrow z \mapsto \bar{z}$ Körperisomorphismus
		      				\item[b)] $\Re{z} =  \frac{z + \bar{z}}{2},~~ \Im{z} = \frac{z - \bar{z}}{2\i}$
		      				\item[c)] $|z| \geq 0,~~ |z| = 0 \Leftrightarrow z = 0$ (positive Definitheit)
		      				\item[d)] $|z| = |\bar{z}| = \sqrt{z\bar{z}}$
		      				\item[e)] $|wz| = |w| \cdot |z|$
		      				\item[f)] $|w+z| \leq |w| + |z|$ Dreiecksungleichung \\
		      				      $|w-z| \geq |w| - |z|$ (Beweis: Übung)
		      			\end{itemize}
		      			\subsection{Bemerkung}
		      			\begin{itemize}
		      				\item[a)] Alternative Konstruktion von $\C$.:\\
		      				      \hyperref[4.40]{4.40}: $\K[x]_n$ wird Körper, wenn man durch irreduzibles Polynom $f$ vom Grad $n$ teilt (Modulorechnung).\\
		      				      Mit $\K = \R, ~~n = 2,~~ f = x^2 +1$ ist 
		      				      \begin{align*}
		      				      	(a+bx) \odot_f (a' + b'x) & = aa' + bb'x^2 + (ab' + ba')x \mod f \\
		      				      	                          & = (aa' - bb') + (ab' + ba')x         
		      				      \end{align*}
		      				      Statt $x$ schreibt man $\i,~~\i^2 = -1$
		      				\item[b)]
		      				      $x^2 +1 = (x-\i)(x+\i)$ ist nicht irreduzibel in $\C[x]$.\\
		      				      Tatsächlich besitzt in $\C$ jede quadratische Gleichung 2 Lösungen.\\
		      				      \\ \uline{Allgemein}: Fundamentalsatz der Algebra: \\
		      				      $f \in \C[x],~~ a_nx^n$ Leitterm, $n \geq 1$. \\
		      				      $\Rightarrow f$ hat genau $n$ Nullstellen $b_1,...,b_n$ (nicht notw. verschieden) mit\\ $f = a_n(x-b_1) \cdot ... \cdot (x-b_n)$ \\
		      				      Das heißt, lineare Polynome $ax +b$ mit $a \neq 0$ sind die einzigen Primelemente in $\C[x]$.
		      				      \marginpar{20.12.16}
		      				\item[c)] Wurzelberechnung: $z  = |z|(\cos\phi + i\sin\phi)\\
		      				      \Rightarrow \pm \sqrt{z} = \pm \sqrt{|z|}(\cos\frac{\phi}{2} + i\sin\frac{\phi}{2})$, da\\ $(e^{i\psi})^2 = e^{i2\psi} = e^{i\psi} \cdot e^{i\psi}$
		      				\item[d)] Übertragung des Grenzwertes von Folgen/Funktionen in $\R$ auf Folgen in $\C$: \\ \\
		      				      \begin{minipage}[c]{0.5\textwidth}
		      				      	\begin{tikzpicture}
		      				      		\node (v1) at (0,0) {};
		      				      		\node (v2) at (0,5.5) {$\i\R$};
		      				      		\node (v3) at (5.5,0) {$\R$};
		      				      		\node (v4) at (1.5,4.5) {$a_n$};
		      				      		\node (v5) at (4.5,1.5) {$c$};
		      				      		\draw [-latex] (v1) edge (v2);
		      				      		\draw [-latex] (v1) edge (v3);
		      				      		\draw [-latex, thick] (v1) edge (v4);
		      				      		\draw [-latex, thick] (v1) edge (v5);
		      				      		\draw [decorate, decoration={brace, amplitude=5pt}] (v4) -- (v5);
		      				      		\node at (3.5,3.5) {$|a_n-c|$};
		      				      	\end{tikzpicture}
		      				      \end{minipage}
		      				      \begin{minipage}[c]{0.5\textwidth}
		      				      	$a_n \rightarrow c ,~~~ a_n, c \in \C \Leftrightarrow \forall \epsilon > 0 ~\exists n_0 \in \N ~\forall n \geq n_0: \underbrace{|a_n - c|}_{\text{Abstand von a und c}} < \epsilon$ \\
		      				      \end{minipage}
		      				      \\
		      				      \begin{minipage}[c]{0.5\textwidth}
		      				      	\begin{tikzpicture}[scale=0.5]
		      				      		\node at (0,0) {};
		      				      		\node (v1) at (0,-5.5) {};
		      				      		\node [above] (v2) at (0,5.5) {$\i\R$};
		      				      		\node (v4) at (5.5,0) {$\R$};
		      				      		\node (v3) at (-5.5,0) {};
		      				      		\draw [-latex] (v1) edge (v2);
		      				      		\draw [-latex] (v3) edge (v4);
		      				      		\draw (4.5,0) arc (0:180:4.5);
		      				      		\draw (2.25,-3.8971) arc (-59.9999:0:4.5);
		      				      		\draw (-4.5,0) arc (180:300:4.5);
		      				      		\node at (0,4.5) {};
		      				      		\node at (-2,0) {};
		      				      		\node at (0,-1) {};
		      				      		\node at (0.5,0) {};
		      				      		\draw (0,4.5);
		      				      		\draw (0,4.5) .. controls (-1,4.5) and (-1.8864,1.552) .. (-2,0);
		      				      		\draw (-2,0) .. controls (-1.7744,-0.6311) and (-1.5,-1) .. (0,-1);
		      				      		\draw (0,-1) .. controls (0.4608,-0.776) and (0.4015,-0.4127) .. (0.5,0);
		      				      		\draw [dashed] (0.5,0) .. controls (0.4519,0.3008) and (0.2447,0.4448) .. (0,0.5);
		      				      		\node at (0.5,5) {$z_1$};
		      				      		\node at (-1.5,0.5) {$z_2$};
		      				      		\node at (0.3108,-1.2795) {$z_3$};
		      				      		\node at (0.8819,0.4564) {$z_4$};
		      				      	\end{tikzpicture}
		      				      \end{minipage}
		      				      \begin{minipage}[c]{0.5\textwidth}
		      				      	$z_n = \frac{1}{n}e^{in\frac{\pi}{2}} \Rightarrow z_n \overset{n \rightarrow \infty}{\rightarrow} 0 $\\
		      				      	~\\
		      				      	$z_1=e^{\i\frac{\pi}{2}}=\i$\\
		      				      	$z_2=\frac{1}{2}e^{\i\pi}=-0.5$\\
		      				      	...
		      				      \end{minipage}
		      				      \begin{itemize}
		      				      	\item Konvergenz von Reihen in $\C$
		      				      	\item Aus absoluter Konvergenz folgt Konvergenz (mit $\triangle$-Ungleichung)\\
		      				      	      $\sum_{n = 1}^{\infty} z_n$ ist absolut konvergent, wenn $\sum_{n = 1}^{\infty}|z_n|$ konvergiert.\\
		      				      	      Beispiel: $\underbrace{\sum_{k = 0}^{\infty}\frac{z^k}{k!}}_{\text{später }= e^z}$ konvergiert $\forall z \in \C$, insbesondere für $z = \i\phi$ (\hyperref[5.5]{5.5})
		      				      \end{itemize}
		      				\item[e)]$\C$ hat alle analytischen Eigenschaften von $\R$, außer: \\
		      				      Auf $\C$ gilt es keine vollständige Ordnung $\leq$, die mit + und $\cdot$ verträglich wäre, d.h. für die gelten würde\\
		      				      \begin{align*}
		      				      	a \leq b,~~~ c \leq d & \Rightarrow a+ c \leq b +d \\
		      				      	a \leq b, ~~~r \geq 0 & \Rightarrow ra \leq rb     
		      				      \end{align*}
		      			\end{itemize}
		      			\subsection{Wiederholung/Zusammenfassung zu $\C$}
		      			(Selbst Zeichnungen analog zu 5.x anfertigen ist hilfreich)
		      			\begin{itemize}
		      				\item Komplexe Zahl: $z = a +bi, a,b \in \R, i^2 = -1$\\
		      				      Im Folgenden ist $z = a +bi, z' = a' + b'i \in \C$\\
		      				      z.B. $x^2 + 2x +3$ hat in $\C$ Nst.\\
		      				      $x_{\nicefrac{1}{2}} = \frac{-2 \pm \sqrt{4-12}}{2} = -1 \pm \sqrt{2}i$\\
		      				\item Es gibt 2 Darstellungen: \\		
		      				      \begin{itemize}
		      				      	\item[1)]
		      				      	      $z = a +bi, z.B. z = 2 + 2i$\\
		      				      	      %Zeichnung \\
		      				      	      $|z| = \sqrt{a^2 + b^2} = \sqrt{8}$
		      				      	\item[2)]
		      				      	      Polarkoordinaten: \\
		      				      	      $z = |z| e^{i\phi}$
		      				      	      %Zeichnung \\
		      				      	      $z^* = \cos(\frac{\pi}{4}) \cdot i \sin(\frac{\pi}{4}) = e^{i\frac{\pi}{4}}$ \\
		      				      	      $\Rightarrow z = |z| z^* = \sqrt{8} e^{i\frac{\pi}{4}}$
		      				      \end{itemize} 
		      				\item Formel von Euler $e^{i\phi} = \cos(\phi) + i \sin(\phi)$
		      				\item Addition: $z + z' = a +a' + (b+ b')i$ \\
		      				      %Zeichnung \\
		      				      Man sieht hier : $ |z + z'| \leq |z| + |z'|$
		      				\item Multiplikation: 
		      				      \begin{align*}
		      				      	zz' & = (a + bi)(a'+b'i)             \\
		      				      	    & = aa' - bb'+(ab'+a'b)i         \\
		      				      	    & = |z| |z'| e^{i\phi}e^{i\phi'} \\
		      				      	    & = |z||z'| e^{i(\phi + \phi')}  \\
		      				      \end{align*}
		      				\item (Drehstreckung)\\
		      				      z.B.:\\
		      				      $1+i = \sqrt{2}e^{i\frac{\pi}{4}}\\
		      				      \frac{1}{2} + \frac{\sqrt{3}}{2}i = e^{i\frac{\pi}{3}}\\
		      				      (1+i)(\frac{1}{2} + \frac{\sqrt{3}}{2}i) = \frac{1- \sqrt{3}}{2} + \frac{1+ \sqrt{3}}{2}i = \sqrt{2}e^{i(\frac{7\pi}{12})}$\\
		      				      (Drehung um $60 \degree$ von $1+i$)
		      				\item $\bar{z} = a -bi$\\
		      				      $z\bar{z} = (a+bi)(a-bi) = a^2 + b^2 = |z|^2$\\
		      				      z.B. $z = 1 +3i, \bar{z} = 1 - 3i, z\bar{z} = 1 + 9 \Rightarrow |z| = \sqrt{10}$ \\
		      			\end{itemize}
		      			\newpage
		      			\section{Lineare Abbildungen}
		      			\subsubsection*{Bemerkung}
		      			Ein $\K-$VR besitzt Skalare $\lambda \in \K,~~ \K$ Körper. \\
		      			Bisher $\K = \R$.\\
		      			Speziell: $\K^n = \{v = (v_1,...,v_n)\mid v_i \in \K ~~\forall i = 1,...,n \}$ ist $\K-$Vektorraum.\\
		      			$\Z_2^2$ ist $\Z_2-$Vektorraum: \\
		      			$\Z_2^2 = \Big\{\vec2{0}{0},\vec2{1}{0},\vec2{0}{1},\vec2{1}{1}\Big\}$
		      			\begin{itemize}
		      				\item $v+w = \vec2{v_1 + w_1 \mod 2}{v_2 + w_2 \mod 2} \qquad  v,w \in \Z_2^2$
		      				\item $\lambda v = \vec2{\lambda v_1 \mod 2}{\lambda v_2 \mod 2} \qquad  \lambda \in \Z_2, ~~v \in \Z_2^2$
		      				\item Nullelement: $\vec2{0}{0}$
		      			\end{itemize}
		      			\subsection{Definition (Lineare Abbildung, Isomorphismus)}
		      			$V,W ~\K-$Vektorräume.\\
		      			\begin{itemize}
		      				\item[i)] $\phi: V \rightarrow W$ heißt \uline{lineare Abbildung}, falls
		      				      \begin{itemize}
		      				      	\item[a)] $\phi(v_1 + v_2) = \phi(v_1) + \phi(v_2)\qquad \forall v_1,v_2 \in V$
		      				      	\item[b)] $\phi(\lambda v) = \lambda \phi(v)\qquad\qquad\qquad~~~~ \forall v \in V~~ \forall \lambda \in \K$
		      				      \end{itemize}
		      				\item[ii)]
		      				      Ist die lineare Abbildung $\phi: V \rightarrow W$ bijektiv, so heißt $\phi$ \uline{(Vektorraum-)Isomorphismus}, man schreibt $V \cong W ~~~(V$ isomorph zu $W)$
		      			\end{itemize}
		      			\subsubsection*{Bemerkung} Erfüllt $\phi$ Bedingung i), so heißt $\phi$ auch (Vektorraum-)Homomorphismus.
		      			\subsection{Bemerkung}
		      			\begin{itemize}
		      				\item[i)] $\phi(\O) = \O $
		      				\item[ii)] $\phi(\sum_{i = 1}^{n} \lambda_i v_i) = \sum_{i = 1}^{n} \lambda_i \phi(v_i)$
		      			\end{itemize}
		      			\subsection{Beispiel}
		      			\begin{itemize}
		      				\item[a)] Nullabbildung $\phi: V \rightarrow W,~~v \mapsto \O$ linear
		      				\item[b)] $\phi: V \rightarrow V, ~~v \mapsto \mu v$ für festes $\mu \in \K$ linear
		      				\item[c)] $\phi: \R^3 \rightarrow \R^3,~~ \vec3{x_1}{x_2}{x_3} \mapsto \vec3{x_1}{x_2}{-x_3}$ Spiegelung an $x_1x_2 -$ Ebene, linear
		      				\item[d)] $\phi: \R^2 \rightarrow \R^2, ~~\vec2{x_1}{x_2} \mapsto \vec2{x_1^2}{x_2}$ nicht linear [$x \mapsto x^2$ nicht linear]
		      			\end{itemize}
		      			\subsection{Bemerkung}
		      			$A \in \mathcal{M}_{m,n}(\K),~~\K$ Körper $\overset{\hyperref[2.6]{2.6}}{\Rightarrow} \phi: \K^n \rightarrow \K^m,~~ v \mapsto Av$ linear\\
		      			Zeigen später: Alle linearen Abbildungen $\phi: \K^n \rightarrow \K^m$ lassen sich durch Matrix $A \in \mathcal{M}_{m,n}(\K)$ darstellen.
		      			\subsection*{Kern und Rang}
		      			\subsubsection*{Motivation}
		      			Gegeben: LGS $Ax = b$ mit $A \in \mathcal{M}_{m,n}(\K),~~ b \in \K^m$\\
		      			Gesucht: Lösung $x \in \K^n$\\
		      			z.B.: $A = \begin{pmatrix}
		      			3 & 0 & 0 \\
		      			0 & 2 & 0 \\
		      			0 & 1 & 0 \\
		      			\end{pmatrix},~~ b  = \vec3{3}{0}{0}$ \\
		      			Spezielle Lösung: $x_0 = \vec3{1}{0}{0}$. Da $A\vec3{0}{0}{\lambda} = \vec3{0}{0}{0}$, ist auch\\ $A\underbrace{\vec3{1}{0}{\lambda}}_{\textrm{Gerade}} = A\Bigg(x_0 + \vec3{0}{0}{\lambda}\Bigg) = \underbrace{Ax_0}_{b} + \underbrace{A\vec3{0}{0}{\lambda}}_{\O} = b\Rightarrow\vec3{0}{0}{\lambda}$ ist Lösung von $Ax = b$\\
		      			$\Rightarrow H'=\Bigg\{\vec3{1}{0}{\lambda}\Bigg|~\lambda\in\R\Bigg\},~~~H=\Bigg\{\vec3
		      			{0}{0}{\lambda}\Bigg|~\lambda\in\R\Bigg\}$ (s.u.)
		      			\subsection{Definition (Homogenes LGS, Lösungsraum)}
		      			$A h = \O, ~~h \in \K^n$ heißt \uline{homogenes LGS}.\\
		      			$\underbrace{H}_{\ker A\textrm{ , vgl. \hyperref[6.8]{6.8}}} := \{h \in \K^n \mid Ah = \O \}$ \uline{Lösungsraum} des homogenen LGS.
		      			\subsection{Satz (Lösung eines LGS)}
		      			\label{6.6}
		      			\marginpar{21.12.16}
		      			Angenommen, es existiert eine Lösung $x_0$ von $Ax = b$. Dann ist\\ $x$ Lösung $\Leftrightarrow x = x_0 +h,~~ h \in H$ 
		      			\subsubsection*{Beweis}
		      			\begin{itemize}
		      				\item[$(\Rightarrow)$] $x$ Lösung $\Rightarrow\O = Ax -Ax_0 = A(\underbrace{x-x_0}_{=:h}) \Rightarrow h \in H$
		      				\item[$(\Leftarrow)$] $x= x_0 + h,~~h \in H \Rightarrow Ax = A(x_0 + h) = Ax_0 + \underbrace{Ah}_{=\O} = b$\qed
		      			\end{itemize}
		      			\subsubsection*{Bemerkung}
		      			\begin{itemize}
		      				\item Wenn $x$ Lösung von $Ax = b$, so setzt sich $x$ zusammen aus spezieller Lösung $x_0 + $Lösung von homogenem LGS.
		      				\item Anzahl Lösungen von $Ax = b$ ist gleich der Anzahl der Lösungen von $Ax = \O$\\
		      				      dim(Lösungsraum) = dim($H$)
		      				\item $H$ heißt Kern von $A$
		      			\end{itemize}
		      			\subsection{Satz (Lineare Abbildung UVR)}
		      			\label{6.7}
		      			$\phi: V \rightarrow W$ linear
		      			\begin{itemize}
		      				\item[i)] $U \leq V$ UVR $\Rightarrow \underbrace{\phi(U)}_{\text{Bild von }U} \leq W $ UVR von $W$.
		      				\item[ii)]
		      				      dim ($U) < \infty \Rightarrow$ dim ($\phi(U)) \leq$ dim $(U)$
		      			\end{itemize}
		      			\subsubsection*{Beweis}
		      			\begin{itemize}
		      				\item[i)]
		      				      \begin{itemize}
		      				      	\item $\mathcal{O} \in U \Rightarrow\phi(\O) = \mathcal{O} \in \phi(U)$
		      				      	\item $v,w  \in U \Rightarrow \phi(v) + \phi(w) = \phi(\underbrace{v+w}_{\in U}) \in \phi(U)$
		      				      	\item $\lambda \in \K,~~ v \in U \Rightarrow \lambda\phi(v) = \phi(\underbrace{\lambda v}_{\in U}) \in \phi(U)$
		      				      \end{itemize}
		      				\item[ii)] $\phi: V \rightarrow W$ linear \\
		      				      $\{u_1,...,u_k\}$ Basis von $U$ $[u \in U \Rightarrow u = \lambda_1 u_1 + ... + \lambda_k u_k]$\\
		      				      $\Rightarrow \{ \phi(u_1),...,\phi(u_k)\}$ Erzeugendensystem von $U$, enthält Basis von $U \Rightarrow$ Behauptung \qed
		      			\end{itemize}
		      			\subsection{Definition (Rang, Kern)}
		      			\label{6.8}
		      			\begin{itemize}
		      				\item[i)] $\phi: V \rightarrow W$ linear, dim($V) < \infty$.\\
		      				      Dann heißt dim($\underbrace{\phi(V)}_{\text{UVR wegen \hyperref[6.7]{6.7}}})$ \uline{Rang von $\phi$}, rg($\phi$).\\
		      				      Im Beispiel (Motivation) ist rg($A) = 2$, weil die Matrix auf eine Ebene abbildet.\\
		      				      $Av = \vec3{a_{11}}{a_{21}}{a_{31}}v_1 + \vec3{a_{12}}{a_{22}}{a_{32}}v_2 + \underbrace{\vec3{a_{13}}{a_{23}}{a_{33}}}_{\O}v_3$
		      				\item[ii)]
		      				      $\phi: V \rightarrow W$ linear.\\
		      				      ker($\phi) =  \{v \in V \mid \phi(v) = \O\}$ heißt \uline{Kern von $\phi$}. \\
		      				      Im Beispiel (Motivation) ist $H = \Bigg\{\vec3{0}{0}{\lambda} \Bigg|~  \lambda \in \R \Bigg\} =$ ker($A$), da jeder Gerade dieser Form auf den Nullvektor, $\mathcal{O}$, abgebildet wird.
		      			\end{itemize}
		      			\subsection{Satz (Kern)}
		      			\label{6.9}
		      			$\phi: V \rightarrow W$ linear
		      			\begin{itemize}
		      				\item[i)] ker$(\phi)$ ist UVR von $V$
		      				\item[ii)] $\phi$ injektiv $\Leftrightarrow$ ker$(\phi) = \{\O\}$
		      			\end{itemize}
		      			\subsubsection*{Beweis}
		      			\begin{itemize}
		      				\item[i)]
		      				      \begin{itemize}
		      				      	\item $\phi(\O) = \mathcal{O} \Rightarrow \mathcal{O} \in$ ker$(\phi)$
		      				      	\item $u,v \in $ker($\phi) \Rightarrow \underbrace{\phi(u)}_{= \O} + \underbrace{\phi(v)}_{=\O} = \O = \phi(u+v) \Rightarrow u+v \in $ ker($\phi$)
		      				      	\item $\lambda \in \K, v \in $ ker($\phi) \Rightarrow \O = \lambda \phi(v) = \phi(\lambda v) \Rightarrow \lambda v \in $ ker($\phi)$
		      				      \end{itemize}
		      				\item[ii)]
		      				      \begin{itemize}
		      				      	\item[$(\Rightarrow)$] $\phi$ injektiv, $\phi(\O) = \O$.\\ Da $\phi$ injektiv, kannn kein weiteres Element auf $\O$ abgebildet werden.
		      				      	\item[$(\Leftarrow)$] Angenommen, $\phi(v_1) = \phi(v_2)~~~ v_1, v_2 \in V$ \\
		      				      	      $\Rightarrow \O = \phi(v_1) - \phi(v_2) = \phi(v_1 - v_2)$ \\
		      				      	      $\Rightarrow v_1  - v_2 = \O$, da ker$(\phi) = \{\O\}$\\
		      				      	      $\Rightarrow v_1 = v_2 $\qed
		      				      \end{itemize}
		      			\end{itemize}
		      			\subsection{Beispiel}
		      			$A = \begin{pmatrix}
		      			3 & 0 & 0 \\
		      			0 & 2 & 0 \\
		      			0 & 1 & 0 \\
		      			\end{pmatrix}, ~~~\phi: \R^3 \rightarrow \R^3,~~~ x \mapsto Ax$ \\
		      			\begin{itemize}
		      				\item $\R^3 = \vecspaceR{e_1, e_2, e_3} \Rightarrow \phi(\R^3) = \vecspaceR{\phi(e_1), \phi(e_2), \phi(e_3)} = \Bigg\langle\vec3{3}{0}{0},\vec3{0}{2}{1},\vec3{0}{0}{0}\Bigg\rangle_\R = \Bigg\langle\vec3{3}{0}{0},\vec3{0}{2}{1}\Bigg\rangle_\R$\\
		      				      $\Rightarrow $rg($\phi) = 2$
		      				\item $\phi(x) = \O \Leftrightarrow Ax = \O \Leftrightarrow x = \vec3{0}{0}{\lambda}, ~~\lambda \in \R$ \\
		      				      ker$(\phi) = H = \Bigg\{\lambda \vec3{0}{0}{1} \Bigg|~ \lambda \in \R \Bigg\}$
		      			\end{itemize}
		      			\subsubsection*{Bemerkung}
		      			\begin{align*}
		      				  & \dim(\ker(\phi)) & + & \textrm{rg}(\phi) & = & \dim(\R^3) \\
		      				  & 1                & + & 2                 & = & 3          
		      			\end{align*}
		      			\subsection{Satz (Lineare Abbildung)}
		      			$V,W$ sind $\K-$Vektorräume, dim($V) = n$\\
		      			Gegeben: $\{v_1,...,v_n\}$ Basis von $V$,~ $w_1,...,w_n \in W$ nicht notw. verschieden \\
		      			\\
		      			$\exists!$ lin.Abb. $\phi: V \rightarrow W$ mit $\phi(v_i) = w_i ~\forall i$, und zwar 
		      			\begin{center}
		      				$(\triangle) \qquad v = \sum_{i = 1}^{n} \lambda_i v_i \overset{\phi}{\rightarrow} w = \sum_{i = 1}^{n} \lambda_i \underbrace{\phi(v_i)}_{w_i}$
		      			\end{center}
		      			Das heißt: Wenn man weiß, wie die Basisvektoren abgebildet werden, dann kennt man die lineare Abbildung vollständig. (vgl. Bemerkung \hyperref[2.5]{2.5} + Beispiel \hyperref[2.4]{2.4}b))
		      			\subsubsection*{Beweis}
		      			Für $\phi$ aus $(\triangle)$ gilt: 
		      			\begin{itemize}
		      				\item $\phi$ linear $\checkmark$
		      				\item $\phi(v_i) = w_i ~\forall i \checkmark$
		      				\item $\phi$ eindeutig: Angenommen es gibt $\psi: V \rightarrow W$ linear mit $\psi(v_i) = w_i\Rightarrow$\\
		      				      $\psi\Big(\underbrace{\sum_{i = 1}^{n} \lambda_i v_i}_{v}\Big) = \sum_{i = 1}^{n}\lambda_i \underbrace{\psi(v_i)}_{= w_i} = \phi\Big(\underbrace{\sum_{i = 1}^{n} \lambda_i v_i}_{v}\Big)$ 
		      			\end{itemize}
		      			\qed
		      			\subsection{Beispiel}
		      			$\phi: \R^3 \rightarrow \R^3$ Drehung um Winkel $\alpha$ um $z-$Achse. \\
		      			$B = \{e_1,e_2,e_3\}$ \\
		      			\begin{minipage}[c]{0.2\textwidth}
		      				~\\
		      			\end{minipage}
		      			\begin{minipage}[c]{0.5\textwidth}
		      					
		      				\InitGraph{5}{8}{1}{1}{0.45cm}
		      				\Viewpoint(285,50,15,8)[2]
		      				\DDArrowAt(-7,0,0)(9,0,0)
		      				\Text[b]{x}
		      				\DDArrowAt(0,-7,0)(0,20,0)
		      				\Text[r]{y}
		      				\DDArrowAt(0,0,-7)(0,0,6)
		      				\Text[l]{z}
		      				\SetDotted
		      				\PCircle(0,0,0,6)
		      				\SetNormal
		      				\PArrowArc(0,0,0,2)(0,45)[25]\Text[b]{$\alpha$}
		      				\PArc(3,0,0,0.7)(90,180)
		      				\DDPointAt(2.8,0.2,0)
		      				\SetDashed
		      				\DDLineAt(4.24,4.24,0)(3,0,0)
		      				\SetNormal
		      				\SetThick{1.2pt}
		      				\DDArrowAt(0,0,0)(6,0,0)\Text[br]{$\vec3{1}{0}{0}$}
		      				\DDArrowAt(0,0,0)(4.24,4.24,0)\Text[tr]{$\vec3{\cos\alpha}{\sin\alpha}{0}$}
		      					
		      				\CloseGraph
		      			\end{minipage}
		      			\begin{minipage}[c]{0.5\textwidth}
		      				$\vec3{1}{0}{0} \overset{\phi}{\rightarrow} \vec3{\cos\alpha}{\sin\alpha}{0}$\\
		      			\end{minipage}\newpage
		      			\begin{minipage}[c]{0.2\textwidth}
		      				~\\
		      			\end{minipage}
		      			\begin{minipage}[c]{0.5\textwidth}
		      				\InitGraph{5}{8}{1}{1}{0.45cm}
		      				%gedrehtes Koordinatensystem!!!
		      				\Viewpoint(195,50,15,8)[2]
		      				\DDArrowAt(-7,0,0)(10,0,0)
		      				\Text[r]{y}
		      				\DDArrowAt(0,7,0)(0,-7,0)
		      				\Text[r]{x}
		      				\DDArrowAt(0,0,-7)(0,0,6)
		      				\Text[t]{z}
		      				\SetDotted
		      				\PCircle(0,0,0,6)
		      				\SetNormal
		      				\PArrowArc(0,0,0,2)(0,45)[25]
		      				\DDMoveTo(0.8,0.3,0)
		      				\Text[c]{$\alpha$}
		      				\PArc(3.7,0,0,1)(90,180)
		      				\DDPointAt(3.2,0.2,0)
		      				\SetDashed
		      				\DDLineAt(4.24,4.24,0)(3.7,0,0)
		      				\SetNormal
		      				\SetThick{1.2pt}
		      				\DDArrowAt(0,0,0)(6,0,0)\Text[br]{$\vec3{0}{1}{0}$}
		      				\DDArrowAt(0,0,0)(4.24,4.24,0)\Text[tl]{$\vec3{-\sin\alpha}{\cos\alpha}{0}$}
		      					
		      				\CloseGraph
		      			\end{minipage}
		      			\begin{minipage}[c]{0.5\textwidth}
		      				$\vec3{0}{1}{0} \overset{\phi}{\rightarrow} \vec3{-\sin\alpha}{\cos\alpha}{0}$ \\
		      				~\\
		      				$\vec3{0}{0}{1} \overset{\phi}{\rightarrow} \vec3{0}{0}{1}$\\
		      				\\
		      			\end{minipage}\\
		      			~\\~\\~\\~\\~\\~\\~\\
		      			
		      			$A=(Ae_1,~Ae_2,~Ae_3)=\begin{pmatrix}
		      			\cos\alpha & -\sin\alpha & 0 \\
		      			\sin\alpha & \cos\alpha & 0 \\
		      			0 & 0 & 1 \\
		      			\end{pmatrix}$, vgl. Bsp. \hyperref[2.4]{2.4}b
		      			\subsection{Beispiel}
		      			\marginpar{10.01.17}
		      			$\phi: \R^3 \rightarrow \R^2,~~ v \mapsto Av,~~ A = \begin{pmatrix}
		      			1 & 2 & 0 \\
		      			2 & 4 & 0 \\
		      			\end{pmatrix}$\\
		      			\begin{itemize}
		      				\item $\ker(\phi) = \Bigg\langle\vec3{0}{0}{1}, \vec3{2}{-1}{0}\Bigg\rangle_\R$
		      				\item Bild von $\R^3$: \begin{align*}
		      				      \phi(\R^3) &= \vecspaceR{\phi(e_1), \phi(e_2),\phi(e_3)}\\
		      				      &= \Big\langle{\vec2{1}{2},\vec2{2}{4},\vec2{0}{0}}\Big\rangle_\R\\
		      				      &= \Big\langle{\vec2{1}{2}}\Big\rangle_\R
		      				\end{align*}
		      				\begin{itemize}
		      					\item $\phi: \ker (\phi) \rightarrow \{\O\}$
		      					\item $v \notin \ker (\phi) \Rightarrow \phi(v) \neq 0$
		      				\end{itemize}
		      			\end{itemize}
		      			\begin{minipage}[c]{0.425\textwidth}
		      				\InitGraph{5}{8}{4}{1}{0.7cm}
		      				\Viewpoint(45,70,10,8)[1]
		      				\SetCMYKColor(0.5,0,0,0)
		      				\ShowFullPlaneThrough(0,0,0)NormalTo(0,1,0)(2.1)
		      				\Text[t]{$\ker\phi$}
		      				\SetCMYKColor(0,0,0,255)
		      				\DDArrowAt(-6,0,0)(4,0,0)
		      				\Text[b]{$x$}
		      				\SetNormal
		      				\DDArrowAt(0,-5,0)(0,5,0)
		      				\Text[b]{$y$}
		      				\DDArrowAt(0,0,-6)(0,0,4)
		      				\Text[r]{$z~~~~~~~~~~~~~~~~~~~~~\R^3$}
		      				\SetCMYKColor(0,1,1,0)
		      				\DDLineAt(-8,-16,0)(-5.5,-11,0)
		      				\DDArrowAt(0,0,0)(2,4,0)
		      				\Text[b]{$\langle v\rangle_\R,~~v\notin\ker\phi$}
		      				\SetDashed
		      				\DDLineAt(-5.5,-11,0)(0,0,0)%dashed
		      				\CloseGraph
		      			\end{minipage}
		      			\begin{minipage}[c]{0.15\textwidth}~\\
		      				~\\
		      				~\\
		      				~\\
		      				~\\
		      				~\\
		      				\centering	$\overset{\phi}{\longrightarrow}$
		      			\end{minipage}
		      			\begin{minipage}[c]{0.425\textwidth}
		      				%%Gedrehtes Koordinatensystem!!!
		      				\InitGraph{5}{8}{4}{1}{0.7cm}
		      				\Viewpoint(0,70,10,8)[1]
		      				\SetCMYKColor(0,0,0,255)
		      				\SetNormal
		      				\DDArrowAt(0,-5,0)(0,5,0)
		      				\Text[b]{$x$}
		      				\DDArrowAt(0,0,-6)(0,0,4)
		      				\Text[r]{$y~~~~~~~~~~~~~~~~~~~~~\R^2$}
		      				\SetCMYKColor(0,1,1,0)
		      				\DDArrowAt(0,-2,-4)(0,2,4)
		      				\Text[b]{$\phi(\R^3)$}
		      				\DDMoveTo(0,-2,-5)
		      				\Text[c]{$\phi(\langle v\rangle)$}
		      				\DDMoveTo(0,1,-0.5)
		      				\Text[c]{$\ker\phi$}
		      				\DDMoveTo(0,0,0)
		      				\SetCMYKColor(0.5,0,0,0)
		      				\BigPoint
		      				\CloseGraph
		      			\end{minipage}\\
		      			\\
		      			\\
		      			\\
		      			\subsection{Satz (Dimensionsformel)}
		      			\label{6.14}
		      			$V,W ~~\K-$Vektorräume, $dim(V) = n,~~ \phi: V \rightarrow W$ lineare Abbildung.\\
		      			Dann ist 
		      			\begin{center}
		      				$\dim(V)= \underbrace{\dim(\ker\phi)}_{\text{'Defekt von }\phi'} + \rg(\phi)$
		      			\end{center}
		      			\subsubsection*{Beweis:}
		      			Sei $\{u_1, ..., u_k\}$ Basis von $\ker \phi$. Ergänze zu Basis $\{u_1,...,u_n\}$ von $V$ und setze $U := \vecspace{u_{k+1},...,u_n}{\K}$\\
		      			Da $\ker\phi \cap U = \{\O\}
		      			$ und $V = U + \ker\phi$, ist 
		      			\begin{center}
		      				$\dim(V) = \dim(\ker\phi) + \dim(U) - \underbrace{\dim(U \cap \ker\phi)}_{= 0}$
		      			\end{center}
		      			Zeige: $\dim(U) \overset{1)}{=} \dim(\phi(U)) \overset{2)}{=} \underbrace{\dim(\phi(V))}_{\rg(\phi)}$\\
		      			\begin{itemize}
		      				\item[1)] \begin{align*}
		      				      \ker\phi \cap U= \{\O\} &\Rightarrow \ker(\nicefrac{\phi}{U}) = \{\O\}\\
		      				      &\overset{\hyperref[6.9]{6.9}}{\Rightarrow} \nicefrac{\phi}{U} \text{ injektiv}\\
		      				      &\Rightarrow \dim(U) = \dim(\phi(U))\\
		      				      \Bigg[\text{Bem:}~~~ \{u_{k+1,...,u_n}\} \text{Basis von }U~~~~~~ &~~~\mathclap{\overset{\nicefrac{\phi}{U}{\text{ injektiv}}}{\Rightarrow}}~~~~~~ \{\phi(u_{k+1}),...,\phi(u_n)\} \text{ Basis von }\phi(U)\Bigg]
		      				\end{align*}
		      				\item[2)]
		      				      \begin{align*}
		      				      	\dim(\phi(U)) & = \dim(\phi(V)), \text{ da}                                                                                     \\
		      				      	\phi(V)       & = \phi(U + \ker(\phi))                                                                                          \\
		      				      	              & ~~\mathclap{\overset{\phi \textrm{ linear}}{=}}~~ ~~~\phi(U) + \underbrace{\phi (\ker(\phi))}_{\{\mathcal{O}\}} \\
		      				      	              & = \phi(U)                                                                                                       
		      				      \end{align*}
		      			\end{itemize}
		      			\subsection{Korollar}
		      			\label{6.15}
		      			$V,W ~~\K-$Vekorräume mit $\dim(V) = \dim(W) = n, ~~\phi: V \rightarrow W$ lineare Abbildung. Dann sind äquivalent:
		      			\begin{itemize}
		      				\item[i)] $\phi$ surjektiv,
		      				\item[ii)] $\phi$ injektiv,
		      				\item[iii)] $\phi$ bijektiv.
		      			\end{itemize}
		      			\subsubsection*{Beweis}
		      			$\hyperref[6.14]{6.14} \Rightarrow n = \dim(\ker\phi) + \rg\phi$\\
		      			$\phi \textrm{ surjektiv } \Leftrightarrow \rg \phi = n \Leftrightarrow \dim(\ker \phi) = 0\overset{\hyperref[6.9]{6.9}}{\Leftrightarrow} \phi \textrm{ injektiv}$ \qed
		      			\subsection*{Lösungen von LGS, Rang von Matrizen}
		      			\uline{Gegeben}: LGS mit $Ax = b, ~~A \in \M_{m,n}(\K),~~ b \in \K^m, ~~\K$ Körper.\\
		      			\uline{Gesucht}: $\mathcal{L} := \{x \in \K^n \mid Ax = b \}$ Lösungsraum\\
		      			\\
		      			Sei $x_0 \in \mathcal{L}$ eine spezielle Lösung.\\
		      			\noindent\hspace*{12mm}$\overset{\hyperref[6.6]{6.6}}{\Rightarrow} \mathcal{L} = x_0 + \ker\phi, ~~~\phi: \K^n \rightarrow \K^m,~~ x \mapsto Ax$\\
		      			D.h. Größe von $\mathcal{L}$ gegeben durch $\dim(\ker \phi)$.
		      			\subsection{Bemerkung}
		      			\label{6.16}
		      			$\dim(\ker \phi) = n - \underbrace{\rg\phi}_{= \dim(\phi(\K^n))}$ \qquad\quad (\hyperref[6.14]{6.14})\\
		      			$\phi(\K^n) = \vecspace{\phi(e_1),...,\phi(e_n)}{\K} = \vecspace{\underbrace{Ae_1,...Ae_n}_{\textrm{Spalten von } A}}{\K}$ \\
		      			$\Rightarrow \rg\phi =$ Anzahl der linear unabhängigen Spalten von $A$ = \uline{Spaltenrang} von $A$\\
		      			\\
		      			Man kann zeigen: Spaltenrang von $A$ = Zeilenrang von $A$ (Anzahl linear unabhängiger Zeilen von $A$)\\
		      			\uline{Insgesamt:}
		      			$\dim(\ker \phi) = n -$ Spaltenrang von $A = n-$ Zeilenrang von $A$
		      			\newpage
		      			\section{Lineare Abbildungen und Matrizen}
		      			\subsubsection*{Erinnerung} (\hyperref[1.29]{1.29}): Ein Vektor hat bezüglich unterschiedlicher Basen unterschiedliche Linearkombinationen und damit auch unterschiedliche Koordinaten, z.B.\\
		      			$v = \vec2{4}{3} \in \R^2$ hat bezüglich $B = \Big\{\vec2{1}{1},\vec2{1}{0}\Big\}$ die Linearkombination $\vec2{4}{3} = \underbrace{3}_{\lambda_1} \cdot \vec2{1}{1} + \underbrace{1}_{\lambda_2} \cdot \vec2{1}{0}$, das heißt $\lambda_1 = 3$ und $\lambda_2 = 1$ sind die Koordinaten von $v$ bezüglich der Basis $B$. Bezüglich der Standardbasis hat $v$ die Koordinaten $\vec2{4}{3} = \underbrace{4} \cdot \vec2{1}{0} + \underbrace{3} \cdot \vec2{0}{1}$.
		      			\subsection{Definition (Koordinatenvektor)}
		      			$V ~~\K-$Vektorraum, $B \subseteq V$ Basis, $B = \{v_1,...,v_n\}$.\\
		      			Wenn $v \in V$ und $v = \lambda_1 v_1 + ... + \lambda_n v_n$, dann heißt K$_B(v) = \vec3{\lambda_1}{\vdots}{\lambda_n} \in \K^n$ \uline{Koordinatenvektor} von $v$ bezüglich der Basis $B$.\\
		      			\\
		      			$\Bigg[$Im Beispiel oben ist K$_B\Bigg(\vec2{4}{3}\Bigg) = \vec2{3}{1} = \vec2{\lambda_1}{\lambda_2}.\Bigg]$\newpage
		      			\subsection*{Basistransformationen}
		      			Umrechnung von Koordinaten bezüglich verschiedener Basen.
		      			\subsection{Beispiel}
		      			\label{7.2}
		      			\begin{minipage}[c]{0.4\textwidth}
		      				\begin{tikzpicture}
		      					\node at (-0.5,-0.5) {$0$};
		      					\node at (0.1,0.1) {\textbullet};
		      					\node (v1) at (0,0) {};
		      					\node (v2) at (5,5) {};
		      					\node (v3) at (3.5,4.5) {};
		      					\node (v4) at (10,4.5) {};
		      					\node (v7) at (0.5,0) {};
		      					\node (v6) at (3,0) {};
		      					\node (v10) at (0,0.5) {};
		      					\node (v11) at (1,1.5) {};
		      					\draw  (v1) edge (v2);
		      					\draw  (v3) edge (v4);
		      					\node at (4.5,4) {};
		      					\node (v9) at (10,4) {};
		      					\node (v8) at (4.5,4) {};
		      					\node (v5) at (1,0) {};
		      					\draw  [-latex] (v5) -- (v6);
		      					\draw  [decorate, decoration={brace, amplitude=5pt}] (v8) -- (v7);
		      					\draw  [decorate, decoration={brace, amplitude=5pt}] (v9) -- (v8);
		      					\draw  [-latex] (v10) -- (v11);
		      					\node at (10.5,4.5) {\textbullet};
		      					\draw (11,5) .. controls (9.5,5.5) and (9.5,3.5) .. (11,4);
		      					\node at (1.8751,-0.2709) {$v_2$};
		      					\node at (0.0781,1.3566) {$v_1$};
		      					\node at (11.5045,4.4534) {~~~~$\vec2{x_1}{x_2}$};
		      					\node at (3,2) {$\lambda_1$};
		      					\node at (7.5,3.5) {$\lambda_2$};
		      				\end{tikzpicture}
		      			\end{minipage}
		      			\begin{minipage}[c]{0.5\textwidth}
		      				~\\
		      				~\\
		      				\uline{Gegeben}: 'Roboterkoordinaten' $\lambda_1, \lambda_2$ bzgl. der Basis $B =\{v_1,v_2\}$.
		      			\end{minipage}\\
		      			\begin{itemize}
		      				\item[1)] \uline{Gesucht}: 'Weltkoordinaten' $(x_1,x_2)^T$ bzgl. Basis $C = \Big\{\vec2{1}{0},\vec2{0}{1}\Big\}$.\\
		      				      Es ist $\vec2{x_1}{x_2} = \lambda_1 \vec2{1}{1} + \lambda_2 \vec2{1}{0} \Leftrightarrow \vec2{x_1}{x_2} = \begin{pmatrix}
		      				      1 & 1 \\
		      				      1 & 0
		      				\end{pmatrix} \cdot \vec2{\lambda_1}{\lambda_2}$\\
		      				Mit z.B.: $\lambda_1 = 3,~~ \lambda_2 = 1:\\
		      				\Rightarrow \vec2{x_1}{x_2} = \underbrace{\begin{pmatrix}
		      					1 & 1\\
		      					1 & 0\\
		      					\end{pmatrix}}_{\substack{\textrm{Basiswechselmatrix }\\S_{BC}\textrm{ (\hyperref[7.3]{7.3})}}} \cdot \underbrace{\vec2{3}{1}}_{\substack{\textrm{Koordinaten}\\\textrm{bzgl. }B}} = \underbrace{\vec2{4}{3}}_{\substack{\textrm{Koordinaten bzgl. }C,\\\textrm{Position des Greifarms}}}$
		      				\item[2)]
		      				      \marginpar{11.01.17}
		      				      \uline{Gesucht}: Koordinaten $\mu_1, \mu_2$ bezüglich Basis $D = \Big\{\vec2{1}{2},\vec2{2}{3}\Big\}$.\\
		      				      Es ist $\mu_1 \vec2{1}{2} + \mu_2 \vec2{2}{3} = \lambda_1 \vec2{1}{1} + \lambda_2 \vec2{1}{0}$\\
		      				      $\lambda_1 = 1,~~ \lambda_2 = 0: \vec2{1}{1} = \overbracket{\underbracket{(-1) \vec2{1}{2}  + 1 \vec2{2}{3}}}$\\
		      				      $\lambda_1 = 0,~~ \lambda_2 = 1: \vec2{1}{0} = (-3) \vec2{1}{2} + 2\vec2{2}{3}$\\
		      				      \\
		      				      Daraus ergibt sich in Matrixschreibweise:\\
		      				      $\underbrace{\begin{pmatrix}
		      				      	\overbracket{-1} & -3 \\
		      				      	\underbracket{~1~} & 2 \\
		      				      	\end{pmatrix}}_{\substack{\textrm{Basiswechselmatrix}\\S_{B,D}}} \cdot \vec2{\lambda_1}{\lambda_2} = \vec2{\mu_1}{\mu_2}$\\
		      				      \\
		      				      Z.B.: $\lambda_1 = 3,~~ \lambda_2 = 1 \Rightarrow \begin{pmatrix}
		      				      -1 & -3 \\
		      				      1 & 2 \\
		      				\end{pmatrix} \cdot \vec2{3}{1} = \vec2{-6}{5} = \vec2{\mu_1}{\mu_2}$ = Koordinaten (-vektor) bzgl. $D$
		      			\end{itemize}
		      			\subsection{Definition (Basiswechselmatrix)}
		      			\label{7.3}
		      			$V$ Vektorraum, $B =\{v_1,...,v_n\},~~ C = \{w_1,...,w_n\}$ Basen von $V$.\\
		      			Schreibe $v_i$ als Linearkombination der Vektoren aus $C:\\
		      			v_1 = \overbracket{\underbracket{s_{11} w_1 + ... + s_{n1}w_n}}\\
		      			~~~\vdots\\
		      			v_n = s_{1n} w_1 + ... + s_{nn} w_n$\\
		      			Dann heißt die Matrix $S_{B,C} = \begin{pmatrix}
		      			\overbracket{s_{11}} &\cdots & s_{1n}\\
		      			\vdots & \ddots& \vdots \\
		      			\underbracket{s_{n1}} & \cdots & s_{nn}
		      			\end{pmatrix}$ \uline{Basiswechselmatrix von Basis B nach C}. \\
		      			Spalte $i$ enthält die Koordinaten von $v_i$ bzgl. C.
		      			\subsection{Satz (Koordinaten umrechnen)}
		      			\label{7.4}
		      			$V,B,C$ wie in \hyperref[7.3]{7.3}.\\
		      			\\
		      			Für $v \in V$ ist K$_C(v) = S_{BC} \cdot $K$_B(v)$
		      			\subsubsection*{Beweis}
		      			\begin{align*}
		      				v & = \sum_{k = 1}^{n} \lambda_k \cdot \underbrace{v_k}_{\sum_{l = 1}^{n}S_{lk}w_l\textrm{~(\hyperref[7.3]{7.3})}} \Rightarrow \text{K}_B(v)= \vec3{\lambda_1}{\vdots}{\lambda_n} \\
		      				  & = \sum_{l = 1}^{n}\Big(\sum_{k = 1}^{n}\lambda_k \cdot s_{lk}\Big)w_l                                                                                                         \\
		      				  & = \mu_l \text{ ~~~~~~ (Koordinaten in Basis }C)                                                                                                                               
		      			\end{align*} \qed
		      			\subsection*{Darstellungsmatrizen}
		      			\subsection{Beispiel}
		      			\label{7.5}
		      			Skizze: Siehe \hyperref[7.2]{7.2}.\\
		      			Roboter soll folgende Operation $\phi: \R^2 \rightarrow \R^2$ ausführen:
		      			\begin{center}
		      				$\phi\Big(\vec2{x_1}{x_2}\Big) = 2 \vec2{x_1}{x_2}$
		      			\end{center}
		      			\uline{Gegeben}: Aktuelle Position $\vec2{x_1}{x_2},~~ B =\Big\{\vec2{1}{1}, \vec2{1}{0}\Big\},~~ C = \Big\{\vec2{1}{0},\vec2{0}{1}\}\Big\}$\\
		      			\uline{Gesucht}: $\lambda_1, \lambda_2$, so dass Greifarm in neuer Position $\phi \Big(\vec2{x_1}{x_2}\Big) = 2\vec2{x_1}{x_2}$.\\
		      			Methode aus \hyperref[7.3]{7.3}:\\
		      			$\begin{rcases}
		      				\phi\Big(\vec2{1}{0}\Big) = \vec2{2}{0} = 0 \cdot \vec2{1}{1} + 2 \vec2{1}{0}\\
		      				\phi\Big(\vec2{0}{1}\Big) = \vec2{0}{2} = 2 \cdot \vec2{1}{1} - 2\vec2{1}{0}\end{rcases}$$\rightarrow$ Matrixschreibweise:\\
		      				$\underbrace{\begin{pmatrix}
		      					0 & 2 \\
		      					2 &-2\\
		      					\end{pmatrix}}_{A_\phi^{C,B}\textrm{ (Def. \hyperref[7.6]{7.6})}} \cdot \underbrace{\vec2{x_1}{x_2}}_{\text{aktuelle Pos. bzgl. C}} = \underbrace{\vec2{\lambda_1}{\lambda_2}}_{\substack{\text{Koord. bzgl. B,}\\\textrm{nachdem }\phi\text{ ausgeführt wurde}}}=\text{K}_B\Bigg(\phi \Big(\vec2{x_1}{x_2}\Big)\Bigg)$\\
		      				Z.B. Greifarm in $\vec2{x_1}{x_2} = \vec2{1}{3}$ soll nach $\phi\Big(\vec2{3}{1}\Big) = \vec2{6}{2}$ bewegt werden. Dazu muss man $\lambda_1,\lambda_2$ auf $\vec2{\lambda_1}{\lambda_2} = \begin{pmatrix}
		      				0 & 2 \\
		      				2 & -2 \\
		      				\end{pmatrix} \cdot \vec2{3}{1} = \vec2{2}{4}$ einstellen.\\
		      				Probe: $\underbrace{\lambda_1}_{=2}\vec2{1}{1} + \underbrace{\lambda_2}_{=4}\vec2{1}{0} = \vec2{6}{2} = \phi\Big(\vec2{3}{1}\Big) \checkmark$
		      				\subsection{Definition (Darstellungsmatrix)}
		      				\label{7.6}
		      				$V,W$ Vektorraum endlicher Dimension mit Basen $B = \{v_1,...,v_n\}$ von $V$ und $C = \{w_1,...,w_m\}$ von $W$. $\phi: V \rightarrow W$ lineare Abbildung.\\
		      				Schreibe $\phi(v_i)$ als Linearkombination der Vektoren aus $C$:\\
		      				$\phi(v_1) = \overbracket{\underbracket{a_{11}w_1 + \cdots + a_{m1} w_m}} \\
		      				\vdots \\
		      				\phi(v_n) = a_{1n} w_1 + ... + a_{mn} w_m$\\
		      				Dann heißt $A_\phi^{B,C} = \begin{pmatrix}
		      				\overbracket{a_{11}} & \cdots & a_{1n} \\
		      				\vdots & \ddots & \vdots \\
		      				\underbracket{a_{m1}} & \cdots & a_{mn}
		      				\end{pmatrix}$ \uline{Darstellungsmatrix von $\phi$} bzgl. $B$ und $C$.\\
		      				\\
		      				\subsubsection*{Schreibweisen}
		      				\begin{itemize}
		      					\item[1)] $A_\phi^{B,B} = A_\phi^B$
		      					\item[2)] Falls $B = \{e_1,...,e_n\} = C \textrm{, (also }V = W)$, schreibe $A_\phi$
		      				\end{itemize}
		      				$\Bigg[$Bem.: $\phi$ durch $A_\phi^{B,C}$ eindeutig bestimmt.$\Bigg]$
		      				\subsection{Satz (Koordinatenvektor und Lineare Abbildung)}
		      				\label{7.7}
		      				$V,W,B,C,\phi$ wie in \hyperref[7.6]{7.6}\\
		      				\uline{Gegeben}: $v \in V,~~ \text{K}_B(v)$.\\
		      				\\
		      				Dann ist K$_C(\phi(v)) = A_\phi^{B,C} \cdot \text{K}_B(v)$
		      				\subsubsection*{Beweis}
		      				\begin{itemize}
		      					\item K$_B(v) = \vec3{\lambda_1}{\vdots}{\lambda_n},~~~ A_\phi^{B,C} = \begin{pmatrix}
		      					      a_{11} & \cdots & a_{1n}\\
		      					      \vdots & \ddots & \vdots \\
		      					      a_{m1} & \cdots & a_{mn}\\
		      					\end{pmatrix}$  \\
		      					$A_\phi^{B,C} \cdot \text{K}_B(v) = \vec3{\sum_{i = 1}^{n}a_{1i}\lambda_i}{\vdots}{\sum_{i = 1}^{n}a_{mi}\lambda_i}$
		      					\item $\phi(v) = \phi(\sum_{i = 1}^{n}v_i\lambda_i) = \sum_{i=1}^{n} \lambda_i \cdot  \underbrace{\phi(v_i)}_{=\sum_{k = 1}^{m}a_{ki}w_k\textrm{ (\hyperref[7.6]{7.6})}} \\
		      					      \noindent\hspace*{34mm}= \sum_{k = 1}^{m}\underbrace{\Big(\sum_{i = 1}^{n} \lambda_i \cdot a_{ki}\Big)}_{\text{Koord. von }\phi(v)\text{ bzgl } C} w_k$
		      				\end{itemize}
		      				$\Rightarrow \text{K}_C(\phi(v)) = \vec3{\sum_{i = 1}^{n} \lambda_i a_{1i}}{\vdots}{\sum_{1 = i}^{n} \lambda_i a_{mi}}$  \qed
		      				\subsection{Beispiel}
		      				\uline{Gegeben}: Basis $B = \{v_1,v_2,v_3\}$ von $V$ und Basis $C = \{w_1,w_2\}$ von $W$,\\
		      				$\phi: V \rightarrow W$ mit $A_\phi^{B,C} = \begin{pmatrix}
		      				1 & 1 & -2 \\
		      				2 & 0 & 3 \\
		      				\end{pmatrix}$.\\
		      				Angenommen, $v \in V$ mit K$_B(v) = \vec3{5}{-2}{4}$. \\
		      				$\Rightarrow \underbrace{\text{K}_C(v)}_{\substack{\textrm{Koordinaten bzgl. }C,\\\textrm{nachdem }\phi\textrm{ ausgeführt wurde}}} = \begin{pmatrix}
		      				1 & 1 & -2 \\
		      				2 & 0 & 3 \\
		      				\end{pmatrix} \cdot \text{K}_B(\phi(v)) = \vec2{-5}{22}$
	
\subsection*{Bemerkung (Geordnete Basen)}
\marginpar{17.01.17}
In \hyperref[7.3]{7.3} und \hyperref[7.6]{7.6} haben die Basisvektoren von $B = \{v_1,..,v_n\}$ und $C = \{w_1,...,w_m\}$ eine bestimmte Reihenfolge (Nummerierung). Man sagt es sind \underline{geordnete Basen} und schreibt dafür $B = (v_1,...,v_n),~~~ C=(w-1,...,w_m)$, um anzuzeigen, dass die Basiselemente nicht vertauscht werden dürfen.
\subsubsection*{Beispiel}
$B = \Bigg(\vec3{1}{0}{0},\vec3{0}{1}{0},\vec3{0}{0}{1}\Bigg),~~~ C = \Bigg(\vec3{0}{1}{0},\vec3{0}{0}{1},\vec3{1}{0}{0}\Bigg) \\
\Rightarrow S_{B,C} = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 \\
\end{pmatrix}\textrm{, da:} \\
\vec3{1}{0}{0} = 0 \cdot \vec3{0}{1}{0} + 0 \cdot \vec3{0}{0}{1} + 1 \cdot \vec3{1}{0}{0} \\
\vec3{0}{1}{0} = 1 \cdot \vec3{0}{1}{0} + 0 \cdot \vec3{0}{0}{1} + 0 \cdot \vec3{1}{0}{0} \\
\vec3{0}{0}{1} = 0 \cdot \vec3{0}{1}{0} + 1 \cdot \vec3{0}{0}{1} + 0 \cdot \vec3{1}{0}{0}$	
\subsection{Beispiel}
\label{7.9}
In \hyperref[7.5]{7.5} ist $A_\phi^{C,B} = \underbrace{S_{C,B}}_{2)} \cdot \underbrace{A_\phi^C}_{1)}$\\
\begin{itemize}
	\item[1)] Streckung im Faktor 2 bezüglich $C = \{e_1,e_2\}$\\
	$A_\phi^C = \begin{pmatrix}
	2 & 0 \\
	0 & 2 \\
	\end{pmatrix}$
	\item[2)] Basiswechssel von $C$ nach $B = \Big(\vec2{1}{1}, \vec2{1}{0}\Big)$\\
	$\vec2{1}{0} = 0 \cdot \vec2{1}{1}  + 1 \cdot \vec2{1}{0} \\
	\vec2{0}{1} = 1 \cdot \vec2{1}{1} + (-1) \cdot \vec2{1}{0}\\
	\Rightarrow S_{C,B} = \begin{pmatrix}
	0 & 1 \\
	1 & -1 \\
	\end{pmatrix}$\\
	Probe: \\
	$\underbrace{\begin{pmatrix}
		0 & 1 \\
		1 & -1 
		\end{pmatrix}}_{S_{C,B}} \cdot\underbrace{\begin{pmatrix}
		2 & 0 \\
		0 & 2 \\
		\end{pmatrix}}_{A_\phi^{C}} = \underbrace{\begin{pmatrix}
		0 & 2 \\
		2 & -2 \\
		\end{pmatrix}}_{A_\phi^{C,B}}$
\end{itemize}	
\subsection{Satz (Umrechnen von Darstellungsmatrizen)}
$\phi: V \rightarrow W$ lineare Abbildung, $B,B'$ Basen von $V$,~~~ $C,C'$ Basen von $W$.\\
\\
$\Rightarrow A_\phi^{B',C'} = S_{C,C'} \cdot A_\phi^{B,C} \cdot S_{B',B}$\\
$\Bigg[$Bemerkung: in \hyperref[7.9]{7.9}: $A_\phi^{C,B} = S_{C,B}  \cdot A_\phi^{C,C} \cdot S_{C,C}\textrm{ mit }  S_{C,C} = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix}=E_2$ ist 'Spezialfall'.$\Bigg]$
\subsubsection*{Beweis:}
Sei $v \in V$.\\
\begin{align*}
A_\phi^{B',C'} \cdot \text{K}_{B'}(v) &\overset{\hyperref[7.7]{7.7}}{=} \text{K}_{C'}(\phi(v)) \\
&\overset{\hyperref[7.4]{7.4}}{=} S_{C,C'} \cdot \overbrace{\text{K}_C(\phi(v))}\\
&\overset{\hyperref[7.7]{7.7}}{=} S_{C,C'} \cdot \overbrace{A_\phi^{B,C} \cdot \underbracket{\text{K}_B(v)}} \\
&\overset{\hyperref[7.4]{7.4}}{=} S_{C,C'} \* A_\phi^{B,C} \cdot \underbracket{S_{B',B} \cdot \text{K}_{B'}(v)} 
\end{align*} \qed
\subsection{Bemerkung zu Darstellungsmatrizen}
$V$ bzw. $W $ $\K$-Vektorraum mit Basen $B = \{v_1,..,v_n\}$ bzw. $C = \{w_1,...,w_n\},~~ \phi: V \rightarrow W$ lineare Abbildung.\\
Für $v \in W$ kann $\text{K}_B(v)$ aufgefasst werden als Bild der Koordinatenabbildung. \\
$\text{K}_B: V \rightarrow \K^n,~~~ v = \sum_{i = 1}^{n} \lambda_i v_i \mapsto \vec3{\lambda_1}{\vdots}{\lambda_n}$\\
Daraus ergibt sich folgendet Übersicht:\\
\begin{tikzpicture}[scale=0.5]
\node (v1) at (-3,3) {$V$};
\node (v2) at (1,3) {$W$};
\node (v3) at (1,-0.5) {$\K^n$};
\node (v4) at (-3,-0.5) {$\K^m$};
\draw  [-latex] (v1) edge (v2);
\draw  [-latex] (v2) edge (v3);
\draw  [-latex] (v1) edge (v4);
\draw  [-latex]  (v4) edge (v3);
\node at (-1.0999,3.6082) {$\substack{\phi}$};
\node at (-1.1175,-0.9476) {$\substack{~\\A_\phi^{BC}}$};
\node at (-4.0525,1.302) {$\substack{\text{K}_B}$};
\node at (1.2589,1.3196) {~~~$\substack{\text{K}_C}$};
\end{tikzpicture}\\
$\Rightarrow$ Jede lineare Abbildung $\phi: \K^n \rightarrow \K^m $ ($\K$ Körper) ist von der Form $\phi(x) = A \cdot x$ für eine geeignete Matrix $A \in \M_{m,n}(\K)$.
\subsubsection*{Beweis}
Wenn $V = \K^n$ und $W = \K^m$, benutze für $B$ und $C$ kanonische Basis.\\
$\Rightarrow \text{K}_C(\phi(v))  = \phi(v) \overset{\hyperref[7.4]{7.4}}{=} A_\phi^{B,C} \cdot \text{K}_B(v) = \underbracket{A_\phi^{B,C} \cdot v}_{\textrm{Matrix }\*\textrm{ Vektor}}$ \qed
\subsection{Satz (Eigenschaften von Darstellungsmatrizen)}
\label{7.12}
$U,V,W$ Vektorräume mit Basen $B,C,D;~~~ \phi,\psi$ lineare Abbildungen.
\begin{itemize}
	\item[i)] Sei $\phi,\psi: V \rightarrow W$. Dann ist\\
	$A_{\phi + \psi}^{B,C} = A^{B,C}_\phi + A^{B,C}_\psi$
	\item[ii)] Sei $\phi:U\rightarrow W$. Dann ist \\
	$A_{\lambda\phi}^{B,C} =  \lambda A^{B,C}_\phi,~~~~ \lambda \in \K$
	\item[iii)] 
	Sei $\phi: U \rightarrow V, ~~~\psi: V \rightarrow W$. Dann ist \\
	$A_{ \psi\circ\phi }^{B,D} = A_\psi^{C,D} \cdot A_\phi^{B,C}$\\
	$\Big[$Bemerkung: Verknüpfung linearer Abbildungen entspricht dem Matrixprodukt der Darstellungsmatrizen. $\Big]$
\end{itemize} 
\subsubsection*{7.12 hier ohne Beweis.}
~~
\subsection*{Matrixinversen}
\subsubsection*{Erinnerung} (\hyperref[4.2]{4.2}): $ \M_n(\K)$ mit Matrixaddition und -multiplikation ist ein Ring mit Eins $(=E_n)$. D.h. $A \in \M_n(\K)$ kann Inverse $A^{-1}$ besitzen.\\
Für $A^{-1}$ gilt: $A \cdot A^{-1} = A^{-1}A = E_n$.\\
\\
Fragen: \begin{itemize}
	\item Welche $A \in \M_n(\K)$ besitzen Inverse $A^{-1} \in \M_n(\K)$?
	\item Wie berechnet man $A^{-1}$? 
\end{itemize}
\subsection{Beispiel}
$A = \begin{pmatrix}
0 & 2 \\
1 & 0
\end{pmatrix}\in\M_2(\R)$ hat Inverse $A^{-1} =  \begin{pmatrix}
0 & 1 \\
\frac{1}{2} & 0\\
\end{pmatrix}$, da:\\
$\begin{pmatrix}
0 & 2 \\
1 & 0 \\
\end{pmatrix} \cdot \begin{pmatrix}
0 & 1 \\
\frac{1}{2} & 0 \\
\end{pmatrix} = \begin{pmatrix}
1 & 0  \\
0 & 1 \\
\end{pmatrix} = E_2$
\subsection{Bemerkung}
\label{7.14}
Idee: $A \in \M_n(\K)$ kann als Darstellungsmatrix $A_\phi^B$ der linearen Abbildung $\phi: \K^n \rightarrow \K^n,~~ \phi(v) = Av$ bezüglich Basis $B$ aufgefasst werden.
\subsection{Satz (Invertierbarkeit)}
\label{7.15}
$V~~\K$-Vektorraum, $\dim(V) = n,~~~ B$ Basis, $\phi: V \rightarrow V$ linear mit Darstellungsmatrix $A_\phi^B$. Dann:
\begin{center}
	$\phi$ invertierbar $\Leftrightarrow A^B_\phi$ invertierbar
\end{center}
Das heißt:  $A_{\phi^{-1}}^B = (A_\phi^B)^{-1}$
\subsubsection*{Beweis}
\begin{itemize}
	\item[($\Rightarrow$)] Zeige: $(A_\phi^B) \cdot (A_{\phi^{-1}}^B) = E_n$\\
	$\phi$ invertierbar $\Rightarrow A_\phi^B \cdot A_{\phi^{-1}}^B \overset{\hyperref[7.12]{7.12}}{=} A_{\phi \circ \phi^{-1}}^B = E_n$\\
	Analog: $A_{\phi^{-1}}^B \cdot A_\phi^B = E_n$
	\item[($\Leftarrow$)] Sei nun $A_\phi^B$ invertierbar. \\
	$\Rightarrow \exists Y \in \M_n(\K): A_\phi^B \cdot Y = Y \cdot A_\phi^B = E_n$\\
	$\overset{\hyperref[7.14]{7.14}}{\Rightarrow} Y = A_\psi^B$ mit $\psi(v) = Y\*v$\\
	$\Rightarrow \begin{cases}
	E_n = A_\phi^B \cdot A_\psi^B \overset{\hyperref[7.12]{7.12}}{=} A_{\phi \circ \psi}^B\\
	\underbracket{E_n} = A_\psi^B \cdot A_\phi^B \overset{\hyperref[7.12]{7.12}}{=} \underbracket{A_{\psi \circ \phi}^B}
	\end{cases}\\
	\Rightarrow \phi \circ \psi = \psi \circ \phi = id_v\\
	\Rightarrow \phi$ hat Inverse $\psi$ \qed
\end{itemize}
\subsection{Satz (Invertierbarkeit, Rang)}
\label{7.16}
\marginpar{18.01.17}
$A \in \M_n(\K)$ invertierbar $\Leftrightarrow \underbrace{\rg(A) = n}_{\substack{\text{d.h. alle Spalten \& Zeilen}\\\text{linear unabhängig}}}$
\subsubsection*{Beweis}
$\hyperref[7.14]{7.14} \Rightarrow A = A_\phi^B$ für $\phi:\K^n \rightarrow \K^n,~~~ \phi(v) = Av$\\
\begin{align*}
A \text{ invertierbar } &\overset{\hyperref[7.15]{7.15}}{\Leftrightarrow} \phi \text{ invertierbar} \\
&\Leftrightarrow \phi \text{ bijektiv }\\
&\overset{\hyperref[6.15]{6.15}}{\Leftrightarrow} \phi \text{ surjektiv}\\
&\Leftrightarrow \rg(\phi) = n\\
&\overset{\hyperref[6.16]{6.16}}{\Leftrightarrow} \rg(A) = n
\end{align*}
\qed
\subsection{Beispiel}
$A = \begin{pmatrix}
1 & 2 \\
2 & 4 \\
\end{pmatrix} \Rightarrow \rg(A) = 1 \Rightarrow A$ nicht invertierbar\\
$A = \underbrace{\begin{pmatrix}
	1 & -1\\
	2 & 0 \\
	\end{pmatrix}}_{\in \M_2(\R)} \Rightarrow \rg(A) = 2 \Rightarrow$ A invertierbar (weil Rang voll).
\subsection{Berechnung der Matrixinverse ($A^{-1}$)}
\uline{Gegeben}: Quadratische Matrix $A0\begin{pmatrix}
a_{11}&...&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&...&a_{nn}
\end{pmatrix}\in\M_n(\K),~~\K$ Körper.
\uline{Gesucht}: Matrixinverse $A^{-1}\in\M_n(\K).$
\subsubsection*{Voraussetzungen}
Das sogenannte \uline{Gauß-Jordan-Verfahren} zur Berechnung der Matrixinversen baut auf der Berechnung der Lösungen von Gleichungssystemen $Ax=b$ mit \textbf{quadratischer} Matrix $A$ auf. Deswegen werden zunächst einige Regeln angegeben, die zur Lösung linearer Gleichungssysteme benutzt werden. Dabei wird im Folgenden das LGS mit Hilfe der erweitertern Koeffizientenmatrix $(A|b)$ beschrieben: Wenn $b=(b_1,...,b_n)^T\in\K^n$ schreibt man \begingroup
\centering $(A|b)=\left ( \begin{array}{ccc}
a_{11}&...&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&...&a_{nn}
\end{array} \right | \left. \begin{array}{c}
b_1\\\vdots\\b_n
\end{array}\right )$.\endgroup\\
Die Lösungsmenge des LGS ändert sich nicht, wenn man an $(A|b)$ folgende elementaren Zeilenumformungen aus dem Gaußverfahren durchführt:
\begin{itemize}
	\item[1.] Erweiterung einer Zeile mit einem Skalar $\lambda\in\K,~~\lambda\neq0$,\item[2.] Addition von Zeilen\item[3.] Tauschen von Zeilen.
\end{itemize}
\subsubsection*{Gauß-Jordan-Algorithmus}
Im Unterschied zum Gauß-Algorithmus bringt man das LGS $Ax=b$ nicht auf Dreiecksform, sondern man formt die Zeilen so um, dass $A$ zur Einheitsmatrix $E_n$ wird. Dabei wird $b$ automatisch zum Lösungsvektor $x$ umgeformt: Man erhält das System $(E_n|x)$.
\subsubsection*{Berechnung der Inversen $A^{-1}$}
Zur Berechnung der Inversen muss nun das System $AX=E_n$ gelöst werden. Man erreicht dies, indem der Gauß-Jordan-Algorithmus simultan auf die $n$ LGS $Ay=e_j,~~j=1,...,n$ angewendet wird. Dazu stellt man das System $(A|E_n)$ auf.\\
Durch Zeilenumformungen überführt man nun $A$ in die Einheitsmatrix, wobei die rechte Seite in die Lösungsmatrix $X$ überführt wird. Man erhält so das System $(E_n|X)$ mit $X=A^{-1}$.\\
$\Bigg[$\uline{Anmerkung:} Das Verfahren zeigt auch, ob $A$ überhaupt eine Inverse besitzt. Besitzt $A$ keine Inverse, so kann man $A$ nicht in die Einheitsmatrix umformen.$\Bigg]$
\subsubsection*{Beispiel}
\uline{Gegeben}: $A = \begin{pmatrix}
2 & 1  \\
1 & 3 \\
\end{pmatrix}$\\
Algorithmus zur Berechnung von $A^{-1}$ erweitert den Gauß-Algorithmus zur Lösung von LGS.\\
z.B. $Ax = b$ mit $b = \vec2{10}{15}$ \\
Gauß-Jordan-Verfahren: \\
\\
$\left ( \begin{array}{cc}
2&1\\1&3
\end{array} \right | \left. \begin{array}{c}
10\\15
\end{array}\right ) \overset{II = I -2 \cdot II}{\longrightarrow} \left ( \begin{array}{cc}
2&1\\0&-5
\end{array} \right | \left. \begin{array}{c}
10\\20
\end{array}\right ) \overset{-\frac{1}{5} \cdot II}{\longrightarrow} \left ( \begin{array}{cc}
2&1\\0&1
\end{array} \right | \left. \begin{array}{c}
10\\4
\end{array}\right )\overset{I = I-II}{\longrightarrow} 
\left ( \begin{array}{cc}
2&0\\0&1
\end{array} \right | \left. \begin{array}{c}
6\\4
\end{array}\right ) \overset{\frac{1}{2}}{\rightarrow}\left ( \begin{array}{cc}
1&0\\0&1
\end{array} \right | \left. \begin{array}{c}
3\\4
\end{array}\right )\\
\Rightarrow x = \vec2{3}{4}$ Lösung\\
Für Inverse: Suche Matrix, die $A \cdot X = E_n$ löst.\\
$AX = E_n \Leftrightarrow A\begin{pmatrix}
x_{11} & x_{12} \\
x_{21} & x_{22} \\
\end{pmatrix} = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix} \Leftrightarrow \underbrace{A \vec2{x_{11}}{x_{12}} = \vec2{1}{0}}_{(*)}$ und $\underbrace{A\vec2{x_{12}}{x_{22}} = \vec2{0}{1}}_{(**)}$\\
Wende Gauß-Jordan-Algorithmus simultan auf LGS (*) und (**) an.\\
$\left ( \begin{array}{cc}
2&1\\1&3
\end{array} \right | \left. \begin{array}{cc}
1&0\\0&1
\end{array}\right ) \overset{II = I -2II}{\rightarrow} \left ( \begin{array}{cc}
2&1\\0&-5
\end{array} \right | \left. \begin{array}{cc}
1&0\\1&-2
\end{array}\right ) \overset{-\frac{1}{5}II}{\rightarrow} \left ( \begin{array}{cc}
2&1\\0&1
\end{array} \right | \left. \begin{array}{cc}
1&0\\-\frac{1}{5}&\frac{2}{5}
\end{array}\right ) \overset{I = I -II}{\rightarrow} \left ( \begin{array}{cc}
2&0\\0&1
\end{array} \right | \left. \begin{array}{cc}
\frac{6}{5}&-\frac{2}{5}\\-1\frac{1}{5}&\frac{2}{5}
\end{array}\right ) \overset{\frac{1}{2}I}{\rightarrow}  \left ( \begin{array}{cc}
1&0\\0&1
\end{array} \right | \left. \begin{array}{cc}
\frac{3}{5}&-\frac{1}{5}\\-1\frac{1}{5}&\frac{2}{5}
\end{array}\right ) \Rightarrow \begin{pmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5} \\
\end{pmatrix} = X = A^{-1}$
\subsection{Lemma}
\label{7.19}
$V ~~\K$-Vektorraum, $B,C$ Basen $\Rightarrow S_{B,C} = (S_{C,B})^{-1}$
\subsubsection*{Beweis}
Sei $v \in V$.\\
$\underbrace{S_{C,B} \*\bigg( S_{B,C}}_{= E_n} \cdot \text{K}_B(v)\bigg) = S_{C,B} \cdot \text{K}_C(v) = \text{K}_B(v)$\qed
\subsection{Beispiel}
$V=\R^2,~~ B= \Big(\vec2{1}{1},\vec2{1}{0}\Big),~~ C = \Big(\vec2{1}{0}, \vec2{0}{1}\Big)$\\
Aus \hyperref[7.2]{7.2}: $S_{B,C} = \begin{pmatrix}
1 & 1 \\
1 & 0 \\
\end{pmatrix}$, aus \hyperref[7.9]{7.9}: $S_{C,B} = \begin{pmatrix}
0 & 1 \\
1 & -1 \\
\end{pmatrix}$\\
Tatsächlich ist $\begin{pmatrix}
1 & 1 \\
1 & 0 \\
\end{pmatrix} \cdot \begin{pmatrix}
0 & 1 \\
1 & -1 \\
\end{pmatrix} = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix}$.
\subsection{Korollar}
$\phi: V \rightarrow V,~~~ B,C$ Basen von $V,~~~ S := S_{B,C}$\\
\\
$\Rightarrow A_\phi^C = S A_\phi^B S^{-1}$
\subsubsection*{Beweis}
$S A_\phi^B S^{-1} \overset{\hyperref[7.19]{7.19}}{=} S_{B,C} A_\phi^{B,B} S_{C,B} = A_\phi^{C,C}  = A_\phi^C$\qed
\subsection{Beispiel}
$V = \R^2, ~~~B = \Big(\vec2{1}{1},\vec2{1}{0}\Big),~~~ C = \Big(\vec2{1}{0},\vec2{0}{1}\Big)$\\
Wie sieht Darstellungsmatrix von einer Drehung $\phi: \R^2 \rightarrow \R^2$  um den Winkel $\frac{\pi}{2}$ bzgl. $B$ aus?\\
Wissen: $D_{\frac{\pi}{2}} = \begin{pmatrix}
0 & 1 \\
-1 & 0 \\
\end{pmatrix} = A_\phi^C$ Drehung um $\frac{\pi}{2}$ bzgl. $C$\\
\begin{align*}
A_\phi^B &= S_{C,B} \underbrace{D_{\frac{\pi}{2}}}_{A_\phi^{C,C}} S_{B,C} \\
&= \begin{pmatrix}
0 & 1 \\ 
1 & -1 \\
\end{pmatrix} \begin{pmatrix}
0 & 1 \\
-1 & 0 \\
\end{pmatrix} \begin{pmatrix}
1 & 1 \\
1 & 0 \\
\end{pmatrix} = \begin{pmatrix}
-1 & -1 \\
2 & 1 \\
\end{pmatrix}
\end{align*}
\newpage
\section{Determinanten}
$\hyperref[7.16]{7.16}: A \in \M_n(\K)$ invertierbar $\Leftrightarrow \rg(A) = n$\\
In diesem Kapitel werden invertierbare Matrizen mit Hilfe der Determinante charakterisiert. \\
Das ist einfacher zu implementieren.
\subsection{Definition ($A_{i,j}$)}
$A \in \M_n(\K),~~~ i,j \in \{1,...,n\}.~~~ A_{i,j} \in \M_{n-1}(\K)$ sei die Matrix, die man aus $A$ durch Streichen der $i-$ten Zeile und $j-$ten Spalte erhält. \\
z.B.: $A = \begin{pmatrix}
1 & 2 & -3 \\
0 & 1 & -1 \\
-2 & 1 & 1 \\
\end{pmatrix} \Rightarrow A_{2,3} = \begin{pmatrix}
1 & 2 \\
-2 & 1 \\
\end{pmatrix}$
\subsection{Definition (Rekursive Definition der Determinante)}
\label{8.2}
\marginpar{24.01.17}
$A \in \M_n(\K).\\
\\
\underline{n = 1:} ~~~~A = (a), \det(A) := a \in \K$\\
\\
$\underline{n \geq 2:}~~~~$ Entwicklung nach der 1. Zeile (\hyperref[7.4]{7.4}): \\

$A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \ddots &\ddots & \vdots \\
a_{n1} & \cdots & \cdots & a_{nn} \\
\end{pmatrix}\\ 
\\
\det(A) =+a_{11}\cdot \det(A_{1,1})- a_{12}\cdot \det(A_{1,2}) + a_{13}\cdot \det(A_{1,3}) \pm ... (-1)^{n+1}a_{1n}\cdot \det(A_{1,n})$
\noindent\hspace*{12mm}$= \sum_{j = 1}^{n} (-1)^{1+j} a_{1j} \cdot \det(A_{1j})$
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $\det\begin{pmatrix}
	1 & 1 \\
	2 & -3
	\end{pmatrix} = 1 \cdot (-3) - 1 \cdot 2 = -5$\\
	$\det\begin{pmatrix}
	\overbracket{a_{11}}^{+} & \overbracket{a_{12}}^{-} \\
	a_{21} & a_{22} \\
	\end{pmatrix} = a_{11} \cdot a_{22} - a_{12} \cdot a_{21}$  \\
	 Man multipliziert die Werte auf der Hauptdiagonale und zieht die der Nebendiagonale ab.
	\item[b)] $\det\begin{pmatrix}
	\overbracket{a_{11}}^{+} &\overbracket{a_{12}}^{-} & \overbracket{a_{13}}^{+} \\
	a_{21} &a_{22} & a_{23} \\
	a_{31} &a_{32} & a_{33} \\
	\end{pmatrix} = a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + \\
	\noindent\hspace*{41mm}a_{13}(a_{21}a_{33} - a_{22}a_{31}) \\\noindent\hspace*{35mm}= ...$\\
\end{itemize}
\subsubsection*{Regel von Sarrus:}
$\begin{pmatrix}
a\tmark{a}_{11} &a\tmark{c}_{12} & a\tmark{e}_{13} ~\tikzmark{oben}& a\tmark{h}_{11} & a\tmark{j}_{12}\\
a_{21} &a_{22} & a_{23} & a_{21} & a_{22}\\
\underset{-}{a\tmark{g}_{31}} &\underset{-}{a\tmark{i}_{32}} & \underset{\nicefrac{-}{+}}{a\tmark{b}_{33}} ~\tikzmark{unten}& \underset{+}{a\tmark{d}_{31}} & \underset{+}{a\tmark{f}_{32}}\\
\end{pmatrix}$
\verticalline{oben}{unten}\\
\TikzLine[cyan, thin]{a}{b}
\TikzLine[red, thin]{c}{d}
\TikzLine[green, thin]{e}{f}
\TikzLine[orange, thin]{e}{g}
\TikzLine[purple, thin]{h}{i}
\TikzLine[brown, thin]{j}{b}
\\
Zum Beispiel:
$\det\begin{pmatrix}
1 & 2 & -1 \\
0 & 1 & 2 \\
1 & 0 & 3 \\
\end{pmatrix} = 3 +4+ 0 -(-1) -0 - 0 = 8$
\begin{itemize}
	\item[c)] Für $n \times n-$Matrix gibt es im Allgemeinen $n!$ Summanden.\\
	Viele Nullen in der Matrix machen die Berechnung einfacher, z.B.:\\
	$\det\begin{pmatrix}
	\overset{+}{0} & \overset{-}{-2} & \overset{+}{0} \\
	1 & 2 & -3 \\
	4 & 1 & 1 \\
	\end{pmatrix} = 0 - (-2)\*\det\begin{pmatrix}
	1 & -3 \\
	4 & 1 \\
	\end{pmatrix} + 0 = 26$\\
	Falls Nullen nicht in 1. Zeile stehen: Man kann nach jeder beliebigen Zeile oder Spalte entwickeln:
\end{itemize}
\subsection*{Regeln zur Berechenung der Determinante}
\subsection{Satz (Entwicklungssatz von Laplace)}
\label{8.4}
$A \in \M_n(\K)$
\begin{itemize}
	\item[i)] Entwicklung nach $i$-ter Zeile: \\
	$\det(A) = \sum_{j = 1}^{n} (-1)^{i+j}a_{ij}\*\det(A_{ij})$
	\item[ii)] Entwicklung nach $j$-ter Spalte: \\
	$\det(A)  = \sum_{i = 1}^{n}(-1)^{i+j} a_{ij} \*\det(A_{ij})$
\end{itemize}
\subsubsection*{8.4 hier ohne Beweis, zu lang.}
\newpage
\subsection{Beispiel}
\begin{itemize}
\item[a)] $A = \begin{pmatrix}
2 & -1 & 1 \\
-1 & 0 & 3 \\
2 & 0 & 4 \\
\end{pmatrix} \qquad\rightarrow\qquad \begin{pmatrix}
+ & - & + \\
- & + & - \\
+ & - & + \\
\end{pmatrix}\leftarrow$(Vorzeichen, $(-1)^{i+j}$,\\ \noindent\hspace*{84mm}Schachbrettmuster)
\begin{itemize}
	\item nach 1. Spalte: \\
	$\det(A) = 2 \cdot \det\begin{pmatrix}
	0 & 3 \\
	0 & 4 \\
	\end{pmatrix} - (-1) \det\begin{pmatrix}
	-1 & 1 \\
	0 & 4 \\
	\end{pmatrix} + 2 \cdot \det\begin{pmatrix}
	-1 & 1 \\
	0 & 3 \\
	\end{pmatrix} \\
	\noindent\hspace*{11mm}= 2 \cdot 0 + 1 \cdot (-4) + 2 \cdot (-3) \\
	\noindent\hspace*{11mm}= -10$
	\item nach 2. Spalte: \\
	$\det(A) = -(-1) \cdot \det\begin{pmatrix}
	-1 & 3 \\ 
	2 & 4 \\
	\end{pmatrix}  + 0 + 0 \\
	\noindent\hspace*{11mm}= -10$
\end{itemize}
Also: Am Besten, man entwickelt nach Zeile oder Spalte, in der viele Nullen stehen.\\
\item[b)] Falls es nur wenige Nullen gibt: Erzeuge möglichst viele Nullen mit Gauß, denn: \\
$A = \underbrace{\begin{pmatrix}
a_{11} &  \cdots & \cdots & a_{1n} \\
0 & \ddots &   & \vdots \\
\vdots & \ddots & \ddots &  \vdots \\
0 & \cdots & 0 & a_{nn}
\end{pmatrix}}_{\textrm{obere Dreiecksmatrix}} \\
\noindent\hspace*{15mm}\Rightarrow \det(A) = a_{11} \cdot \det\begin{pmatrix}
a_{22}&\cdots&\cdots&a_{2n}\\
0&\ddots&&\vdots\\
\vdots&\ddots&\ddots&\vdots\\
0&\cdots&0&a_{nn}
\end{pmatrix} \\
\noindent\hspace*{32mm}= a_{11} \cdot a_{22} \cdot \det\begin{pmatrix}
a_{33}&\cdots&\cdots&a_{3n}\\
0&\ddots&&\vdots\\
\vdots&\ddots&\ddots&\vdots\\
0&\cdots&0&a_{nn}
\end{pmatrix}\\
\noindent\hspace*{32mm}=... \\
\noindent\hspace*{32mm}= a_{11} \cdot a_{22} \cdot a_{33} \cdot ... \cdot a_{nn}$\\
\\
Analog für unter Dreiecksmatrix: \\
$\det\begin{pmatrix}
a_{11} &  0 & \cdots & 0 \\
\vdots & \ddots &  \ddots & \vdots \\
\vdots &  & \ddots &  0 \\
a_{n1} & \cdots & \cdots & a_{nn}
\end{pmatrix} = a_{11} \cdot ... \cdot a_{nn}$\\
Für den Gauß-Algorithmus müssen folgende Regeln beachtet werden:
\end{itemize}
\subsection{Satz (Eigenschaften von Determinanten)}
\label{8.6}
$A,B  \in \M_n(\K),~~~ A = (S_1,...,S_n), ~~~s_1,...,s_n \in \K^n,~~~ s_i' \in \K^n$\\
\\
Folgende Eigenschaften gelten sowohl für Spalten als auch für Zeilen:
\begin{itemize}
	\item[D1)] $\det(s_1,~...,\underbrace{s_i + s_i'}_{i-\text{te Spalte}},~...,~s_n) = \det(s_1,~...,~s_i,~...,~s_n) + \det(s_1,~...,~s_i',~...,~s_n)$\\
	\textbf{Beweis:} Nach Spalte $i$ entwickeln.
	\item[D2)] Beim Vertauschen von 2 Spalten ändert sich das Vorzeichen der Determinante.\\
	\textbf{Beweis} Hier ohne Beweis.
	\item[D3)] $\det(s_1,~...,~\lambda s_i,~...,~s_n) = \lambda \cdot \det(s_1,~...,~s_n),\qquad \lambda \in \K$\\
	\textbf{Beweis:} Nach Spalte $i$ entwickeln.
	\item[D4)] $\det(\lambda \cdot A) = \det(\lambda s_1,~...,~\lambda s_n) \overset{D3}{=} \lambda^n \det(A)$
	\item[D5)] Ist $s_i = \vec3{0}{\vdots}{0}$, so ist $\det(A) = 0$\\
	\textbf{Beweis:} Nach Spalte $i$ entwickeln.
	\item[D6)] Besitzt $A$ zwei identische Spalten, so ist $\det(A) = 0$.\\
	\textbf{Beweis:} Vertausche Spalten und erhalte Matrix $A'$ mit $A'= A$.\\
	Nach D2: $\det(A) = -\det(A) \Rightarrow \det(A) = 0$, falls $\K \neq \Z_2$.\\
	Falls $\K = \Z_2:$ Es gilt auch $\det(A) = 0$ mit vollständiger Induktion.
	\item[D7)] $\det(s_1,~...,~\underbrace{s_i+\lambda s_j}_{i-\text{te Spalte}},~...,~s_n) = \det(A) \qquad(i \neq j,~j\in[1,n])$\\
	\textbf{Beweis:} D1, D3, D6.
	\item[D8)] $\det(A \cdot B) = \det(A) \cdot \det(B)$\\
	\textbf{Beweis} Hier ohne Beweis.
	\item[D9)] $\det(A^T) = \det(A)$\\
	\textbf{Beweis:} Folgt aus \hyperref[8.4]{8.4}.
\end{itemize}
\subsection{Beispiel}
$\det\begin{pmatrix}
0 & 1 & 2 \\
-2 & 0 & 3 \\
0 & -2 & 3 \\
\end{pmatrix} \overset{z_1 \leftrightarrow z_2}{=} -\det\begin{pmatrix}
-2 & 0 & 3 \\
0 & 1 & 2 \\
0 & -2 & 3 \\
\end{pmatrix} \overset{III = III + 2II}{=} -\det\begin{pmatrix}
-2 & 0 & 3 \\
0 & 1 & 2 \\
0 & 0 & 7 \\
\end{pmatrix}\\
\noindent\hspace*{34.5mm} = - 14$
\subsection*{Charakterisierung invertierbarer Matrizen}
\subsection{Satz (Invertierbarkeit von Matrizen)}
$A \in \M_n(\K)$ invertierbar $\Leftrightarrow \det(A) \neq 0$\\
In diesem Fall gilt: $\det(A^{-1}) = (\det(A))^{-1}$.
\subsubsection*{Beweis}
\begin{itemize}
	\item[($\Rightarrow$)] $\det(A) \cdot \det(A^{-1}) \overset{\hyperref[8.6]{D8}}{=} \det(A \cdot A^{-1}) = \det(E) = 1\\
	\noindent\hspace*{25mm} \Rightarrow \det(A) \neq 0 ,~~ \det(A^{-1}) = \det(A)$
	\item[$(\Leftarrow)$] Sei $A$ nicht invertierbar $\overset{\hyperref[7.16]{7.16}}{\Rightarrow} \rg(A) < n \\
	\noindent\hspace*{40.5mm}\Rightarrow$ Spalten von $A$ sind linear abhängig, d.h.:\\
	\\
	\noindent\hspace*{40.5mm}$\qquad\exists i: s_i = \sum_{k = 1, k\neq i}^{n} \lambda_k s_k\\\noindent\hspace*{40.5mm}\qquad s_1,...,s_n$ Spalten von $A$ \\
	\noindent\hspace*{40.5mm}$\Rightarrow \det(A) \overset{\hyperref[8.6]{D7}}{=} \det(s_1,~...,~\underbrace{s_i - \sum_{k = 1, k\neq i}^{n} \lambda_k s_k}_{i-\text{te Spalte}},~...,~s_n)$\\
	\noindent\hspace*{57.5mm}$= \det(s_1,...,\underbrace{0}_{i\text{-te Spalte}},...,s_n) \overset{\hyperref[8.6]{D5}}{=} 0$\qed
\end{itemize}
\subsection{Bemerkung}
\label{8.9}
\begin{itemize}
	\item[a)] Seien $v_1,v_2,v_3 \in \R^3,$ z.B. \\
	$v_1 = \vec3{1}{0}{0},~~ v_2 = \vec3{0}{1}{0},~~ v_3 = \vec3{1}{0}{2}$\\
	\\
	\\
	Das von $v_1,v_2,v_3$ gebildete Parallelepiped $P$:\\
	\InitGraph{4}{6}{4}{1.25	}{1cm}
	\Viewpoint(300,60,10,20)[1]
	%
	\DDArrowAt(0,0,0)(1,0,0)
	\DDArrowAt(0,0,0)(0,1,0)
	\DDArrowAt(0,0,0)(1,0,2)
	%linksrechts
	\DDLineAt(1,0,2)(2,0,2)
	\DDLineAt(0,1,0)(1,1,0)
	\DDLineAt(1,1,2)(2,1,2)
	%vornhinten
	\DDLineAt(1,0,0)(1,1,0)
	\DDLineAt(1,0,2)(1,1,2)
	\DDLineAt(2,0,2)(2,1,2)
	%untenoben
	\DDLineAt(1,0,0)(2,0,2)
	\DDLineAt(0,1,0)(1,1,2)
	\DDLineAt(1,1,0)(2,1,2)
	%Text
	\DDMoveTo(1,0.5,1)
	\Text[c]{$P$}
	\DDMoveTo(0.5,0,0)
	\Text[b]{$v_1$}
	\DDMoveTo(0,0.5,0)
	\Text[r]{$v_2$}
	\DDMoveTo(0.5,0,1)
	\Text[l]{$v_3$}
	\CloseGraph\\
	Man kann ausrechnen, dass $| \det(v_1,v_2,v_3) |$ das Volumen von $P$ ist. Es ist $\Bigg|\det\begin{pmatrix}
	1 & 0 & 1 \\
	0 & 1 & 0 \\
	0 & 0 & 2 \\
	\end{pmatrix} \Bigg|= 2$. Dies gilt in analoger Weise in $\R^2$ für ein Parallelogramm, das von $v_1, v_2 \in \R^2$ gebildet wird und für höhere Dimensionen $ n \geq 4$.
	\item[b)] Es gibt eine alternative Berechnung von $A^{-1}$, z.B. wenn $A = \begin{pmatrix}
	a & b \\
	c & d \\
	\end{pmatrix} \in \M_2(\K) \Rightarrow A^{-1} = (\det(A))^{-1} \cdot \begin{pmatrix}
	d & -b \\
	-c & a \\
	\end{pmatrix}$, denn $\begin{pmatrix}
	a & b \\
	c & d \\
	\end{pmatrix} \cdot \begin{pmatrix}
	d & -b \\
	-c & a \\
	\end{pmatrix} = \begin{pmatrix}
	ad - bc & 0 \\
	0 & \underbrace{ad-bc}_{\det(A)}
	\end{pmatrix}$\\
	Allgemeine Formel für $A \in \M_n(\K)$ komplizierter (auf unserem Level nicht verständlich).
\end{itemize}
\newpage
\section{Eigenwerte und Eigenvektoren}
\subsubsection*{Anwendungen} Markov-Ketten (Kaufverhalten), Eigenfaces, Page-Rank-Algorithm, etc.
\subsection{Beispiel}
$A = \begin{pmatrix}
2 & 0 \\
0 & \frac{1}{3}
\end{pmatrix}	 \in \M_2(\R)$. \\
Da $A\vec2{1}{0} = 2 \cdot \vec2{1}{0}$ und $A\vec2{0}{1} = \frac{1}{3}\vec2{0}{1}$, streckt $A$ in Richtung $\vec2{1}{0}$ um den Faktor 2 und staucht in Richtung $\vec2{0}{1}$ um den Faktor $\frac{1}{3}$.\\
%Zeichung
Man nennt $\vec2{1}{0}$ Eigenvektor (EV) von $A$ zum Eigenwert (EW) 2 und $\vec2{0}{1}$ Eigenvektor zum Eigenwert $\frac{1}{3}$.\\
\\
\begin{minipage}[c]{0.5\textwidth}
	\InitGraph{4}{4}{2}{2}{1cm}
	\Coordinates(0.8,1)(0.8,1)
	\TextAt(2,0)[b]{$x$}
	\TextAt(0,2)[l]{$y$}
	\MoveTo(0,0)
	\Circle(0.8)
	\CloseGraph
\end{minipage}
$\longrightarrow$~~~~~~~~~~~~~~~~~
\begin{minipage}[c]{0.5\textwidth}
	\InitGraph{4}{4}{2}{2}{1cm}
	\Coordinates(0.8,1)(0.8,1)
	\TextAt(2,0)[b]{$x$}
	\TextAt(0,2)[l]{$y$}
	\TextAt(-0.2,0.4)[c]{$\frac{1}{3}$}
	\MoveTo(0,0)
	\SetDarkgrey
	\Ellipse(1.6,0.3)
	\CloseGraph
\end{minipage}
	
\subsection{Definition (Eigenvektor, Eigenwert, Eigenraum)}
Sei $A \in \M_n(\K),~~ v \in \K^n,~~ v \neq \mathcal{O}$, heißt \uline{Eigenvektor (EV)} zum \uline{Eigenwert (EW)} $\lambda \in \K$, falls $Av = \lambda v$.\\
\\
Die Menge $Eig(\lambda) := \{v \in \K^n | Av = \lambda v \}$ heißt \uline{Eigenraum} von $\lambda$.\\
$\Bigg[$z.B. ist $\vec2{4}{0}$ auch EV zum EW 2 von $A = \begin{pmatrix}
2 & 0 \\
0 & \frac{1}{3}
\end{pmatrix}\Bigg]$
\subsection{Beispiel}
Konstruiere Matrix $A \in \M_2(\R)$, die in Richtung $v_1 = \vec2{1}{1}$ um $\lambda_1 = 2$ streckt und in Richtung $v_2 = \vec2{3}{-1}$ um $\lambda_2 = \frac{2}{3}$ staucht. \\
%Zeichung
Man erhält: 
\begin{itemize}
	\item[a)] $A\vec2{1}{1} = 2 \vec2{1}{1} \Rightarrow \begin{cases}
	I \qquad a_{11} + a_{12} = 2 \\
	II \qquad a_{21} + a_{22} = 2\\
	\end{cases}$
	\item[b)] $A \vec2{3}{-1} = \frac{2}{3}\vec2{3}{-1} \Rightarrow \begin{cases}
	III \qquad 3a_{11} - a_{12} = 2\\
	IV \qquad 3a_{21} - a_{22} = -\frac{2}{3}\\
	\end{cases}$
\end{itemize}
$III = III +I: \quad 4a_{11} = 4,~~~~~ a_{11} = 1 \overset{I}{\Rightarrow} a_{12} = 1 \\
IV = IV + II:\quad 4a_{21} = \frac{4}{3},~~~~~ a_{21} = \frac{1}{3} \overset{II}{\Rightarrow} a_{22} = \frac{5}{3}\\
\Rightarrow A = \begin{pmatrix}
1 & 1 \\
\frac{1}{3} & \frac{5}{3} \\
\end{pmatrix}$\\
\\
\\
\begin{minipage}[c]{0.5\textwidth}
	\InitGraph{4}{4}{2}{2}{1cm}
	\Coordinates(0.8,1)(0.8,1)
	\TextAt(2,0)[b]{$x$}
	\TextAt(0,2)[l]{$y$}
	\MoveTo(0,0)
	\Circle(0.8)
	\CloseGraph
\end{minipage}
$\longrightarrow$~~~~~~~~~~~~~~~~~
\begin{minipage}[c]{0.5\textwidth}
	\InitGraph{4}{4}{2}{2}{1cm}
	\Coordinates(1,1)(1,1)
	\TextAt(2,0)[b]{$x$}
	\TextAt(0,2)[l]{$y$}
	\MoveTo(0,0)
	\LineAt(-2,-2,2,2)
	\TextAt(2,2)[t]{$Eig(2)=\Big\langle\vec2{1}{1}\Big\rangle_\R$}
	\LineAt(-3,1,3,-1)
	\TextAt(2,-2)[c]{$Eig(\frac{2}{3})=\Big\langle\vec2{3}{-1}\Big\rangle_\R$}
	\begin{Transform}{1}{1}{0.33333}{1.66666}
		\CircleAt(0,0)(1)
	\end{Transform}
	\CloseGraph
\end{minipage}
\subsection*{Eigenwertproblem}
Geg.: $A \in \M_n(\K)$. Ges.: Eigenvektor und Eigenwert \\
\underline{Grundidee} zur Berechnung von EV + EW:\\
Ang. $v \neq 0$ ist EV von $A$ zum EW $\lambda \in \K$.
\begin{align*}
Av = \lambda v &\Leftrightarrow Av=(\lambda \cdot E_n) v\\
&\Leftrightarrow Av - \lambda E_n v = 0 \\
&\Leftrightarrow \underbrace{(A - \lambda E_n)}_{\in \M_n(\K)} v = 0 \\
\end{align*}
D.h. $v \in \ker(A - \lambda E_n)!$ Da $v \neq 0$, ist $\ker(A - \lambda E_n) \neq \{\O\}$ und somit $A - \lambda E_n$ weder injektiv (\hyperref[6.9]{6.9}) noch umkehrbar (\hyperref[6.15]{6.15}). Ergebnis: 
\subsection{Satz ($A-\lambda E_n$)}
\label{9.4}
Sei $A \in \M_n(\K)$.
\begin{itemize}
	\item[1)] $\lambda$ EW von $A \Leftrightarrow \det(A- \lambda E_n) = 0$
	\item[2)] $Eig(\lambda) = \ker(A -\lambda E_n)$
	\item[3)] EV $v \neq 0$ ist Lösung $(A -\lambda E_n)v = 0$.
\end{itemize}
\subsubsection*{Beweis}
Siehe oben.\qed
\subsection{Beispiel}
\label{9.5}
\uline{Gegeben:} $A = \begin{pmatrix}
1 & 1 \\
-2 & 4\\
\end{pmatrix} \in \M_2(\R)$\\
\uline{Gesucht:} EW + EV \\
\\\\
Benutze \hyperref[9.4]{9.4.1)}: Es ist $\det(A- \lambda E_2) = det\begin{pmatrix}
1- \lambda & 1 \\
-2 & 4 - \lambda
\end{pmatrix} = (1- \lambda)(4 -\lambda) +2\\
\noindent\hspace*{60mm} =  \lambda^2 - 5\lambda  + 6 \\
\noindent\hspace*{58mm}\overset{\hyperref[9.4]{9.4.1)}}{=} 0$\\
$\Rightarrow \lambda_1 = 3,~~~ \lambda_2 = 2 \overset{\hyperref[9.4]{9.4.1)}}{\Rightarrow} A$ hat EW $\lambda_1$ und $\lambda_2$.\\
\\
Die EV $v_1,v_2 \in \R^2$ erfüllen somit 
\begin{itemize}
	\item[a)] $(A - \lambda_1 E_2)v_1 = 0 \\
	\Leftrightarrow \begin{pmatrix}
	1 -3 & 1 \\
	-2 & 4-3 \\
	\end{pmatrix} \cdot \vec2{x}{y} = 0 \Leftrightarrow I: -2x +y = 0, ~~~II: -2x +y = 0\\
	\noindent\hspace*{49mm} \Leftrightarrow y = 2x$\\
	\noindent\hspace*{49mm}$\overset{x = 1}{\Rightarrow} v_1 = \vec2{1}{2}$ EV zum EW $\lambda_1 = 3.\\
	\noindent\hspace*{55mm} Eig(3) = \vecspaceR{\vec2{1}{2}}$
	\item[b)] Analog für $\lambda_2 = 2$. Zu Lösen $\begin{pmatrix}
	-1 & 1 \\
	-2 & 2
	\end{pmatrix} \cdot \underbrace{\vec2{x'}{y'}}_{v_2} = 0 \Rightarrow v_2 = \vec2{1}{1}.\\
	 Eig(2) = \vecspaceR{\vec2{1}{1}}$
\end{itemize}
\subsection{Definition (charakteristisches Polynom)}
\marginpar{31.01.17}
Für $A \in \M_n(\K)$ heißt $P_A(\lambda) = \det(A)-(\lambda E_n)$ das \uline{charakteristische Polynom} von $A$.
\subsection{Bemerkung}
$P_A(\lambda)$ ist Polynom vom Grad $n$, falls $A \in \M_n(\K)$ (folgt aus Definition der Determinante \hyperref[8.2]{8.2}). Die Nullstelle von $P_A(\lambda)$ sind die Eigenwerte von $A$.\\
$\Rightarrow $ für $\K = \R:~~A $ hat $ \geq n$ Eigenwerte.\\
$K = \C: $ genau $n$ Eigenwerte ( nicht notwendigerweise verschieden), \hyperref[5.11]{5.11 b)}.
\subsection*{Diagonalisierbarkeit von Matrizen}
\subsection{Definition (Diagonalmatrix)}
$D \in \M_(\K)$ heißt \uline{Diagonalmatrix}, wenn $D = \begin{pmatrix}
\lambda_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_n \\
\end{pmatrix}$
\subsection{Bemerkung}
\begin{itemize}
	\item[a)] Mit Diagonalmatrizen kann man leichter rechnen, denn:
	\begin{itemize}
		\item $\begin{pmatrix}
		\lambda_1 & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \lambda_n \\
		\end{pmatrix} \cdot \begin{pmatrix}
		\mu_1 & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \mu_n \\
		\end{pmatrix} = \begin{pmatrix}
		\lambda_1\mu_1 & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \lambda_n\mu_1 \\
		\end{pmatrix}$
		\item $\begin{pmatrix}
		\lambda_1 & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \lambda_n \\
		\end{pmatrix}^k = \begin{pmatrix}
		\lambda_1^k & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \lambda_n^k \\
		\end{pmatrix}, \qquad k \in \N$
	\end{itemize}
	\item[b)]
	Deswegen folgende Grundidee:\\
	Sei $A \in \M_n(\K)$.\\
	 Bringe $A$ auf Diagonalgestalt: Fasse dazu $A$ als Darstellungsmatrix von $\phi(v) = Av$ bzgl. Standardbasis $E$ auf, d.h. $A = A_\phi^E$. Suche Basis $B$, so dass $A_\phi^B$ Diagonalmatrix ist. Wenn es eine solche Basis $B$ gibt, dann gilt:
	\begin{tikzpicture}[scale=0.75]
	\node (v1) at (-3,3) {$\K^n$};
	\node (v2) at (1,3) {$\K^n$};
	\node (v3) at (1,-0.5) {$\K^n$};
	\node (v4) at (-3,-0.5) {$\K^n$};
	\draw  [-latex] (v1) edge (v2);
	\draw  [-latex] (v3) edge (v2);
	\draw  [-latex] (v1) edge (v4);
	\draw  [-latex]  (v4) edge (v3);
	\node at (-1.0999,3.6082) {$\substack{A=A_\phi^E}$};
	\node at (-1.1175,-0.9476) {$\substack{A_\phi^B=D}$};
	\node at (-4.0525,1.302) {$\substack{S^{-1}=S_{EB}}$};
	\node at (1.2589,1.3196) {~~~~~~$\substack{S_{BE}=S}$};
	\end{tikzpicture}
\end{itemize}
\subsection{Beispiel}
$A = \begin{pmatrix}
1 & 1 \\
-2 & 4 \\
\end{pmatrix} \in \M_2(\R)$.\\
Aus \hyperref[9.5]{9.5} EV: $v_1 = \vec2{1}{2}, ~~v_2 = \vec2{1}{1}.$ EW: $\lambda_1 = 3,~~ \lambda_2 = 2$.\\
Wähle als Basis $B = \{v_1,v_2\}$.\\
$\Rightarrow S_{B,E} =\begin{pmatrix}
1 & 1 \\
2 & 1 \\
\end{pmatrix} = S \Rightarrow S_{E,B} = S^{-1} \overset{\hyperref[8.9]{8.9b)}}{=} \begin{pmatrix}
-1 & 1 \\
2 & -1 \\
\end{pmatrix}\\
\Rightarrow D = S^{-1}AS = \begin{pmatrix}
-1 & 1 \\
2 & -1 \\
\end{pmatrix}\cdot \begin{pmatrix}
1 & 1 \\
-2 & 4 \\
\end{pmatrix} \cdot \begin{pmatrix}
1 & 1 \\
2 & 1 \\
\end{pmatrix} = \underbrace{\begin{pmatrix}
	3 & 0 \\
	0 & 2 \\
	\end{pmatrix}}_{\begin{pmatrix}
	\lambda_1 & 0 \\ 0 & \lambda_2 \\
	\end{pmatrix}}$\\
Somit ist z.B. $A^5 = \underbrace{SDS^{-1}}_{A} \cdot \underbrace{SDS^{-1}}_{A} \cdot ... \cdot \underbrace{SDS^{-1}}_{A}\\
\noindent\hspace*{29.5mm}= SD^5S^{-1} =  \begin{pmatrix}
1 & 1 \\
2 & 1 \\
\end{pmatrix}\cdot \begin{pmatrix}
243 & 0 \\
0 & 32 \\
\end{pmatrix} \cdot \begin{pmatrix}
-1 & 1 \\
2 & -1 \\
\end{pmatrix} = \begin{pmatrix}
-179 & 211 \\
-422 & 454 \\
\end{pmatrix}$
\subsection*{Fragen}
\begin{itemize}
	\item[1)] Ist jeder $A \in \M_n(\K)$ diagonalisierbar?
	\item[2)] Wie diagonalisiert man $A$?
\end{itemize}
\subsection{Definition (Diagonalisierbarkeit)}
\label{9.11}
\begin{itemize}
	\item[i)] $A \in \M_n(\K)$ heißt \uline{diagonalisierbar}, wenn es eine invertierbare Matrix \\$S \in \M_n(\K)$ gibt, so dass $A = SDS^{-1},~~~ D$ Diagonalmatrix.
	\item[ii)] Eine lineare Abbildung $\phi:  V \rightarrow V,~~ \dim(V) < \infty$, heißt diagonalisierbar, falls es eine Basis $B$ gibt, so dass $A_\phi^B$ Diagonalmatrix.
\end{itemize}
\subsection{Satz (Spektralsatz)}
\begin{itemize}
	\item[i)] $A \in \M_n(\K)$ diagonalisierbar $\Leftrightarrow \exists n$ linear unabhängig EV $\underbrace{v_1,...,v_n}_{\text{Basis von }\K^n}$. In diesem Fall ist $A = SDS^{-1}$, wobei $S = (v_1,...,v_n)$ und $D = \begin{pmatrix}
	\lambda_1 & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & \lambda_n \\
	\end{pmatrix}$ mit $v_i$ EV zum EW $\lambda_i$ von $A$.
	\item[ii)] $A$ hat $n$ verschiedene EW $\lambda_1,...,\lambda_n \Rightarrow A$ diagonalisierbar.
\end{itemize}
\subsubsection*{Beweis}
\begin{itemize}
\item[i)] $A$ diagonalisierbar $\overset{\hyperref[9.11]{9.11i)}}{\Leftrightarrow} \exists S$ mit $S^{-1}AS = \begin{pmatrix}
\mu_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \mu_n \\
\end{pmatrix}$\\
$\Leftrightarrow AS = S \cdot \begin{pmatrix}
\mu_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \mu_n \\
\end{pmatrix}$.\\
Sei $S = (s_1,...,s_n)$. Für Spalte $i: As_i = \mu_is_i \qquad i = 1,..., n$\\
$\Leftrightarrow s_i$ EV zum EW $\mu_i$, damit muss $s_i = v_i, ~~\mu_i = \lambda_i$.\\
Insgesamt: $\underbrace{S\text{ invertierbar}}_{A \text{ diagonalisierbar}} \Leftrightarrow \underbrace{ \rg(S) = n}_{\text{d.h. Spalten l.u.}}$
\item[ii)]
$\lambda_1,...,\lambda_n$ sind paarweise verschiedene EW. Zeige per Induktion, dass EV linear unabhängig:\\
$n = 1 \rightarrow\checkmark$\\
Induktion: $n -1 \rightarrow n$\\
IV: $\underbrace{v_1,...,v_{n-1}}_{\text{EV}}$ linear unabhängig\\
IA: $v_1,...,v_n$ linear unabhängig \\
Angenommen nicht, dann ist $v_n = \sum_{i = 1}^{n-1}a_iv_i ~~~~~\big(*\big)$
$\Rightarrow$ \begin{itemize}
	\item[a)] $\lambda_nv_n = \sum_{i = 1}^{n-1}a_i\lambda_n v_i$
	\item[b)] $\lambda_nv_n = Av_n \overset{(*)}{=} \sum_{i = 1}^{n-1}a_iA v_i = \sum_{i = 1}^{n -1}a_i\lambda_iv_i$
\end{itemize}
IV: $v_1,...,v_{n-1}$ linear unabhängig $\Rightarrow$ mind. ein $a_i \neq 0$\\
$\overset{a) = b)}{\Rightarrow} \lambda_i = \lambda_n$ \Lightning\qed	
\end{itemize}
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $A = \begin{pmatrix}
	0 & -1 \\
	1 & 0 \\
	\end{pmatrix}$ ist nicht diagonalisierbar, da $P_A(\lambda) = \lambda^2 +1$ keine Nullstellen in $\R$ hat.
	\item[b)]
	Nicht jede Matrix hat $n$ verschiedene EW, z.B. $A = \begin{pmatrix}
	2 & 0 & 0 \\
	0 & 2 & 0 \\
	0 & 0 & 1 \\
	\end{pmatrix}$ hat EW $\lambda_1 = 2,~~ \lambda_2 = 1$ mit EV $v_1 = \vec3{1}{0}{0},~~ v_1' = \vec3{0}{1}{0},~~ v_2 = \vec3{0}{0}{1}$ \\
	wobei $Eig(2) = \Bigg\langle\vec3{1}{0}{0}, \vec3{0}{1}{0}\Bigg\rangle_\R,~~~ Eig(1) = \Bigg\langle\vec3{0}{0}{1}\Bigg\rangle_\R$
\end{itemize}
\newpage
\section{Norm und Skalarprodukt}
\subsection{Beispiel}
\label{10.1}
Im $\R^2$ hat man folgende Möglichkeiten:
\begin{itemize}
	\item Längenmessung: Norm eines Vektors $v \in \R^2,~~ v = \vec2{x}{y}$
	\begin{center}
		$\| v \| := \sqrt{x^2 + y^2}$
	\end{center}
	\item Abstandsmessung zwischen 2 Elementen $v = \vec2{x}{y},~~ v' = \vec2{x'}{y'}$
	\begin{center}
		$d(v,v') := \|v- v'\|$
	\end{center}
	\item Winkelberechnung mit Skalarprodukt: Sei $\alpha$ der Winkel, der von $v$ und $v'$ eingeschlossen wird und \begin{center}
		$(v | v') = \Big(\vec2{x}{y} \Big| \vec2{x'}{y'}\Big) := xx' + yy'$
	\end{center} das Skalarpodukt von $v$ und $v'$. Dann ist 
	\begin{center}
		$\cos(\alpha) = \frac{(v | v')}{\|v\| \cdot \|v'\|}$
	\end{center}
	Wenn $\|v\| = \|v'\| = 1$, so ist $\cos(\alpha) = (v|v')$.\\
	\\
	 Es ist für $v=\vec2{1}{1}$ und $v'=\vec2{0}{1}$: $\|v\| = \sqrt{1^2 + 1^2} = \sqrt{2},~~ \\\|v'\| = \sqrt{1^2 + 0^2} = 1,~~ d(v,v') = \Big\| \vec2{1}{1}- \vec2{1}{0}\Big\| = \Big\| \vec2{0}{1}\Big\| = 1,\\ (v|v') = 1 \cdot 1 + 1 \cdot 0 = 1, ~~\cos(\alpha) = \frac{(v|v')}{\|v\| \cdot \|v'\|} = \frac{1}{2} \Rightarrow \alpha = \frac{\pi}{4} (45\degree)$
\end{itemize}
Wie kann man Norm (Länge, Abstand) und Skalarprodukt (Winkel) für beliebige $\R$-Vektorräume verallgemeinern?
\subsection{Definition (Skalarprodukt, Norm, Abstand, Vektorraum)}
\label{10.2}
\marginpar{01.02.17}
Sei $V ~~\R-$Vektorraum.
\begin{itemize}
	\item[a)] Eine Abbildung $(\cdot | \cdot): V \times V \rightarrow \R,~~ (v,w)\mapsto (v|w)$\\
	heißt \uline{Skalarprodukt}, falls:
	\begin{itemize}
		\item[i)] (Positive Definitheit)\\
		$(v|v) \geq 0 ~~\forall v \in v$\\
		$(v|v) = 0 \Leftrightarrow v= 0$ 
		\item[ii)] (Symmetrie)\\$(v|w) = (w|v)~~ \forall v,w \in V$ 
		\item[iii)] (Bilinearität)
		\begin{itemize}
			\item $(\lambda v|w) = (v|\lambda w) = \lambda(v|w)~~ \forall \lambda \in \R ~~\forall v,w \in V$
			\item $(u+v | w) = (u|w) + (v|w) ~~\forall u,v,w \in V$
		\end{itemize}
		
	\end{itemize}
	\item[b)] Ein $\R-$Vektorraum mit Skalarprodukt heißt \uline{Euklidischer Vektorraum}.
	\item[c)] $\|v\| := \sqrt{(v|v)}$ heißt \uline{(Euklidische) Norm} und $d(v,w) = \|v -w\|$ \uline{(Euklidischer) Abstand}.
\end{itemize}
\subsection{Beispiel}
\begin{itemize}
	\item[a)] Das Skalarprodukt in \hyperref[10.1]{10.1} erfüllt a)i)-iii) von Def \hyperref[10.2]{10.2}
	\begin{itemize}
		\item[i)] $(v|v) = \Big(\vec2{x}{y}| \vec2{x}{y}\Big) = x^2 + y^2 \geq 0 ~~\forall v \in \R$ und \\$(v|v) = 0 \Leftrightarrow x = y = 0 \Leftrightarrow v = 0$\checkmark
		\item[ii),iii)] nachrechnen \checkmark
	\end{itemize}
	\item[b)] Allgemein heißt im $\R^n ~~~(v|w) := \sum_{i = 1}^{n}v_i w_i, ~~v = \vec3{v_1}{\vdots}{v_n},~~ w = \vec3{w_1}{\vdots}{w_n}$ das Standardskalarprodukt
	$\|v\| = \sqrt{(v|v)} = \sqrt{v_1^2 + ... + v_n^2}$.
	\item[c)] Für $V = \mathcal{C}[a,b] = \{f:[a,b]\rightarrow \R\mid f \text{ ist stetig} \}$ kann man leicht nachrechnen, dass \begin{center}
		$(f|g) := \int_{a}^{b} f(t) \cdot g(t) dt$
	\end{center} 
	ein  Skalarprodukt ist. Die Norm ist dann 
	\begin{center}
		$\|f\| = \sqrt{\int_{a}^{b} f^2(t)dt}$
	\end{center}
	und erfüllt folgende Eigenschaften:
\end{itemize}
\subsection{Satz (Eigenschaften Norm)}
\label{10.4}
$V ~~\R-$Vektorraum.
\begin{itemize}
	\item[i)] (Positive Definitheit)\\ $\|v\| \geq 0 \qquad \forall v\in V$\\
	$\|v\| = 0 \Leftrightarrow v = \mathcal{O}$ 
	\item[ii)] $\|\lambda v\| = |\lambda| \cdot \|v\| \qquad \forall\lambda \in \R ~~\forall v \in V$
	\item[iii)]($\triangle-$Ungleichung)\\
	 $\|v+w\| \leq \|v\| + \|w\| \qquad \forall v,w \in V$ 
	%Zeichnung
\end{itemize}
\subsubsection*{Bemerkung}
\begin{itemize}
	\item[i) und ii)] sind klar.
	\item[iii)] beweist man mit \hyperref[10.5]{10.5}, Cauchy-Schwarz-Ungleichung (C-S).
\end{itemize}
\subsection{Satz (Cauchy-Schwarz-Ungleichung)}
\label{10.5}
$|(v|w)| \leq \|v\| \cdot \|w\| \qquad \forall v,w \in V,~~ V~~\R-$Vektorraum\\
Gleichheit $\Leftrightarrow v,w$ linear abhängig
\subsubsection*{Beweis}
Hier ohne Beweis, siehe Literatur oder Wikipedia: \href{https://de.wikipedia.org/wiki/Cauchy-Schwarzsche_Ungleichung#Reeller_Fall}{Cauchy-Schwartzsche Ungleichung}.\qed
\subsubsection*{Beweis von $\triangle-$Ungleichung aus \hyperref[10.4]{10.4}}
\begin{align*}
\|v+w\|^2 &= (v+w |v+w) \\
&= \underbrace{(v|v)}_{\|v\|^2} + \underbrace{2(v|w)}_{\leq 2\|v\|\cdot ||w||} + \underbrace{(w|w)}_{\|w\|^2} \\
&\overset{C-S}{\leq}(\|v\| + \|w\|)^2
\end{align*}\qed
\subsection{Bemerkung}
Es ist $(v|w) = \underbrace{v^T \cdot w}_{\text{Matrixprodukt}}$ für $v,w \in \R^n$\\
z.B $\Bigg(\vec3{1}{0}{3}\Bigg|\vec3{-1}{2}{1}\Bigg) = (1,0,3) \cdot \vec3{-1}{2}{1}=-1+0+3=2$
\subsection{Beispiel}
$v = \vec3{-1}{2}{1}, ~~~w = \vec3{2}{2}{4} \in \R^3$\\
$(v|w) = -2 + 4 + 4 = 6\\
 \|v\| = \sqrt{1+4+1} = \sqrt{6}\\
 \|w\|=\sqrt{4+4+16}=\sqrt{24}\\
  d(v,w) = \|v -w\| = \sqrt{9 + 0 +9} = \sqrt{18}\\
   \cos(\alpha) = \frac{(v|w)}{\|v\| \cdot \|w\|} = \frac{6}{\sqrt{6}\cdot \sqrt{24}} = \frac{1}{2} \Leftrightarrow \alpha = \frac{\pi}{3}$
   \newpage
   \section{Orthonormalsysteme}
   \subsection{Definition (Grundbegriffe)}
   $V$ euklidischer Vektorraum
   \begin{itemize}
   	\item[i)] $v,w$ heißen orthogonal (senkrecht), $v \perp w,$ falls $(v|w) = 0$, ($\mathcal{O}$ ist $\perp$ zu allen $v \in V$).
   	\item[ii)] $M \subseteq V$ heißt \uline{Orthogonalsystem (OGS)}, falls $(v|w) = 0 \qquad \forall v,w \in M$ und $v \neq w$.\\
   	 Wenn zusätzlich $\|v\| = 1 \qquad \forall v \in M$, so heißt $M$ \uline{Orthonormalsystem (ONS)}.
   	\item[iii)] Ist $\dim(V) < \infty$, so heißt $M$ \uline{Orthonormalbasis} von $V$, falls $M$ ONS und $M$ ist Basis von $V$.
   \end{itemize}
\subsection{Bemerkung}
Jedes ONS ist linear unabhängig: $\{v_1,...,v_n\} \subseteq V$ ONS.\\
$\mathcal{O} = \lambda_1v_1 + ... + \lambda_kv_k$, zu zeigen: $\lambda_1 = ... = \lambda_k = 0$\\
$\Leftrightarrow0 = (v_1|\lambda_1v_1 + ... + \lambda_kv_k) = \lambda_1\underbrace{(v_1|v_1)}_{=\|v\|=1} + \lambda_2\underbrace{(v_1|v_2)}_{\perp,\text{ also }0} + ... + \lambda_k\underbrace{(v_1| v_k)}_{\perp,\text{ also }0} = \lambda_1 \Rightarrow \lambda_1 = 0$\\
Analog für $\lambda_2,...,\lambda_k$
\subsection*{Gram-Schmidtsches Orthogonalisierungsverfahren}
Grundidee im $\R^n$ mit 3 Vektoren $v_1,v_2,v_3 \in \R^n$:\\
\uline{Gegeben:} $v_1,v_2,v_3 \in \R^n$\\
\uline{Gesucht:} OGS $\{w_1,w_2,w_3\}$ mit $\vecspaceR{w_1,w_2,w_3} = \vecspaceR{v_1,v_2,v_3}$
%Zeichung
\begin{enumerate}
	\item $w_1 = v_1$
	\item $w_2 = \lambda \cdot w_1 + v_2$\\
	(verlängere/verkürze $w_!$, so dass $w_2\perp w_1$)
	$\mathcal{O} = (w_1|w_2) = (w_1| \lambda w_1 + v_2) = \lambda \|w_1\|^2 + (w_1|v_2) \Leftrightarrow \lambda = -\frac{(w_1|v_2)}{\|w_1\|^2}$
	\item $w_3 = \lambda_1'w_1 + \lambda_2'w_2 + v_3$\\
	$\mathcal{O} = (w_3|w_2) = (\lambda_1'w_1 + \lambda_2'w_2 + v_3| w_2) = \lambda_2' \|w_2\|^2 + (v_3|w_2) \Rightarrow \lambda_2' = -\frac{(v_3|w_2)}{\|w_2\|^2}$\\
	$\mathcal{O} = (w_3|w_1) \Leftrightarrow \lambda_1' = -\frac{(w_1|v_3)}{\|w_1\|^2}$
\end{enumerate}
Allgemein:
\subsection{Satz (Gram-Schmidt)}
\uline{Gegeben:} $v_1,...,v_k \in V, ~~V$ euklidischer Vektorraum. \\
\uline{Gesucht:} ONS von $\vecspaceR{v_1,...,v_k}$. \\
\\
Definiere dazu $w_1 := v_1,~~ w_{r+1} = v_{r+1} + \sum_{i = 1}^{r} \lambda_i^{(r+1)} w_i$ mit $\lambda_i^{(r+1)} = -\frac{(w_i|v_{r+1})}{\|w_i\|^2}$ (falls $w_i \neq \mathcal{O}$) und $y_r := \frac{w_r}{\|w_r\|}$ (falls $w_r \neq \mathcal{O}$).\\
Dann gilt
\begin{itemize}
	\item[1)] Bricht die Iteration $\overbrace{\text{nach } i \text{ Schritten}}^{\text{d.h. } w_i \neq 0 \text{ für } i = 1,...,k}$ ab mit $i \leq k$ \underline{nicht} ab, so ist $\{w_1,...,w_k\}$ OGS und $\{y_1,...,y_k\}$ ONS von $\vecspaceR{v_1,...,v_k}$
	\item[2)] Bricht die Iteration nach $r$ Schritten ab (d.h. $w_r = 0$), so gilt: $v_1,...,v_{r-1}$ linear unabhängig und $v_1,...,v_r$ linear abhängig
\end{itemize}
\subsubsection*{Beweis}
Wie oben, vollständige Induktion.\qed 
\subsection{Beispiel}
\marginpar{07.02.17}
$v_1,v_2 \in \R^3,~~ v_1 = \vec3{1}{1}{0},~ v_2 = \vec3{1}{3}{2}$\\
Suche ONB der Ebene $\vecspaceR{v_1,v_2}$.\\
Gram-Schmidt:
\begin{enumerate}
	\item $w_1 = v_1 = \vec3{1}{1}{0}$
	\item $w_2 = v_2 + \lambda_1 w_1$ mit $\lambda_1 = -\frac{(v_2,w_1)}{\|w_1\|^2} = -\frac{4}{2} = -2\\
	\Rightarrow w_2 = \vec3{1}{3}{2} - 2\cdot \vec3{1}{1}{0} = \vec3{-1}{1}{2}\\
	\Rightarrow \text{OGB}: \Bigg\{\vec3{1}{1}{0},~\vec3{-1}{1}{2}\Bigg\}\\
	\text{ONB}: \Bigg\{\frac{1}{\sqrt{2}}\vec3{1}{1}{0},~~\frac{1}{\sqrt{6}}\vec3{-1}{1}{2}\Bigg\}$
\end{enumerate}
\subsection{Definition (Orthogonale Matrix)}
$A \in \M_n(\R)$ heißt \uline{orthogonal}, falls ihre Spalten eine Orthogonalbasis des $\R^n$ bilden.\\
$\mathcal{O}(n) := \{A \in \M_n(\R)\mid A \text{ orthogonal}\}$ heißt orthogonale Gruppe ($\mathcal{O}(n)$ ist tatsächlich Gruppe).
\subsection{Beispiel}
$ A = \begin{pmatrix}
\cos(\phi) & -\sin(\phi) \\
\sin(\phi) & \cos(\phi) \\
\end{pmatrix}, ~~~\phi \in \R$\\
\begin{itemize}
	\item $\Big(\vec2{\cos(\phi)}{\sin(\phi)}\Big| \vec2{-\sin(\phi)}{\cos(\phi)}\Big) = 0$
	\item $\Big\|\vec2{\cos(\phi)}{\sin(\phi)}\Big\| = \Big\|\vec2{-\sin(\phi)}{\cos(\phi)}\Big\| = \sqrt{\cos^2(\phi) + \sin^2(\phi)} = 1$
\end{itemize}
$E_n$ ist auch orthogonal (Ist die Eins $\upharpoonleft$ in der Gruppe $\mathcal{O}(n)$, \hyperref[3.9]{3.9}).
\subsection{Satz (Orthogonale Matrix)}
Für $A \in \mathcal{O}(n)$ gilt:
\begin{itemize}
	\item[i)] $A^T\*A = E_n$, d.h. $A^{-1} = A^T$
	\item[ii)] $\|Av\| = \|v\|$ \qquad Längentreue
	\item[iii)] $|\underbrace{\det(A)}_{\in \R}| = 1$
\end{itemize}
\subsubsection*{Beweis}
$A = (s_1,...,s_n)$
\begin{itemize}
	\item[i)] $\{s_1,...,s_n\}$ ONB $\Rightarrow (s_i,s_j) = \begin{cases}
	1 \qquad i = j\\
	0 \qquad u \neq j\\
	\end{cases}\Rightarrow A^T \cdot A = E_n$
	\item[ii)] $\|Av\|^2 =(\underbrace{Av}_{\in \R^n},\underbrace{Av}_{\in \R^n}) = (Av)^T \cdot (Av) = v^T \cdot \underbrace{A^T \cdot A}_{= E_n} \cdot v = (v|v) = \|v\|^2$
	\item[iii)] $1 = \det(E_n) = \det(A^T \cdot A) \overset{\hyperref[8.6]{8.6,~D9}}{=} \det(A^T) \cdot \det(A)\\
	\noindent\hspace*{20mm} = (\det(A))^2 \Rightarrow \det(A) = \pm 1$
\end{itemize}\qed
\subsection{Bemerkung}
Man kann zeigen, dass jede symmetrische Matrix $A \in \M_n(\R)$\quad $n$ (nicht notwendigerweise verschiedene) reelle Eigenwerte hat und othogonal diagonalisierbar ist, d.h. $\exists ~S \in \mathcal{O}(n): \underbrace{S^{-1} \cdot A \cdot S}_{S^TAS} = D$ ($D$ Diagonalmatrix, die die EW von $A$ enthält).\\
Die Spalten von $S$ sind die EV von $A$.
\newpage
\section{Taylorreihen}
\subsubsection*{Ziel}
 Beweis von $e^{\i\phi} = \cos(\phi) + \i\sin(\phi),\qquad (\phi \in \R)$.\\
Dazu zeigt man
\begin{enumerate}
	\item $e^x = \exp(x) = \sum_{j = 0}^{\infty} \frac{x^j}{j!} \qquad (x \in \R)$ 
	\item Man erweitert für $z \in \C ~~~\exp(z) := \sum_{j = 0}^{\infty}\frac{z^j}{j!}$
	\item Zum Schluss zeigt man $\exp(\i\phi)= \cos(\phi) + \i \sin(\phi)$, indem man $\cos(\phi)$ und $\sin(\phi)$ als Reihen darstellt.
\end{enumerate}
Hier wird nur ein Teil von 1) bewiesen. Dabei wird $(e^x)' = e^x \quad \forall x \in \R$ als bekannt vorrausgesetzt. 3) wird ebenfalls gezeigt.\\
Dazu Taylorpolynome:\\
\\
\begin{tikzpicture}[scale=0.85]
\draw[->]
(-0.2,0) -- (4.2,0) node[right] {$x$};
\draw[->]
(0,-0.2) -- (0,4.2) node[above] {$f(x)$};
\draw[color=black, domain=-0.5:3.5]
plot(\x,0.125*\x*\x*\x-3*0.125*\x*\x+3*0.125*\x+0.5) node[right]{$f$};
\draw[color=blue, domain=-0.6:3.2, very thin]
plot(\x,0.08*\x*\x*\x-3*0.08*\x*\x+3*0.08*\x+0.55) node[right]{bessere Näherung, bei der die Ableitung in $x_0$ übereinstimmt ($T_k$)};
\draw[color=gray,domain=-0.2:4] 
plot(\x,\x-0.6) node[right]{schlechte Annäherung};
\end{tikzpicture}
\\
Man möchte $k-$mal diffbare Funktion $f: I \rightarrow \R$ durch ein Polynom $T_k(x)$ möglichst gut annähern. Dazu wählt man $T_k$ so, dass $T_k^{(j)}(x_0) = f^{(j)}(x_0)$ für ein $x_0 \in I, ~~~j = 0,~...,~k.$
\subsection{Definition (Taylorpolynom, Restglied)}
Sei $I = (a,b),~~ x_0 \in I,~~~ f:I \rightarrow \R~~~~k-$mal differenzierbar, dann heißt \begin{center}
	$T_k :\R \rightarrow \R,~~~T_k(x) := \sum_{j = 0}^{k}\frac{f^{(j)} (x_0)}{j!}(x-x_0)^j$
\end{center}
\uline{$k-$tes Taylerpolynom} von $f$ in $x_o$. \\
Die Fehlerdifferenz \begin{center}
	$R_k: I \rightarrow \R,~~~ R_k(x) := f(x) - T_k(x)$
\end{center} nennt man \uline{$k-$tes Restglied} von $f$ in $x_0$. 
\subsection{Bemerkung}
$T_k$ ist das eindeutig bestimmte Polynom vom Grad $\leq k$, das $T_k^{(j)}(x_0) = f^{(j)}(x_0)$ erfüllt $\forall j = 0,~...,~k$:\\
\\
$T_k(x) = a_0 + a_1(x-x_0) + ... + a_j(x-x_0)^j + ... + a_k(x-x_0)^k$\\
$a_j = \frac{f^{(j)}(x_0)}{j!}$\\
$\Rightarrow T_k^{(j)}(x) = j!a_j + c_j(x-x_0) + ... + c_k(x-x_0)^{k-j}\\
\Rightarrow T_k^{(j)}(x_0) = j! \cdot a_j = f^{(j)}(x_0)$
\subsection{Satz von Taylor}
\label{12.3}
Sei $x_0 \in I =(a,b),~~~ f: I \rightarrow \R~~~(k+1)$-mal diffbar, $k \in \N_0$. Dann gibt es zu jedem $x \in I$ eine Stelle $\xi$ zwischen $x$ und $x_0$, so dass
\begin{center}
	$R_k(x) = \frac{f^{(k+1)}(\xi)}{(k+1)!} (x-x_0)^{k+1}$
\end{center}
(Lagrange-Form des Restgliedes).
\subsubsection*{Beweis}
Sei $g(x) = (x-x_0)^{k+1}$. Es gilt $g^{(j)}(x_0) = 0$ und $R_k^{(j)}(x_0) = 0 \qquad \forall j = 0,...,k.$ Verwendet wird der 2. Mittelwertsatz (Mathe II).\\
$\Rightarrow \frac{R_k(x)}{g(x)} = \frac{R_k(x) - \overbrace{R_k(x_0)}^{=0}}{g(x) - \underbrace{g(x_0)}_{=0}} \overset{2.MWS}{=} \frac{R_k'(\xi_1)}{g'(\xi_1)} \qquad \xi_1$ zwischen $x$ und $x_0$\\
\noindent\hspace*{15mm}$= \frac{R_k'(\xi_1) - R_k'(x_0)}{g'(\xi_1) - g'(x_0)} \overset{2.MWS}{=} \frac{R_k''(\xi_2)}{g''(\xi_2)}\qquad \xi_2$ zwischen $\xi_1$ und $x_0$.\\
\noindent\hspace*{16mm}$\vdots$\\
\noindent\hspace*{11.5mm}$\overset{2MWS}{=} \frac{R_k^{(k+1)}(\xi_{k+1})}{g^{k+1}(\xi_{k+1})} = \frac{f^{(k+1)}(\xi_{k+1})}{(k+1)!}\qquad \xi_{k+1}$ zwischen $\xi_k$ und $x_0$\\
Setze $\xi = \xi_{k+1}$, Behauptung folgt.\qed
\subsection{Beispiel}
\marginpar{08.02.17}
Berechne $\sin(1)$ mit einer Fehlerdifferenz kleiner als $10^{-3}$.\\
Aus \hyperref[12.3]{12.3} $f(x) = T_k(x) + \overbrace{R_k(x)}^{\text{Fehler}}$\\

$\Rightarrow |R_k(x)| = \frac{|f^{(k+1)}(\xi)|}{(k+1)!} |x - x_0|^{k+1} < 10^{-3}$ mit $\xi$ zwischen $x$ und $x_0$.\\
\\
Suche $k \in \N$, für das Ungleichung erfüllt ist: \\
$f(x) = \sin(x),~~ f'(x) = \cos(x),~~ f''(x) = -\sin(x),\\ f'''(x) = -\cos(x),~~f^{(4)}(x)=f(x)$\\

$\Rightarrow f^{(2n)}(x) = (-1)^n \sin(x),~~~ f^{(2n+1)}(x) = (-1)^n \cos(x) \qquad n \geq 0$\\
Wähle als Entwicklungspunkt $x_0 = 0$.\\
\\
Damit ist $|R_k(x)| = |R_k(1)| = \frac{|f^{(k+1)}(\xi)|}{(k+1)!} |1-0|^{k+1} \leq \frac{1}{(k+1)!} \overset{!}{<} \frac{1}{1000}$\\

$ \Leftrightarrow (k+1)! > 1000 \Leftrightarrow k \geq 6$\\
\\
Wähle $k = 6$:\\
Dann ist $f(1) = \sin(1)\\
\noindent\hspace*{22.5mm}\approx T_6(1) = \frac{\sin(0)}{0!}(1-0)^0 +\frac{\cos(0)}{1!}(1-0)^1 + \frac{-\sin(0)}{2!}(1-0)^2\\
\noindent\hspace*{41mm}+ ... + \frac{-\sin(0)}{6!}(1-0)^6\\
\noindent\hspace*{36mm} = 0 + 1 + 0+ -\frac{1}{6} + 0 + \frac{1}{120} - 0 = \frac{101}{120}\\
\noindent\hspace*{36mm}= 0,841\overline{6}$\\
\\
Für Funktionen, die unendlich oft differenzierbar sind (z.B. $e^x,\sin x)$, kann man sogar eine Taylorreihe aufstellen:
\subsection{Definition (Tylorreihe)}
Sei $x_0 \in I = (a,b), ~~~f: I \rightarrow \R$ unendlich oft diffbar. Dann heißt \begin{center}
	$T: \R \rightarrow \R,~~~ T(x) = \sum_{j = 0}^{\infty} \frac{f^{(j)}(x_0)}{j!} (x-x_0)^j$
\end{center}
\uline{Taylorreihe} von $f$ in $x_0$.
\subsection{Bemerkung}
\begin{itemize}
	\item[1)] $T(x)$ muss nicht konvergent sein.
	\item[2)] Wenn $T(x)$ konvergiert für ein $x \neq x_0$, so muss $T(x)$ nicht notwendig gegen $f(x)$ konvergieren.
\end{itemize}
\subsection{Beispiel}
Man kann zeigen, dass $f:\R \rightarrow \R$ mit $f(x) = \begin{cases}
e^{-\frac{1}{x}} \quad x > 0\\
0 \qquad x \leq 0
\end{cases}$ beliebig oft diffbar ist.\\
Da $f^{(j)}(0) = 0 ~~~\forall j \in \N_0$, ist $T(x) = 0 \quad  \forall x \in \R$, aber $f(x) \neq 0$ für $x > 0$.
\subsection{Satz (Konvergenz Taylorreihe)}
\label{12.8}
Seien $x_0, x \in I$ und sei $f$ unendlich oft diffbar. \\
$T(x)$ konvergiert genau dann gegen $f(x)$, wenn $R_k(x) \overset{k \rightarrow \infty}{\rightarrow} 0$
\subsubsection*{Beweis}
Da $T_k(x)$ die $k$-te Partialsumme von $T(x)$ ist und $|f(x) - T_k(x)| = |R_k(x)| \overset{k \rightarrow \infty}{\rightarrow} 0$, ist $f(x)$ der Grenzwert von $T_k(x)$ für $k \rightarrow \infty$. \qed
\subsection{Beispiel}
\begin{itemize}
	\item[a)] $f(x) = \sin(x),~~~ x_0 = 0~\text{ (nur ungerade Ableitungen relevant, sonst }=0):\\
	\Rightarrow T(x) = \sum_{j = 0}^{\infty} \frac{(-1)^j x^{2j+1}}{(2j+1)!}$\\
	Da $|R_k(x)| = \frac{|f^{(k+1)} (\xi)}{(k+1)!}|x-x_0|^{k+1} \leq \frac{1}{(k+1)|} |x|^{k+1} \overset{k \rightarrow \infty}{\rightarrow} 0$ (weil Fakultät schneller wächst als jedes Polynom, Mathe II)\\
	ist $T(x) = \sin(x) \quad \forall x \in \R$
	\item[b)] Ebenso ist für $f(x) = \cos(x),~~~ x_0 = 0$:\\
	$\cos(x) =  \sum_{j = 0}^{\infty} \frac{f^{(j)}(0)}{j!} (x-0)^j = \sum_{j = 0}^{\infty} \frac{(-1)^j x^{2j}}{(2j)!}$ \\
	Beweis Konvergenz analog zu a)
	\item[c)] $f(x) = e^x,~~~ x_0 = 0$:\\
	$\Rightarrow T(x) = \sum_{j = 0}^{\infty} \frac{e^0}{j!}x^j = \sum_{j  = 0}^{\infty} \frac{x^j}{j!}$\\
	Für jedes $x \in \R$ ist $|R_k(x)| = \frac{f^{(k+1)}(\xi)}{(k+1)!}|x|^{k+1} \leq \frac{e^{|x|}}{(k+1)!}|x|^{k+1} \overset{k \rightarrow \infty}{\rightarrow} 0$ (Begründung wie bei a))\\
	$\overset{\hyperref[12.8]{12.8}}{\Rightarrow} T(x) = e^x \quad \forall x \in \R$. Somit ist $\exp(x) = e^x$ gezeigt.
	\item[d)] Für $z \in \C$ definiert man nun $e^z := \exp(z) = \sum_{j = 0}^{\infty} \frac{z^j}{j!}$.\\
	Der Konvergenzradius $\rho$ von $\exp(z)$ ist nach Euler (Mathe II)
	\begin{center}
		$\rho = \lim\limits_{j\rightarrow \infty}\Big| \frac{a_j}{a_{j+1}}\Big|$ mit $a_j = \frac{1}{j!}$
	\end{center}
	Da $\Big| \frac{a_j}{a_{j+1}}\Big| = \frac{(j+1)!}{j!} = j+1 \overset{j \rightarrow \infty}{\rightarrow}\infty$, ist $\rho = \infty$ und $\exp(z)$ ist absolut konvergent $\forall z \in \C$ (\hyperref[5.11]{5.11 d}).\\
	Deswegen kann man $\exp(z)$ umordnen und für $z = \i x,~~ x \in \R$, ergibt sich Formel von Euler (\hyperref[5.5]{5.5}) :
	\begin{center}
		$e^{\i x} = \exp(\i x) = \sum_{j = 0}^{\infty} \frac{(\i x)^j}{j!} = \underbrace{\sum_{j = 0}^{\infty} \frac{(-1)^j x^{2j}}{(2j)!}}_{\cos(x)}  + \i\underbrace{\sum_{j = 0}^{\infty} \frac{(-1)^j x^{2j+1}}{(2j+1)!}}_{\sin(x)}$,
	\end{center}
	da $\i^0 = 1,~~ \i^1 = \i,~~ \i^2 = -1,~~ \i^3 = -1,~~\i^4=\i$
	\item[e)] Wegen c): $e^1 = e = 1+1+\frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + ...$                                                   
\end{itemize}
\end{document}
